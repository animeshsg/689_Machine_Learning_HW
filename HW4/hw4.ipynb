{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "torch.manual_seed(1)\n",
    "torch.set_default_dtype(torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('mps' if torch.backends.mps.is_available() else 'mps')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data       = np.load(\"./data/ecg_data.npz\")\n",
    "Xtr        = torch.tensor(data[\"Xtr\"]).type(torch.float).to(device)    #Clean train\n",
    "Xtr_noise  = torch.tensor(data[\"Xtr_noise\"]).type(torch.float).to(device)  #Noisy train\n",
    "Xte_noise  = torch.tensor(data[\"Xte_noise\"]).type(torch.float).to(device)  #Noisy test\n",
    "\n",
    "params = np.load(\"./data/ecg_params.npz\")\n",
    "W = torch.FloatTensor(params[\"W\"]).type(torch.float).to(device)  #Decoder parameters\n",
    "V = torch.FloatTensor(params[\"V\"]).type(torch.float).to(device)  #Encoder parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ECGDataset(Dataset):\n",
    "    def __init__(self, X,R):\n",
    "        self.X = X\n",
    "        self.R= R\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.X[idx]\n",
    "        r = self.R[idx]\n",
    "        return x,r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class linear_AE(nn.Module):\n",
    "    def __repr__(self):\n",
    "        return {\"model\":\"LinearAE\",\"K\":self.K,\"lr\":self.lr,\"epoch\":self.epochs}\n",
    "    \n",
    "    def __init__(self, dim,K,epoch=1000,lr=0.001):\n",
    "        super(linear_AE, self).__init__()\n",
    "        self.model=nn.Sequential(\n",
    "            nn.Linear(dim, K),\n",
    "            nn.Linear(K,dim)\n",
    "            )\n",
    "        self.lr=lr\n",
    "        self.epochs=epoch\n",
    "        self.criterion=nn.MSELoss(reduction=\"mean\")\n",
    "        self.K=K\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        return x\n",
    "    \n",
    "    def reconstruct(self,x,W,V):\n",
    "        self.model[0].weight.data=W\n",
    "        self.model[1].weight.data=V\n",
    "        x=self.model(x)\n",
    "        return x\n",
    "    \n",
    "    def loss(self,X,X_hat):\n",
    "        return self.criterion(X,X_hat).item()\n",
    "    \n",
    "    def dataloader(self,X,R,batch_size=128):\n",
    "        dataset = ECGDataset(X,R)\n",
    "        dataloader=DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "        return dataloader\n",
    "    \n",
    "    def fit(self,X,R):\n",
    "        train_loss_list=[]\n",
    "        optimizer=optim.Adam(self.parameters(),lr=self.lr)\n",
    "        for epoch in range(self.epochs+1):\n",
    "            train_loss = 0.0\n",
    "            self.model.train()\n",
    "            optimizer.zero_grad()\n",
    "            output = self.model(X)\n",
    "            loss = self.criterion(output, R)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss_list.append(loss)\n",
    "            #if(epoch%100==0):print('Epoch [{}/{}], Train Loss: {:.9f}'.format(epoch, self.epochs, loss))\n",
    "        #self.saver(self.model.state_dict())\n",
    "        return train_loss\n",
    "\n",
    "    def test(self,X,R):\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            output=self.model(X)\n",
    "            loss=self.criterion(output,R)\n",
    "            return loss\n",
    "\n",
    "    def print_param(self):\n",
    "        for param in self.parameters():\n",
    "            print(f'parameter shape:{param.shape}')\n",
    "            #print(f'parameter value:{param.data}')\n",
    "    \n",
    "    @staticmethod\n",
    "    def plot_loss(X,X_hat):\n",
    "        time=len(X)\n",
    "        plt.plot(range(1, time+1), X, label='Noisy Data')\n",
    "        plt.plot(range(1, time+1), X_hat, label='Reconstructed Data')\n",
    "        plt.xlabel('Time')\n",
    "        plt.ylabel('ECG')\n",
    "        plt.title('Noisy Data vs. Reconstructed Data')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "    def saver(self,best_weights,PATH=\"./models\"):\n",
    "        name=''.join(f\"{key}{val}\" for key, val in self.__repr__().items())\n",
    "        path=PATH+\"/\"+name\n",
    "        torch.save(best_weights,path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=linear_AE(100,5,lr=0.0001).to(device)\n",
    "X_hat=model.reconstruct(Xtr_noise[:5],V,W)\n",
    "bloss=model.loss(Xtr_noise[:5],X_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model': 'LinearAE', 'K': 5, 'lr': 0.0001, 'epoch': 1000}"
      ]
     },
     "execution_count": 355,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.__repr__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.646934449672699\n"
     ]
    }
   ],
   "source": [
    "print(bloss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0/1500], Train Loss: 2.852277517\n",
      "Epoch [100/1500], Train Loss: 0.206439704\n",
      "Epoch [200/1500], Train Loss: 0.139985576\n",
      "Epoch [300/1500], Train Loss: 0.134292096\n",
      "Epoch [400/1500], Train Loss: 0.132769600\n",
      "Epoch [500/1500], Train Loss: 0.132250130\n",
      "Epoch [600/1500], Train Loss: 0.131983653\n",
      "Epoch [700/1500], Train Loss: 0.132048294\n",
      "Epoch [800/1500], Train Loss: 0.131748363\n",
      "Epoch [900/1500], Train Loss: 0.131882519\n",
      "Epoch [1000/1500], Train Loss: 0.131742924\n",
      "Epoch [1100/1500], Train Loss: 0.131601170\n",
      "Epoch [1200/1500], Train Loss: 0.132043496\n",
      "Epoch [1300/1500], Train Loss: 0.131595954\n",
      "Epoch [1400/1500], Train Loss: 0.132189572\n",
      "Epoch [1500/1500], Train Loss: 0.131549552\n"
     ]
    }
   ],
   "source": [
    "model=linear_AE(100,10,lr=0.01,epoch=1500).to(device)\n",
    "loss=model.fit(Xtr,Xtr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0/1000], Train Loss: 2.645266294\n",
      "Epoch [100/1000], Train Loss: 0.792887092\n",
      "Epoch [200/1000], Train Loss: 0.450813681\n",
      "Epoch [300/1000], Train Loss: 0.342895806\n",
      "Epoch [400/1000], Train Loss: 0.282044500\n",
      "Epoch [500/1000], Train Loss: 0.241769731\n",
      "Epoch [600/1000], Train Loss: 0.214009836\n",
      "Epoch [700/1000], Train Loss: 0.195932508\n",
      "Epoch [800/1000], Train Loss: 0.183821037\n",
      "Epoch [900/1000], Train Loss: 0.175416544\n",
      "Epoch [1000/1000], Train Loss: 0.169668272\n"
     ]
    }
   ],
   "source": [
    "model=linear_AE(100,10,lr=0.001,epoch=1000).to(device)\n",
    "loss=model.fit(Xtr_noise,Xtr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss:0.4173484742641449\n",
      "K:5 Train Loss:0.0   Test Loss:0.4173484742641449\n",
      "Test Loss:0.18094928562641144\n",
      "K:10 Train Loss:0.0   Test Loss:0.18094928562641144\n",
      "Test Loss:0.11624838411808014\n",
      "K:15 Train Loss:0.0   Test Loss:0.11624838411808014\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m X_train, X_val, R_train, R_val \u001b[39m=\u001b[39m train_test_split(Xtr_noise, Xtr, test_size\u001b[39m=\u001b[39m\u001b[39m0.2\u001b[39m)\n\u001b[1;32m      6\u001b[0m model\u001b[39m=\u001b[39mlinear_AE(\u001b[39m100\u001b[39m,k,lr\u001b[39m=\u001b[39m\u001b[39m0.001\u001b[39m,epoch\u001b[39m=\u001b[39m\u001b[39m1000\u001b[39m)\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m----> 7\u001b[0m train_loss\u001b[39m=\u001b[39mmodel\u001b[39m.\u001b[39;49mfit(X_train,R_train)\n\u001b[1;32m      8\u001b[0m test_loss\u001b[39m=\u001b[39mmodel\u001b[39m.\u001b[39mtest(X_val,R_val)\n\u001b[1;32m      9\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mK:\u001b[39m\u001b[39m{\u001b[39;00mk\u001b[39m}\u001b[39;00m\u001b[39m Train Loss:\u001b[39m\u001b[39m{\u001b[39;00mtrain_loss\u001b[39m}\u001b[39;00m\u001b[39m   Test Loss:\u001b[39m\u001b[39m{\u001b[39;00mtest_loss\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[7], line 41\u001b[0m, in \u001b[0;36mlinear_AE.fit\u001b[0;34m(self, X, R)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mtrain()\n\u001b[1;32m     40\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m---> 41\u001b[0m output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(X)\n\u001b[1;32m     42\u001b[0m loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcriterion(output, R)\n\u001b[1;32m     43\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/container.py:204\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    203\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 204\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    205\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "K=[k for k in range(5,150,5)]\n",
    "training_loss_list=[]\n",
    "test_loss_list=[]\n",
    "for k in K:\n",
    "    X_train, X_val, R_train, R_val = train_test_split(Xtr_noise, Xtr, test_size=0.2)\n",
    "    model=linear_AE(100,k,lr=0.001,epoch=1000).to(device)\n",
    "    train_loss=model.fit(X_train,R_train)\n",
    "    test_loss=model.test(X_val,R_val)\n",
    "    print(f'K:{k} Train Loss:{train_loss}   Test Loss:{test_loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
