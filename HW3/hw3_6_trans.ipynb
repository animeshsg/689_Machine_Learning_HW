{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "torch.manual_seed(1)\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load(\"./data/ae_data.npz\")\n",
    "\n",
    "Str = data[\"Str\"]\n",
    "Xtr = torch.from_numpy(data[\"Xtr\"]).type(torch.int)\n",
    "Ytr = torch.from_numpy(data[\"Ytr\"]).type(torch.float)\n",
    "\n",
    "Ste = data[\"Ste\"]\n",
    "Xte = torch.from_numpy(data[\"Xte\"]).type(torch.int)\n",
    "Yte = torch.from_numpy(data[\"Yte\"]).type(torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, input_dim, d_model, nhead, num_layers, output_dim):\n",
    "        super(TransformerModel, self).__init__()\n",
    "\n",
    "        # Embedding layer\n",
    "        self.embedding = nn.Embedding(input_dim, d_model)\n",
    "\n",
    "        # Transformer model\n",
    "        self.transformer = nn.Transformer(d_model=d_model, nhead=nhead, num_encoder_layers=num_layers,\n",
    "                                          num_decoder_layers=num_layers)\n",
    "\n",
    "        # Linear output layer\n",
    "        self.output = nn.Linear(d_model, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Prepare the input tensor for the transformer model\n",
    "        x = self.embedding(x)\n",
    "        x = x.permute(1, 0, 2)\n",
    "\n",
    "        # Forward propagate transformer model\n",
    "        out = self.transformer(x, x)\n",
    "\n",
    "        # Decode the output of the transformer model\n",
    "        out = self.output(out[-1, :, :])\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArithmeticExpressionDataset(Dataset):\n",
    "    def __init__(self, X, Y):\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.X[idx]\n",
    "        y = self.Y[idx]\n",
    "        return x, y\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(Xtr, Ytr, test_size=0.2)\n",
    "\n",
    "train_dataset = ArithmeticExpressionDataset(X_train, Y_train)\n",
    "val_dataset = ArithmeticExpressionDataset(X_val, Y_val)\n",
    "\n",
    "# Create data loaders for training and validation\n",
    "batch_size = 1024\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_vocab_size=1\n",
    "num_features=13\n",
    "d_model=64\n",
    "nhead=4\n",
    "num_layers=4\n",
    "model = TransformerModel(num_features,d_model,nhead,num_layers,output_vocab_size).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-2)\n",
    "\n",
    "# Define the loss function\n",
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1000], Train Loss: 207282.8617, Val Loss: 191866.3227\n",
      "Epoch [2/1000], Train Loss: 188633.6627, Val Loss: 188994.8092\n",
      "Epoch [3/1000], Train Loss: 188312.2085, Val Loss: 188953.6089\n",
      "Epoch [4/1000], Train Loss: 188307.5800, Val Loss: 188965.2516\n",
      "Epoch [5/1000], Train Loss: 188304.2548, Val Loss: 188953.9068\n",
      "Epoch [6/1000], Train Loss: 188295.5712, Val Loss: 188984.6981\n",
      "Epoch [7/1000], Train Loss: 188305.5356, Val Loss: 188972.2552\n",
      "Epoch [8/1000], Train Loss: 188297.2342, Val Loss: 188987.3893\n",
      "Epoch [9/1000], Train Loss: 188301.6283, Val Loss: 188955.4251\n",
      "Epoch [10/1000], Train Loss: 188304.5099, Val Loss: 189016.3532\n",
      "Epoch [11/1000], Train Loss: 188311.2674, Val Loss: 188954.1896\n",
      "Epoch [12/1000], Train Loss: 188299.0313, Val Loss: 188970.7957\n",
      "Epoch [13/1000], Train Loss: 188306.1633, Val Loss: 188955.6313\n",
      "Epoch [14/1000], Train Loss: 188301.5314, Val Loss: 188953.4036\n",
      "Epoch [15/1000], Train Loss: 188307.9870, Val Loss: 188954.2195\n",
      "Epoch [16/1000], Train Loss: 188308.5697, Val Loss: 188954.9598\n",
      "Epoch [17/1000], Train Loss: 188309.7113, Val Loss: 188958.4302\n",
      "Epoch [18/1000], Train Loss: 188318.9949, Val Loss: 189055.8417\n",
      "Epoch [19/1000], Train Loss: 188356.4945, Val Loss: 188954.9115\n",
      "Epoch [20/1000], Train Loss: 188295.0343, Val Loss: 188952.9817\n",
      "Epoch [21/1000], Train Loss: 188293.6753, Val Loss: 188952.7807\n",
      "Epoch [22/1000], Train Loss: 188306.0664, Val Loss: 189014.8545\n",
      "Epoch [23/1000], Train Loss: 188301.8473, Val Loss: 188955.2060\n",
      "Epoch [24/1000], Train Loss: 188294.3078, Val Loss: 188957.1791\n",
      "Epoch [25/1000], Train Loss: 188309.8760, Val Loss: 188959.6686\n",
      "Epoch [26/1000], Train Loss: 188300.7274, Val Loss: 188954.9648\n",
      "Epoch [27/1000], Train Loss: 188309.9977, Val Loss: 188952.8483\n",
      "Epoch [28/1000], Train Loss: 188300.7058, Val Loss: 188968.7398\n",
      "Epoch [29/1000], Train Loss: 188297.2308, Val Loss: 188965.4178\n",
      "Epoch [30/1000], Train Loss: 188311.2014, Val Loss: 188954.3013\n",
      "Epoch [31/1000], Train Loss: 188302.3193, Val Loss: 188956.1958\n",
      "Epoch [32/1000], Train Loss: 188321.3341, Val Loss: 188966.7195\n",
      "Epoch [33/1000], Train Loss: 188294.9928, Val Loss: 188991.1592\n",
      "Epoch [34/1000], Train Loss: 188312.2878, Val Loss: 188995.8233\n",
      "Epoch [35/1000], Train Loss: 188312.7253, Val Loss: 188965.1405\n",
      "Epoch [36/1000], Train Loss: 188300.9254, Val Loss: 188953.2827\n",
      "Epoch [37/1000], Train Loss: 188331.1119, Val Loss: 188980.6670\n",
      "Epoch [38/1000], Train Loss: 188300.4313, Val Loss: 188983.3497\n",
      "Epoch [39/1000], Train Loss: 188312.9693, Val Loss: 188953.0030\n",
      "Epoch [40/1000], Train Loss: 188298.4211, Val Loss: 189011.2445\n",
      "Epoch [41/1000], Train Loss: 188312.6992, Val Loss: 189012.0155\n",
      "Epoch [42/1000], Train Loss: 188313.1998, Val Loss: 188961.6762\n",
      "Epoch [43/1000], Train Loss: 188315.8851, Val Loss: 188957.7861\n",
      "Epoch [44/1000], Train Loss: 188293.7723, Val Loss: 188979.0277\n",
      "Epoch [45/1000], Train Loss: 188328.5960, Val Loss: 188959.6315\n",
      "Epoch [46/1000], Train Loss: 188306.2452, Val Loss: 188952.7822\n",
      "Epoch [47/1000], Train Loss: 188307.8055, Val Loss: 188969.2675\n",
      "Epoch [48/1000], Train Loss: 188306.2062, Val Loss: 188984.1041\n",
      "Epoch [49/1000], Train Loss: 188300.8502, Val Loss: 188966.3326\n",
      "Epoch [50/1000], Train Loss: 188310.7491, Val Loss: 188960.4827\n",
      "Epoch [51/1000], Train Loss: 188321.7006, Val Loss: 188959.6313\n",
      "Epoch [52/1000], Train Loss: 188333.9249, Val Loss: 188954.7344\n",
      "Epoch [53/1000], Train Loss: 188318.8614, Val Loss: 188952.9142\n",
      "Epoch [54/1000], Train Loss: 188338.8809, Val Loss: 188953.3895\n",
      "Epoch [55/1000], Train Loss: 188342.2752, Val Loss: 188995.6801\n",
      "Epoch [56/1000], Train Loss: 188307.1802, Val Loss: 188956.8516\n",
      "Epoch [57/1000], Train Loss: 188295.2674, Val Loss: 188952.7946\n",
      "Epoch [58/1000], Train Loss: 188300.2777, Val Loss: 188954.0477\n",
      "Epoch [59/1000], Train Loss: 188298.5101, Val Loss: 188953.6892\n",
      "Epoch [60/1000], Train Loss: 188315.5424, Val Loss: 188953.4604\n",
      "Epoch [61/1000], Train Loss: 188293.2196, Val Loss: 188953.8235\n",
      "Epoch [62/1000], Train Loss: 188312.7744, Val Loss: 188979.9893\n",
      "Epoch [63/1000], Train Loss: 188295.1461, Val Loss: 189020.4615\n",
      "Epoch [64/1000], Train Loss: 188312.5246, Val Loss: 188967.5542\n",
      "Epoch [65/1000], Train Loss: 188311.2434, Val Loss: 188981.8589\n",
      "Epoch [66/1000], Train Loss: 188296.4736, Val Loss: 188956.8242\n",
      "Epoch [67/1000], Train Loss: 188311.5612, Val Loss: 188955.1226\n",
      "Epoch [68/1000], Train Loss: 188299.5847, Val Loss: 189009.5512\n",
      "Epoch [69/1000], Train Loss: 188318.5230, Val Loss: 188955.2662\n",
      "Epoch [70/1000], Train Loss: 188309.2910, Val Loss: 188968.4302\n",
      "Epoch [71/1000], Train Loss: 188308.6280, Val Loss: 188989.0386\n",
      "Epoch [72/1000], Train Loss: 188310.0199, Val Loss: 188961.2492\n",
      "Epoch [73/1000], Train Loss: 188322.0221, Val Loss: 188977.0830\n",
      "Epoch [74/1000], Train Loss: 188297.1963, Val Loss: 188976.5436\n",
      "Epoch [75/1000], Train Loss: 188298.6687, Val Loss: 188991.8408\n",
      "Epoch [76/1000], Train Loss: 188299.2636, Val Loss: 188964.5335\n",
      "Epoch [77/1000], Train Loss: 188318.5627, Val Loss: 188962.7009\n",
      "Epoch [78/1000], Train Loss: 188313.4548, Val Loss: 188961.4731\n",
      "Epoch [79/1000], Train Loss: 188296.3740, Val Loss: 188970.3447\n",
      "Epoch [80/1000], Train Loss: 188312.6728, Val Loss: 188964.1683\n",
      "Epoch [81/1000], Train Loss: 188298.0295, Val Loss: 188965.4231\n",
      "Epoch [82/1000], Train Loss: 188286.2289, Val Loss: 189050.4252\n",
      "Epoch [83/1000], Train Loss: 188339.6305, Val Loss: 189101.0795\n",
      "Epoch [84/1000], Train Loss: 188292.7611, Val Loss: 188954.0357\n",
      "Epoch [85/1000], Train Loss: 188318.4690, Val Loss: 189051.4573\n",
      "Epoch [86/1000], Train Loss: 188343.6852, Val Loss: 188962.4036\n",
      "Epoch [87/1000], Train Loss: 188297.8886, Val Loss: 188953.7931\n",
      "Epoch [88/1000], Train Loss: 188298.8240, Val Loss: 188966.4170\n",
      "Epoch [89/1000], Train Loss: 188313.5030, Val Loss: 188957.9386\n",
      "Epoch [90/1000], Train Loss: 188320.7203, Val Loss: 188985.5283\n",
      "Epoch [91/1000], Train Loss: 188323.4982, Val Loss: 188954.2333\n",
      "Epoch [92/1000], Train Loss: 188294.1047, Val Loss: 188961.1898\n",
      "Epoch [93/1000], Train Loss: 188304.7547, Val Loss: 188973.4991\n",
      "Epoch [94/1000], Train Loss: 188297.4532, Val Loss: 188957.3639\n",
      "Epoch [95/1000], Train Loss: 188327.8552, Val Loss: 188952.8066\n",
      "Epoch [96/1000], Train Loss: 188301.1669, Val Loss: 188952.7698\n",
      "Epoch [97/1000], Train Loss: 188324.7273, Val Loss: 188978.7165\n",
      "Epoch [98/1000], Train Loss: 188318.1982, Val Loss: 188956.5832\n",
      "Epoch [99/1000], Train Loss: 188305.9179, Val Loss: 188953.1653\n",
      "Epoch [100/1000], Train Loss: 188317.7884, Val Loss: 188981.1441\n",
      "Epoch [101/1000], Train Loss: 188321.3098, Val Loss: 188955.2686\n",
      "Epoch [102/1000], Train Loss: 188312.6848, Val Loss: 188959.4980\n",
      "Epoch [103/1000], Train Loss: 188305.3777, Val Loss: 188957.5820\n",
      "Epoch [104/1000], Train Loss: 188303.4002, Val Loss: 188965.5574\n",
      "Epoch [105/1000], Train Loss: 188319.8755, Val Loss: 188952.7575\n",
      "Epoch [106/1000], Train Loss: 188283.3735, Val Loss: 188975.1989\n",
      "Epoch [107/1000], Train Loss: 188319.6453, Val Loss: 188957.0075\n",
      "Epoch [108/1000], Train Loss: 188299.3379, Val Loss: 188978.8045\n",
      "Epoch [109/1000], Train Loss: 188325.5286, Val Loss: 188970.9831\n",
      "Epoch [110/1000], Train Loss: 188339.2208, Val Loss: 188981.4083\n",
      "Epoch [111/1000], Train Loss: 188299.4529, Val Loss: 188953.0623\n",
      "Epoch [112/1000], Train Loss: 188320.0036, Val Loss: 188956.2081\n",
      "Epoch [113/1000], Train Loss: 188302.2240, Val Loss: 188994.4643\n",
      "Epoch [114/1000], Train Loss: 188334.9067, Val Loss: 188973.9625\n",
      "Epoch [115/1000], Train Loss: 188295.6372, Val Loss: 189013.7794\n",
      "Epoch [116/1000], Train Loss: 188298.8164, Val Loss: 188956.9166\n",
      "Epoch [117/1000], Train Loss: 188300.1548, Val Loss: 188959.0252\n",
      "Epoch [118/1000], Train Loss: 188304.2086, Val Loss: 188952.9842\n",
      "Epoch [119/1000], Train Loss: 188307.7703, Val Loss: 188953.8455\n",
      "Epoch [120/1000], Train Loss: 188317.0799, Val Loss: 188956.5185\n",
      "Epoch [121/1000], Train Loss: 188315.3969, Val Loss: 188985.9819\n",
      "Epoch [122/1000], Train Loss: 188296.9500, Val Loss: 188993.9868\n",
      "Epoch [123/1000], Train Loss: 188317.6211, Val Loss: 188952.9458\n",
      "Epoch [124/1000], Train Loss: 188324.2637, Val Loss: 188986.4902\n",
      "Epoch [125/1000], Train Loss: 188307.3003, Val Loss: 188954.1647\n",
      "Epoch [126/1000], Train Loss: 188317.1116, Val Loss: 188963.3633\n",
      "Epoch [127/1000], Train Loss: 188312.7896, Val Loss: 188952.8080\n",
      "Epoch [128/1000], Train Loss: 188314.4497, Val Loss: 188953.2152\n",
      "Epoch [129/1000], Train Loss: 188340.1267, Val Loss: 188954.2168\n",
      "Epoch [130/1000], Train Loss: 188296.0212, Val Loss: 188953.9675\n",
      "Epoch [131/1000], Train Loss: 188304.3100, Val Loss: 188975.0692\n",
      "Epoch [132/1000], Train Loss: 188281.7763, Val Loss: 188964.0839\n",
      "Epoch [133/1000], Train Loss: 188327.4924, Val Loss: 188955.8321\n",
      "Epoch [134/1000], Train Loss: 188324.1460, Val Loss: 188952.7976\n",
      "Epoch [135/1000], Train Loss: 188321.5512, Val Loss: 188952.7645\n",
      "Epoch [136/1000], Train Loss: 188296.6372, Val Loss: 188958.0701\n",
      "Epoch [137/1000], Train Loss: 188297.8009, Val Loss: 188952.7854\n",
      "Epoch [138/1000], Train Loss: 188325.7128, Val Loss: 188958.0570\n",
      "Epoch [139/1000], Train Loss: 188305.2110, Val Loss: 188954.7413\n",
      "Epoch [140/1000], Train Loss: 188325.3303, Val Loss: 188953.8072\n",
      "Epoch [141/1000], Train Loss: 188330.2052, Val Loss: 188974.5050\n",
      "Epoch [142/1000], Train Loss: 188360.9194, Val Loss: 189200.4672\n",
      "Epoch [143/1000], Train Loss: 188341.4798, Val Loss: 188953.8307\n",
      "Epoch [144/1000], Train Loss: 188324.3789, Val Loss: 188963.9853\n",
      "Epoch [145/1000], Train Loss: 188304.4080, Val Loss: 188969.0909\n",
      "Epoch [146/1000], Train Loss: 188296.7605, Val Loss: 188999.5725\n",
      "Epoch [147/1000], Train Loss: 188308.3955, Val Loss: 188953.7921\n",
      "Epoch [148/1000], Train Loss: 188321.5714, Val Loss: 188960.3149\n",
      "Epoch [149/1000], Train Loss: 188299.5862, Val Loss: 188953.9187\n",
      "Epoch [150/1000], Train Loss: 188309.0395, Val Loss: 188955.9148\n",
      "Epoch [151/1000], Train Loss: 188310.3107, Val Loss: 188953.9391\n",
      "Epoch [152/1000], Train Loss: 188297.3878, Val Loss: 189006.0019\n",
      "Epoch [153/1000], Train Loss: 188317.9235, Val Loss: 188983.7718\n",
      "Epoch [154/1000], Train Loss: 188309.9285, Val Loss: 188953.6316\n",
      "Epoch [155/1000], Train Loss: 188314.6028, Val Loss: 188967.3814\n",
      "Epoch [156/1000], Train Loss: 188299.3865, Val Loss: 189006.9097\n",
      "Epoch [157/1000], Train Loss: 188324.4308, Val Loss: 188973.0846\n",
      "Epoch [158/1000], Train Loss: 188308.9408, Val Loss: 188961.0467\n",
      "Epoch [159/1000], Train Loss: 188327.6074, Val Loss: 188954.3932\n",
      "Epoch [160/1000], Train Loss: 188301.9045, Val Loss: 189040.6688\n",
      "Epoch [161/1000], Train Loss: 188324.9277, Val Loss: 188998.0587\n",
      "Epoch [162/1000], Train Loss: 188328.3013, Val Loss: 188966.5790\n",
      "Epoch [163/1000], Train Loss: 188300.6262, Val Loss: 188959.0208\n",
      "Epoch [164/1000], Train Loss: 188324.6902, Val Loss: 188976.1306\n",
      "Epoch [165/1000], Train Loss: 188305.2890, Val Loss: 188973.6448\n",
      "Epoch [166/1000], Train Loss: 188293.5252, Val Loss: 188964.5972\n",
      "Epoch [167/1000], Train Loss: 188293.5887, Val Loss: 188996.9053\n",
      "Epoch [168/1000], Train Loss: 188325.2578, Val Loss: 188953.4470\n",
      "Epoch [169/1000], Train Loss: 188306.0199, Val Loss: 188961.0445\n",
      "Epoch [170/1000], Train Loss: 188325.1718, Val Loss: 188957.2081\n",
      "Epoch [171/1000], Train Loss: 188311.8072, Val Loss: 188953.3744\n",
      "Epoch [172/1000], Train Loss: 188298.4365, Val Loss: 188998.9805\n",
      "Epoch [173/1000], Train Loss: 188295.9526, Val Loss: 188971.5044\n",
      "Epoch [174/1000], Train Loss: 188309.7556, Val Loss: 188975.3463\n",
      "Epoch [175/1000], Train Loss: 188308.2131, Val Loss: 188977.4939\n",
      "Epoch [176/1000], Train Loss: 188322.7901, Val Loss: 188953.4904\n",
      "Epoch [177/1000], Train Loss: 188326.7151, Val Loss: 188953.6349\n",
      "Epoch [178/1000], Train Loss: 188300.0386, Val Loss: 188953.0826\n",
      "Epoch [179/1000], Train Loss: 188305.6815, Val Loss: 188953.2213\n",
      "Epoch [180/1000], Train Loss: 188318.0808, Val Loss: 188956.6764\n",
      "Epoch [181/1000], Train Loss: 188309.3140, Val Loss: 188962.3774\n",
      "Epoch [182/1000], Train Loss: 188315.2358, Val Loss: 188955.2466\n",
      "Epoch [183/1000], Train Loss: 188292.7553, Val Loss: 188987.2860\n",
      "Epoch [184/1000], Train Loss: 188295.1828, Val Loss: 189007.0373\n",
      "Epoch [185/1000], Train Loss: 188305.3551, Val Loss: 188970.9840\n",
      "Epoch [186/1000], Train Loss: 188318.3648, Val Loss: 188952.7871\n",
      "Epoch [187/1000], Train Loss: 188284.7337, Val Loss: 188962.9180\n",
      "Epoch [188/1000], Train Loss: 188334.8557, Val Loss: 188996.3822\n",
      "Epoch [189/1000], Train Loss: 188314.5024, Val Loss: 188988.7235\n",
      "Epoch [190/1000], Train Loss: 188293.4911, Val Loss: 188965.6675\n",
      "Epoch [191/1000], Train Loss: 188289.7136, Val Loss: 188965.2835\n",
      "Epoch [192/1000], Train Loss: 188301.8621, Val Loss: 188955.9772\n",
      "Epoch [193/1000], Train Loss: 188311.7623, Val Loss: 188963.4851\n",
      "Epoch [194/1000], Train Loss: 188291.4927, Val Loss: 188954.1351\n",
      "Epoch [195/1000], Train Loss: 188325.2693, Val Loss: 188959.4424\n",
      "Epoch [196/1000], Train Loss: 188306.2596, Val Loss: 188954.6527\n",
      "Epoch [197/1000], Train Loss: 188307.6342, Val Loss: 188954.2768\n",
      "Epoch [198/1000], Train Loss: 188303.2758, Val Loss: 188953.0920\n",
      "Epoch [199/1000], Train Loss: 188305.3589, Val Loss: 188967.9933\n",
      "Epoch [200/1000], Train Loss: 188300.6991, Val Loss: 188952.7628\n",
      "Epoch [201/1000], Train Loss: 188340.0812, Val Loss: 189019.6277\n",
      "Epoch [202/1000], Train Loss: 188306.4747, Val Loss: 188957.2026\n",
      "Epoch [203/1000], Train Loss: 188302.4814, Val Loss: 188961.8181\n",
      "Epoch [204/1000], Train Loss: 188312.7443, Val Loss: 188962.0640\n",
      "Epoch [205/1000], Train Loss: 188298.5045, Val Loss: 188953.7132\n",
      "Epoch [206/1000], Train Loss: 188300.5333, Val Loss: 189069.3706\n",
      "Epoch [207/1000], Train Loss: 188332.4158, Val Loss: 188962.1983\n",
      "Epoch [208/1000], Train Loss: 188307.6424, Val Loss: 188952.7644\n",
      "Epoch [209/1000], Train Loss: 188310.6027, Val Loss: 188958.0509\n",
      "Epoch [210/1000], Train Loss: 188296.8537, Val Loss: 188957.0259\n",
      "Epoch [211/1000], Train Loss: 188318.2827, Val Loss: 188958.7424\n",
      "Epoch [212/1000], Train Loss: 188306.2549, Val Loss: 188953.2343\n",
      "Epoch [213/1000], Train Loss: 188303.6130, Val Loss: 188988.3524\n",
      "Epoch [214/1000], Train Loss: 188303.6748, Val Loss: 188954.1072\n",
      "Epoch [215/1000], Train Loss: 188309.3727, Val Loss: 188960.8888\n",
      "Epoch [216/1000], Train Loss: 188305.7195, Val Loss: 189002.6368\n",
      "Epoch [217/1000], Train Loss: 188300.5873, Val Loss: 188953.6349\n",
      "Epoch [218/1000], Train Loss: 188321.2239, Val Loss: 188963.6297\n",
      "Epoch [219/1000], Train Loss: 188316.1336, Val Loss: 188966.6304\n",
      "Epoch [220/1000], Train Loss: 188311.4918, Val Loss: 188995.3458\n",
      "Epoch [221/1000], Train Loss: 188323.9566, Val Loss: 188958.3581\n",
      "Epoch [222/1000], Train Loss: 188296.0836, Val Loss: 188952.8112\n",
      "Epoch [223/1000], Train Loss: 188295.9157, Val Loss: 188953.6544\n",
      "Epoch [224/1000], Train Loss: 188328.7981, Val Loss: 188973.0888\n",
      "Epoch [225/1000], Train Loss: 188325.2796, Val Loss: 188954.6955\n",
      "Epoch [226/1000], Train Loss: 188301.1848, Val Loss: 188984.9924\n",
      "Epoch [227/1000], Train Loss: 188307.9201, Val Loss: 188958.2773\n",
      "Epoch [228/1000], Train Loss: 188310.8212, Val Loss: 189003.4977\n",
      "Epoch [229/1000], Train Loss: 188333.2405, Val Loss: 188953.1848\n",
      "Epoch [230/1000], Train Loss: 188304.3778, Val Loss: 188972.8676\n",
      "Epoch [231/1000], Train Loss: 188316.0580, Val Loss: 188957.3822\n",
      "Epoch [232/1000], Train Loss: 188336.0003, Val Loss: 188989.0622\n",
      "Epoch [233/1000], Train Loss: 188295.8323, Val Loss: 188964.4714\n",
      "Epoch [234/1000], Train Loss: 188303.2566, Val Loss: 188953.6758\n",
      "Epoch [235/1000], Train Loss: 188320.1354, Val Loss: 188991.5537\n",
      "Epoch [236/1000], Train Loss: 188300.2857, Val Loss: 188962.7822\n",
      "Epoch [237/1000], Train Loss: 188294.0191, Val Loss: 188953.5033\n",
      "Epoch [238/1000], Train Loss: 188306.0242, Val Loss: 188956.1107\n",
      "Epoch [239/1000], Train Loss: 188317.1878, Val Loss: 188962.4193\n",
      "Epoch [240/1000], Train Loss: 188311.7567, Val Loss: 188953.4942\n",
      "Epoch [241/1000], Train Loss: 188321.1797, Val Loss: 188990.0909\n",
      "Epoch [242/1000], Train Loss: 188309.7475, Val Loss: 188965.8660\n",
      "Epoch [243/1000], Train Loss: 188295.8458, Val Loss: 188962.3126\n",
      "Epoch [244/1000], Train Loss: 188310.8438, Val Loss: 188981.1191\n",
      "Epoch [245/1000], Train Loss: 188297.0361, Val Loss: 188967.0830\n",
      "Epoch [246/1000], Train Loss: 188303.7891, Val Loss: 188953.0630\n",
      "Epoch [247/1000], Train Loss: 188299.7397, Val Loss: 188953.4353\n",
      "Epoch [248/1000], Train Loss: 188299.7051, Val Loss: 188976.0258\n",
      "Epoch [249/1000], Train Loss: 188306.4219, Val Loss: 188952.9191\n",
      "Epoch [250/1000], Train Loss: 188350.5020, Val Loss: 188954.5022\n",
      "Epoch [251/1000], Train Loss: 188310.6003, Val Loss: 188969.5984\n",
      "Epoch [252/1000], Train Loss: 188294.7587, Val Loss: 188993.2144\n",
      "Epoch [253/1000], Train Loss: 188301.3407, Val Loss: 188952.8854\n",
      "Epoch [254/1000], Train Loss: 188302.6896, Val Loss: 188953.9956\n",
      "Epoch [255/1000], Train Loss: 188316.9805, Val Loss: 188970.9945\n",
      "Epoch [256/1000], Train Loss: 188318.1735, Val Loss: 188956.9525\n",
      "Epoch [257/1000], Train Loss: 188304.0488, Val Loss: 188967.7120\n",
      "Epoch [258/1000], Train Loss: 188313.1539, Val Loss: 189025.7368\n",
      "Epoch [259/1000], Train Loss: 188312.3293, Val Loss: 188954.0190\n",
      "Epoch [260/1000], Train Loss: 188297.6954, Val Loss: 189070.5413\n",
      "Epoch [261/1000], Train Loss: 188298.5751, Val Loss: 188965.9247\n",
      "Epoch [262/1000], Train Loss: 188298.3280, Val Loss: 188981.9959\n",
      "Epoch [263/1000], Train Loss: 188303.3692, Val Loss: 188963.8479\n",
      "Epoch [264/1000], Train Loss: 188305.9672, Val Loss: 188956.6975\n",
      "Epoch [265/1000], Train Loss: 188304.0744, Val Loss: 188957.6180\n",
      "Epoch [266/1000], Train Loss: 188317.7827, Val Loss: 188955.7327\n",
      "Epoch [267/1000], Train Loss: 188304.4055, Val Loss: 188954.8605\n",
      "Epoch [268/1000], Train Loss: 188315.8942, Val Loss: 189040.5752\n",
      "Epoch [269/1000], Train Loss: 188305.8982, Val Loss: 188996.9043\n",
      "Epoch [270/1000], Train Loss: 188291.3301, Val Loss: 188953.2989\n",
      "Epoch [271/1000], Train Loss: 188291.9382, Val Loss: 188979.1176\n",
      "Epoch [272/1000], Train Loss: 188301.4580, Val Loss: 188954.2071\n",
      "Epoch [273/1000], Train Loss: 188329.7983, Val Loss: 188956.2607\n",
      "Epoch [274/1000], Train Loss: 188318.5602, Val Loss: 188953.3281\n",
      "Epoch [275/1000], Train Loss: 188310.7318, Val Loss: 188953.5483\n",
      "Epoch [276/1000], Train Loss: 188308.5761, Val Loss: 188953.3133\n",
      "Epoch [277/1000], Train Loss: 188306.5594, Val Loss: 188957.8644\n",
      "Epoch [278/1000], Train Loss: 188326.0767, Val Loss: 188955.7778\n",
      "Epoch [279/1000], Train Loss: 188296.5043, Val Loss: 188991.7859\n",
      "Epoch [280/1000], Train Loss: 188308.9290, Val Loss: 188956.6114\n",
      "Epoch [281/1000], Train Loss: 188341.3148, Val Loss: 188965.6329\n",
      "Epoch [282/1000], Train Loss: 188355.7237, Val Loss: 188979.6486\n",
      "Epoch [283/1000], Train Loss: 188310.9623, Val Loss: 188954.0457\n",
      "Epoch [284/1000], Train Loss: 188293.6011, Val Loss: 188964.4889\n",
      "Epoch [285/1000], Train Loss: 188297.8435, Val Loss: 188953.1272\n",
      "Epoch [286/1000], Train Loss: 188314.5847, Val Loss: 188971.3728\n",
      "Epoch [287/1000], Train Loss: 188310.6753, Val Loss: 188955.4815\n",
      "Epoch [288/1000], Train Loss: 188319.0795, Val Loss: 188961.0604\n",
      "Epoch [289/1000], Train Loss: 188287.1191, Val Loss: 188953.6785\n",
      "Epoch [290/1000], Train Loss: 188300.1135, Val Loss: 188954.7831\n",
      "Epoch [291/1000], Train Loss: 188324.7308, Val Loss: 189047.1219\n",
      "Epoch [292/1000], Train Loss: 188336.2206, Val Loss: 188952.8001\n",
      "Epoch [293/1000], Train Loss: 188314.6660, Val Loss: 188960.9434\n",
      "Epoch [294/1000], Train Loss: 188315.8579, Val Loss: 188982.4666\n",
      "Epoch [295/1000], Train Loss: 188307.8573, Val Loss: 188965.5627\n",
      "Epoch [296/1000], Train Loss: 188303.7719, Val Loss: 188953.8669\n",
      "Epoch [297/1000], Train Loss: 188299.9974, Val Loss: 188968.4777\n",
      "Epoch [298/1000], Train Loss: 188310.6238, Val Loss: 188957.4176\n",
      "Epoch [299/1000], Train Loss: 188295.7186, Val Loss: 188971.8911\n",
      "Epoch [300/1000], Train Loss: 188309.7425, Val Loss: 188952.8758\n",
      "Epoch [301/1000], Train Loss: 188300.8834, Val Loss: 188954.1513\n",
      "Epoch [302/1000], Train Loss: 188308.9246, Val Loss: 188958.1860\n",
      "Epoch [303/1000], Train Loss: 188305.8718, Val Loss: 188996.2142\n",
      "Epoch [304/1000], Train Loss: 188315.1067, Val Loss: 189091.7392\n",
      "Epoch [305/1000], Train Loss: 188347.0074, Val Loss: 188958.5666\n",
      "Epoch [306/1000], Train Loss: 188283.3571, Val Loss: 189042.1805\n",
      "Epoch [307/1000], Train Loss: 188313.8676, Val Loss: 188957.7120\n",
      "Epoch [308/1000], Train Loss: 188313.2799, Val Loss: 188993.6607\n",
      "Epoch [309/1000], Train Loss: 188289.1136, Val Loss: 188953.0941\n",
      "Epoch [310/1000], Train Loss: 188303.9275, Val Loss: 188968.0696\n",
      "Epoch [311/1000], Train Loss: 188302.6756, Val Loss: 188952.9761\n",
      "Epoch [312/1000], Train Loss: 188286.3328, Val Loss: 188953.8655\n",
      "Epoch [313/1000], Train Loss: 188300.0207, Val Loss: 188974.1905\n",
      "Epoch [314/1000], Train Loss: 188308.5485, Val Loss: 188965.3747\n",
      "Epoch [315/1000], Train Loss: 188319.9746, Val Loss: 188998.9760\n",
      "Epoch [316/1000], Train Loss: 188294.8559, Val Loss: 188962.4440\n",
      "Epoch [317/1000], Train Loss: 188302.0226, Val Loss: 188955.1050\n",
      "Epoch [318/1000], Train Loss: 188308.4824, Val Loss: 188954.3626\n",
      "Epoch [319/1000], Train Loss: 188327.9076, Val Loss: 188953.2880\n",
      "Epoch [320/1000], Train Loss: 188319.6731, Val Loss: 188952.8587\n",
      "Epoch [321/1000], Train Loss: 188319.2754, Val Loss: 189047.0742\n",
      "Epoch [322/1000], Train Loss: 188309.7368, Val Loss: 188965.5227\n",
      "Epoch [323/1000], Train Loss: 188313.8543, Val Loss: 188977.0605\n",
      "Epoch [324/1000], Train Loss: 188340.3742, Val Loss: 188973.3539\n",
      "Epoch [325/1000], Train Loss: 188306.0383, Val Loss: 188963.8902\n",
      "Epoch [326/1000], Train Loss: 188316.4580, Val Loss: 188978.1059\n",
      "Epoch [327/1000], Train Loss: 188301.0976, Val Loss: 188956.9370\n",
      "Epoch [328/1000], Train Loss: 188299.6044, Val Loss: 188954.5896\n",
      "Epoch [329/1000], Train Loss: 188330.6263, Val Loss: 188962.1903\n",
      "Epoch [330/1000], Train Loss: 188293.6763, Val Loss: 189003.4338\n",
      "Epoch [331/1000], Train Loss: 188310.9076, Val Loss: 188994.7683\n",
      "Epoch [332/1000], Train Loss: 188323.1016, Val Loss: 188996.6260\n",
      "Epoch [333/1000], Train Loss: 188327.8289, Val Loss: 188969.8923\n",
      "Epoch [334/1000], Train Loss: 188305.7030, Val Loss: 188966.5402\n",
      "Epoch [335/1000], Train Loss: 188294.9001, Val Loss: 188958.0507\n",
      "Epoch [336/1000], Train Loss: 188292.4748, Val Loss: 188963.4605\n",
      "Epoch [337/1000], Train Loss: 188321.1798, Val Loss: 189009.2845\n",
      "Epoch [338/1000], Train Loss: 188317.6939, Val Loss: 188956.1790\n",
      "Epoch [339/1000], Train Loss: 188295.0180, Val Loss: 188952.7795\n",
      "Epoch [340/1000], Train Loss: 188317.1712, Val Loss: 188971.6270\n",
      "Epoch [341/1000], Train Loss: 188309.9436, Val Loss: 188953.6322\n",
      "Epoch [342/1000], Train Loss: 188309.8094, Val Loss: 189005.6534\n",
      "Epoch [343/1000], Train Loss: 188319.9550, Val Loss: 188954.1528\n",
      "Epoch [344/1000], Train Loss: 188314.5172, Val Loss: 188953.4804\n",
      "Epoch [345/1000], Train Loss: 188321.5958, Val Loss: 188996.9316\n",
      "Epoch [346/1000], Train Loss: 188299.4696, Val Loss: 188989.2791\n",
      "Epoch [347/1000], Train Loss: 188292.7622, Val Loss: 188953.2666\n",
      "Epoch [348/1000], Train Loss: 188292.8668, Val Loss: 188959.7959\n",
      "Epoch [349/1000], Train Loss: 188306.8363, Val Loss: 188975.4694\n",
      "Epoch [350/1000], Train Loss: 188309.9555, Val Loss: 189017.7922\n",
      "Epoch [351/1000], Train Loss: 188307.6615, Val Loss: 188953.2470\n",
      "Epoch [352/1000], Train Loss: 188298.1781, Val Loss: 189034.9158\n",
      "Epoch [353/1000], Train Loss: 188297.7777, Val Loss: 188952.8120\n",
      "Epoch [354/1000], Train Loss: 188304.8800, Val Loss: 188955.4901\n",
      "Epoch [355/1000], Train Loss: 188297.2745, Val Loss: 188964.0394\n",
      "Epoch [356/1000], Train Loss: 188306.7335, Val Loss: 188956.0571\n",
      "Epoch [357/1000], Train Loss: 188310.8209, Val Loss: 188958.1915\n",
      "Epoch [358/1000], Train Loss: 188298.1411, Val Loss: 188959.9096\n",
      "Epoch [359/1000], Train Loss: 188293.2074, Val Loss: 189039.1219\n",
      "Epoch [360/1000], Train Loss: 188327.4107, Val Loss: 188959.4179\n",
      "Epoch [361/1000], Train Loss: 188297.1227, Val Loss: 188960.3650\n",
      "Epoch [362/1000], Train Loss: 188284.9503, Val Loss: 188965.3786\n",
      "Epoch [363/1000], Train Loss: 188306.1415, Val Loss: 188952.7814\n",
      "Epoch [364/1000], Train Loss: 188294.8442, Val Loss: 188978.5253\n",
      "Epoch [365/1000], Train Loss: 188300.6773, Val Loss: 188965.5759\n",
      "Epoch [366/1000], Train Loss: 188302.1017, Val Loss: 188963.5512\n",
      "Epoch [367/1000], Train Loss: 188309.1299, Val Loss: 188997.1376\n",
      "Epoch [368/1000], Train Loss: 188329.7052, Val Loss: 188954.9389\n",
      "Epoch [369/1000], Train Loss: 188303.4092, Val Loss: 188977.3760\n",
      "Epoch [370/1000], Train Loss: 188309.3777, Val Loss: 188956.7661\n",
      "Epoch [371/1000], Train Loss: 188327.5542, Val Loss: 188952.8753\n",
      "Epoch [372/1000], Train Loss: 188295.7595, Val Loss: 188982.6919\n",
      "Epoch [373/1000], Train Loss: 188297.5835, Val Loss: 188954.8969\n",
      "Epoch [374/1000], Train Loss: 188313.2794, Val Loss: 188955.7218\n",
      "Epoch [375/1000], Train Loss: 188292.3948, Val Loss: 189030.7558\n",
      "Epoch [376/1000], Train Loss: 188307.4171, Val Loss: 188995.4751\n",
      "Epoch [377/1000], Train Loss: 188328.4853, Val Loss: 188955.5069\n",
      "Epoch [378/1000], Train Loss: 188312.8684, Val Loss: 188979.8551\n",
      "Epoch [379/1000], Train Loss: 188303.6746, Val Loss: 188964.3994\n",
      "Epoch [380/1000], Train Loss: 188293.1811, Val Loss: 188952.8477\n",
      "Epoch [381/1000], Train Loss: 188317.3542, Val Loss: 188959.4063\n",
      "Epoch [382/1000], Train Loss: 188294.8478, Val Loss: 188956.8435\n",
      "Epoch [383/1000], Train Loss: 188299.8551, Val Loss: 189017.7358\n",
      "Epoch [384/1000], Train Loss: 188304.8308, Val Loss: 189012.0403\n",
      "Epoch [385/1000], Train Loss: 188320.6950, Val Loss: 188970.2717\n",
      "Epoch [386/1000], Train Loss: 188300.8793, Val Loss: 188971.7786\n",
      "Epoch [387/1000], Train Loss: 188300.3238, Val Loss: 188960.8811\n",
      "Epoch [388/1000], Train Loss: 188301.4617, Val Loss: 188959.9688\n",
      "Epoch [389/1000], Train Loss: 188296.1425, Val Loss: 188959.2288\n",
      "Epoch [390/1000], Train Loss: 188301.8412, Val Loss: 188978.7502\n",
      "Epoch [391/1000], Train Loss: 188310.5385, Val Loss: 188989.0844\n",
      "Epoch [392/1000], Train Loss: 188323.7080, Val Loss: 188952.7718\n",
      "Epoch [393/1000], Train Loss: 188314.6085, Val Loss: 188955.6089\n",
      "Epoch [394/1000], Train Loss: 188298.1169, Val Loss: 189052.0208\n",
      "Epoch [395/1000], Train Loss: 188320.8559, Val Loss: 188955.4720\n",
      "Epoch [396/1000], Train Loss: 188299.2349, Val Loss: 188959.5137\n",
      "Epoch [397/1000], Train Loss: 188327.5405, Val Loss: 188962.2355\n",
      "Epoch [398/1000], Train Loss: 188320.4805, Val Loss: 188993.0065\n",
      "Epoch [399/1000], Train Loss: 188307.9361, Val Loss: 188982.4469\n",
      "Epoch [400/1000], Train Loss: 188314.8320, Val Loss: 188955.5699\n",
      "Epoch [401/1000], Train Loss: 188302.8449, Val Loss: 188975.4295\n",
      "Epoch [402/1000], Train Loss: 188315.1369, Val Loss: 188957.6733\n",
      "Epoch [403/1000], Train Loss: 188299.6577, Val Loss: 188958.8586\n",
      "Epoch [404/1000], Train Loss: 188316.9810, Val Loss: 188964.7992\n",
      "Epoch [405/1000], Train Loss: 188305.0702, Val Loss: 188987.9912\n",
      "Epoch [406/1000], Train Loss: 188306.1410, Val Loss: 188953.5363\n",
      "Epoch [407/1000], Train Loss: 188301.6808, Val Loss: 188954.1032\n",
      "Epoch [408/1000], Train Loss: 188293.9336, Val Loss: 188973.7939\n",
      "Epoch [409/1000], Train Loss: 188299.5532, Val Loss: 188953.0062\n",
      "Epoch [410/1000], Train Loss: 188293.8832, Val Loss: 188958.7123\n",
      "Epoch [411/1000], Train Loss: 188294.9959, Val Loss: 188956.8750\n",
      "Epoch [412/1000], Train Loss: 188296.4962, Val Loss: 188958.9137\n",
      "Epoch [413/1000], Train Loss: 188305.2981, Val Loss: 188960.8969\n",
      "Epoch [414/1000], Train Loss: 188310.4648, Val Loss: 188957.6836\n",
      "Epoch [415/1000], Train Loss: 188302.9776, Val Loss: 188979.3073\n",
      "Epoch [416/1000], Train Loss: 188312.2861, Val Loss: 189007.0144\n",
      "Epoch [417/1000], Train Loss: 188293.3194, Val Loss: 188957.9479\n",
      "Epoch [418/1000], Train Loss: 188314.1607, Val Loss: 188965.7057\n",
      "Epoch [419/1000], Train Loss: 188313.8755, Val Loss: 188968.7255\n",
      "Epoch [420/1000], Train Loss: 188325.2768, Val Loss: 188963.5565\n",
      "Epoch [421/1000], Train Loss: 188306.3852, Val Loss: 188958.7283\n",
      "Epoch [422/1000], Train Loss: 188313.9570, Val Loss: 188963.4804\n",
      "Epoch [423/1000], Train Loss: 188313.9229, Val Loss: 188959.7941\n",
      "Epoch [424/1000], Train Loss: 188308.0843, Val Loss: 188953.2493\n",
      "Epoch [425/1000], Train Loss: 188286.1917, Val Loss: 188958.0686\n",
      "Epoch [426/1000], Train Loss: 188311.1768, Val Loss: 188978.1248\n",
      "Epoch [427/1000], Train Loss: 188326.4241, Val Loss: 189020.7326\n",
      "Epoch [428/1000], Train Loss: 188308.8837, Val Loss: 188961.7753\n",
      "Epoch [429/1000], Train Loss: 188322.3301, Val Loss: 188964.8535\n",
      "Epoch [430/1000], Train Loss: 188310.8452, Val Loss: 188953.6082\n",
      "Epoch [431/1000], Train Loss: 188295.2404, Val Loss: 188953.7796\n",
      "Epoch [432/1000], Train Loss: 188303.6082, Val Loss: 188960.0846\n",
      "Epoch [433/1000], Train Loss: 188301.3670, Val Loss: 188952.9782\n",
      "Epoch [434/1000], Train Loss: 188311.5947, Val Loss: 188963.1986\n",
      "Epoch [435/1000], Train Loss: 188306.6721, Val Loss: 188953.1262\n",
      "Epoch [436/1000], Train Loss: 188309.4105, Val Loss: 188954.5348\n",
      "Epoch [437/1000], Train Loss: 188315.8658, Val Loss: 188952.8106\n",
      "Epoch [438/1000], Train Loss: 188315.1087, Val Loss: 188966.9822\n",
      "Epoch [439/1000], Train Loss: 188303.5929, Val Loss: 189068.7187\n",
      "Epoch [440/1000], Train Loss: 188355.3996, Val Loss: 188956.9300\n",
      "Epoch [441/1000], Train Loss: 188325.7319, Val Loss: 189004.7066\n",
      "Epoch [442/1000], Train Loss: 188309.3360, Val Loss: 188975.3599\n",
      "Epoch [443/1000], Train Loss: 188306.2000, Val Loss: 188952.8101\n",
      "Epoch [444/1000], Train Loss: 188307.5876, Val Loss: 188972.7032\n",
      "Epoch [445/1000], Train Loss: 188308.8362, Val Loss: 188968.2519\n",
      "Epoch [446/1000], Train Loss: 188333.8576, Val Loss: 188991.9852\n",
      "Epoch [447/1000], Train Loss: 188326.1276, Val Loss: 188955.7245\n",
      "Epoch [448/1000], Train Loss: 188318.9904, Val Loss: 188964.0135\n",
      "Epoch [449/1000], Train Loss: 188300.6593, Val Loss: 188955.2686\n",
      "Epoch [450/1000], Train Loss: 188302.4069, Val Loss: 189060.9315\n",
      "Epoch [451/1000], Train Loss: 188319.0227, Val Loss: 188964.8273\n",
      "Epoch [452/1000], Train Loss: 188306.3920, Val Loss: 188967.2314\n",
      "Epoch [453/1000], Train Loss: 188298.0469, Val Loss: 189004.1559\n",
      "Epoch [454/1000], Train Loss: 188316.4227, Val Loss: 188954.5193\n",
      "Epoch [455/1000], Train Loss: 188303.4808, Val Loss: 189000.7159\n",
      "Epoch [456/1000], Train Loss: 188295.2727, Val Loss: 188952.7935\n",
      "Epoch [457/1000], Train Loss: 188307.0954, Val Loss: 188952.9975\n",
      "Epoch [458/1000], Train Loss: 188312.4944, Val Loss: 188985.3476\n",
      "Epoch [459/1000], Train Loss: 188313.6277, Val Loss: 188958.9463\n",
      "Epoch [460/1000], Train Loss: 188318.1880, Val Loss: 188954.7805\n",
      "Epoch [461/1000], Train Loss: 188316.7507, Val Loss: 188952.7886\n",
      "Epoch [462/1000], Train Loss: 188293.9525, Val Loss: 188978.4055\n",
      "Epoch [463/1000], Train Loss: 188316.2100, Val Loss: 188954.2536\n",
      "Epoch [464/1000], Train Loss: 188321.1878, Val Loss: 188990.5063\n",
      "Epoch [465/1000], Train Loss: 188335.0537, Val Loss: 188974.4534\n",
      "Epoch [466/1000], Train Loss: 188300.0310, Val Loss: 188988.9319\n",
      "Epoch [467/1000], Train Loss: 188315.7171, Val Loss: 188959.0560\n",
      "Epoch [468/1000], Train Loss: 188298.6614, Val Loss: 188963.0925\n",
      "Epoch [469/1000], Train Loss: 188315.8607, Val Loss: 188964.7592\n",
      "Epoch [470/1000], Train Loss: 188299.5605, Val Loss: 188958.5217\n",
      "Epoch [471/1000], Train Loss: 188316.2715, Val Loss: 188959.8668\n",
      "Epoch [472/1000], Train Loss: 188296.1563, Val Loss: 188952.8946\n",
      "Epoch [473/1000], Train Loss: 188302.8740, Val Loss: 188953.8715\n",
      "Epoch [474/1000], Train Loss: 188292.1198, Val Loss: 189001.6100\n",
      "Epoch [475/1000], Train Loss: 188309.4653, Val Loss: 188960.4684\n",
      "Epoch [476/1000], Train Loss: 188310.9655, Val Loss: 188962.3087\n",
      "Epoch [477/1000], Train Loss: 188307.1122, Val Loss: 188981.1591\n",
      "Epoch [478/1000], Train Loss: 188313.3654, Val Loss: 188960.7112\n",
      "Epoch [479/1000], Train Loss: 188323.6887, Val Loss: 188990.5709\n",
      "Epoch [480/1000], Train Loss: 188321.9151, Val Loss: 189014.6251\n",
      "Epoch [481/1000], Train Loss: 188319.4859, Val Loss: 188954.2484\n",
      "Epoch [482/1000], Train Loss: 188312.8994, Val Loss: 188952.7733\n",
      "Epoch [483/1000], Train Loss: 188323.5733, Val Loss: 188955.2657\n",
      "Epoch [484/1000], Train Loss: 188295.2882, Val Loss: 188958.4659\n",
      "Epoch [485/1000], Train Loss: 188314.6099, Val Loss: 188954.8350\n",
      "Epoch [486/1000], Train Loss: 188295.4238, Val Loss: 188995.0107\n",
      "Epoch [487/1000], Train Loss: 188316.1040, Val Loss: 188990.0266\n",
      "Epoch [488/1000], Train Loss: 188301.2640, Val Loss: 188952.8541\n",
      "Epoch [489/1000], Train Loss: 188301.6688, Val Loss: 188954.6696\n",
      "Epoch [490/1000], Train Loss: 188307.4131, Val Loss: 188962.7860\n",
      "Epoch [491/1000], Train Loss: 188296.8349, Val Loss: 188953.1611\n",
      "Epoch [492/1000], Train Loss: 188297.8133, Val Loss: 188957.6730\n",
      "Epoch [493/1000], Train Loss: 188291.2433, Val Loss: 188999.9452\n",
      "Epoch [494/1000], Train Loss: 188317.2045, Val Loss: 188967.9283\n",
      "Epoch [495/1000], Train Loss: 188302.7841, Val Loss: 188959.3084\n",
      "Epoch [496/1000], Train Loss: 188308.6021, Val Loss: 188988.6070\n",
      "Epoch [497/1000], Train Loss: 188322.8016, Val Loss: 188960.3568\n",
      "Epoch [498/1000], Train Loss: 188306.8304, Val Loss: 188962.9475\n",
      "Epoch [499/1000], Train Loss: 188310.2881, Val Loss: 189026.9327\n",
      "Epoch [500/1000], Train Loss: 188300.4902, Val Loss: 188976.0126\n",
      "Epoch [501/1000], Train Loss: 188319.3686, Val Loss: 188993.4399\n",
      "Epoch [502/1000], Train Loss: 188318.4617, Val Loss: 188954.7027\n",
      "Epoch [503/1000], Train Loss: 188309.1125, Val Loss: 188990.5700\n",
      "Epoch [504/1000], Train Loss: 188315.8696, Val Loss: 188956.8579\n",
      "Epoch [505/1000], Train Loss: 188325.9188, Val Loss: 189075.1999\n",
      "Epoch [506/1000], Train Loss: 188303.8904, Val Loss: 188953.8910\n",
      "Epoch [507/1000], Train Loss: 188304.8551, Val Loss: 188958.8414\n",
      "Epoch [508/1000], Train Loss: 188310.5359, Val Loss: 188952.8864\n",
      "Epoch [509/1000], Train Loss: 188307.2448, Val Loss: 188991.4705\n",
      "Epoch [510/1000], Train Loss: 188325.8207, Val Loss: 188978.5565\n",
      "Epoch [511/1000], Train Loss: 188304.1684, Val Loss: 188974.1007\n",
      "Epoch [512/1000], Train Loss: 188309.7071, Val Loss: 188954.7771\n",
      "Epoch [513/1000], Train Loss: 188328.4656, Val Loss: 188961.8915\n",
      "Epoch [514/1000], Train Loss: 188301.6478, Val Loss: 188988.2259\n",
      "Epoch [515/1000], Train Loss: 188294.9305, Val Loss: 188953.2514\n",
      "Epoch [516/1000], Train Loss: 188299.0385, Val Loss: 188952.8196\n",
      "Epoch [517/1000], Train Loss: 188316.9939, Val Loss: 188986.2034\n",
      "Epoch [518/1000], Train Loss: 188308.8646, Val Loss: 189038.4328\n",
      "Epoch [519/1000], Train Loss: 188325.7251, Val Loss: 188953.1260\n",
      "Epoch [520/1000], Train Loss: 188296.3785, Val Loss: 188954.6530\n",
      "Epoch [521/1000], Train Loss: 188300.5121, Val Loss: 188953.7112\n",
      "Epoch [522/1000], Train Loss: 188301.3204, Val Loss: 188963.9303\n",
      "Epoch [523/1000], Train Loss: 188315.6542, Val Loss: 188962.7937\n",
      "Epoch [524/1000], Train Loss: 188303.4533, Val Loss: 188980.4152\n",
      "Epoch [525/1000], Train Loss: 188306.2090, Val Loss: 188954.4078\n",
      "Epoch [526/1000], Train Loss: 188303.4045, Val Loss: 188958.3889\n",
      "Epoch [527/1000], Train Loss: 188332.7827, Val Loss: 189021.0489\n",
      "Epoch [528/1000], Train Loss: 188300.8704, Val Loss: 188952.7595\n",
      "Epoch [529/1000], Train Loss: 188295.7300, Val Loss: 188955.0295\n",
      "Epoch [530/1000], Train Loss: 188320.2352, Val Loss: 188983.3274\n",
      "Epoch [531/1000], Train Loss: 188303.6257, Val Loss: 188977.9087\n",
      "Epoch [532/1000], Train Loss: 188329.5879, Val Loss: 188991.4987\n",
      "Epoch [533/1000], Train Loss: 188298.1421, Val Loss: 188991.5762\n",
      "Epoch [534/1000], Train Loss: 188302.8857, Val Loss: 188961.5822\n",
      "Epoch [535/1000], Train Loss: 188314.9072, Val Loss: 188952.8041\n",
      "Epoch [536/1000], Train Loss: 188308.9488, Val Loss: 188981.2197\n",
      "Epoch [537/1000], Train Loss: 188307.0782, Val Loss: 188953.3960\n",
      "Epoch [538/1000], Train Loss: 188295.8905, Val Loss: 188965.3316\n",
      "Epoch [539/1000], Train Loss: 188307.2668, Val Loss: 188989.6240\n",
      "Epoch [540/1000], Train Loss: 188317.0964, Val Loss: 188959.9134\n",
      "Epoch [541/1000], Train Loss: 188327.4022, Val Loss: 188953.1128\n",
      "Epoch [542/1000], Train Loss: 188287.4230, Val Loss: 189051.7311\n",
      "Epoch [543/1000], Train Loss: 188331.7919, Val Loss: 188960.6990\n",
      "Epoch [544/1000], Train Loss: 188298.1134, Val Loss: 188985.2770\n",
      "Epoch [545/1000], Train Loss: 188315.6734, Val Loss: 188973.1384\n",
      "Epoch [546/1000], Train Loss: 188293.5072, Val Loss: 188952.8844\n",
      "Epoch [547/1000], Train Loss: 188311.0889, Val Loss: 188953.2925\n",
      "Epoch [548/1000], Train Loss: 188304.9669, Val Loss: 188958.7611\n",
      "Epoch [549/1000], Train Loss: 188305.4647, Val Loss: 188972.7680\n",
      "Epoch [550/1000], Train Loss: 188299.9067, Val Loss: 188968.1585\n",
      "Epoch [551/1000], Train Loss: 188344.2423, Val Loss: 189079.1928\n",
      "Epoch [552/1000], Train Loss: 188315.8829, Val Loss: 188982.7858\n",
      "Epoch [553/1000], Train Loss: 188305.8782, Val Loss: 188953.1135\n",
      "Epoch [554/1000], Train Loss: 188314.7719, Val Loss: 188961.0533\n",
      "Epoch [555/1000], Train Loss: 188309.2712, Val Loss: 188991.0349\n",
      "Epoch [556/1000], Train Loss: 188310.9185, Val Loss: 188952.9536\n",
      "Epoch [557/1000], Train Loss: 188299.1510, Val Loss: 188963.0973\n",
      "Epoch [558/1000], Train Loss: 188303.3364, Val Loss: 188959.6839\n",
      "Epoch [559/1000], Train Loss: 188302.0727, Val Loss: 188963.1915\n",
      "Epoch [560/1000], Train Loss: 188306.3756, Val Loss: 188970.4175\n",
      "Epoch [561/1000], Train Loss: 188295.3937, Val Loss: 189011.3491\n",
      "Epoch [562/1000], Train Loss: 188315.5010, Val Loss: 188953.1847\n",
      "Epoch [563/1000], Train Loss: 188311.7224, Val Loss: 188956.8126\n",
      "Epoch [564/1000], Train Loss: 188302.0070, Val Loss: 188952.8822\n",
      "Epoch [565/1000], Train Loss: 188295.9473, Val Loss: 188963.2384\n",
      "Epoch [566/1000], Train Loss: 188326.1228, Val Loss: 188960.0062\n",
      "Epoch [567/1000], Train Loss: 188316.4292, Val Loss: 188953.7145\n",
      "Epoch [568/1000], Train Loss: 188301.6783, Val Loss: 188977.0883\n",
      "Epoch [569/1000], Train Loss: 188310.2334, Val Loss: 188980.3485\n",
      "Epoch [570/1000], Train Loss: 188303.4972, Val Loss: 188960.8410\n",
      "Epoch [571/1000], Train Loss: 188307.7719, Val Loss: 188961.0590\n",
      "Epoch [572/1000], Train Loss: 188298.5470, Val Loss: 188966.3547\n",
      "Epoch [573/1000], Train Loss: 188294.8907, Val Loss: 189061.0582\n",
      "Epoch [574/1000], Train Loss: 188329.2568, Val Loss: 188994.3316\n",
      "Epoch [575/1000], Train Loss: 188306.6978, Val Loss: 188965.6576\n",
      "Epoch [576/1000], Train Loss: 188308.5078, Val Loss: 188954.2634\n",
      "Epoch [577/1000], Train Loss: 188309.2491, Val Loss: 188966.7591\n",
      "Epoch [578/1000], Train Loss: 188302.7113, Val Loss: 188954.0228\n",
      "Epoch [579/1000], Train Loss: 188301.1530, Val Loss: 188971.9469\n",
      "Epoch [580/1000], Train Loss: 188302.6635, Val Loss: 188952.8008\n",
      "Epoch [581/1000], Train Loss: 188298.0554, Val Loss: 188956.8493\n",
      "Epoch [582/1000], Train Loss: 188294.6418, Val Loss: 188968.4048\n",
      "Epoch [583/1000], Train Loss: 188305.1734, Val Loss: 188955.2642\n",
      "Epoch [584/1000], Train Loss: 188295.9882, Val Loss: 188954.0000\n",
      "Epoch [585/1000], Train Loss: 188307.9372, Val Loss: 188968.8526\n",
      "Epoch [586/1000], Train Loss: 188313.5520, Val Loss: 188969.4211\n",
      "Epoch [587/1000], Train Loss: 188303.2576, Val Loss: 188961.1219\n",
      "Epoch [588/1000], Train Loss: 188295.0199, Val Loss: 188966.4862\n",
      "Epoch [589/1000], Train Loss: 188302.1604, Val Loss: 188953.9893\n",
      "Epoch [590/1000], Train Loss: 188294.5448, Val Loss: 188954.6742\n",
      "Epoch [591/1000], Train Loss: 188306.4083, Val Loss: 188973.8386\n",
      "Epoch [592/1000], Train Loss: 188305.5825, Val Loss: 188953.2199\n",
      "Epoch [593/1000], Train Loss: 188299.0539, Val Loss: 189008.5005\n",
      "Epoch [594/1000], Train Loss: 188317.9418, Val Loss: 188954.2461\n",
      "Epoch [595/1000], Train Loss: 188304.3234, Val Loss: 188953.3832\n",
      "Epoch [596/1000], Train Loss: 188301.6857, Val Loss: 188952.8099\n",
      "Epoch [597/1000], Train Loss: 188305.0612, Val Loss: 188952.9948\n",
      "Epoch [598/1000], Train Loss: 188310.0837, Val Loss: 188953.9744\n",
      "Epoch [599/1000], Train Loss: 188315.0309, Val Loss: 188953.4924\n",
      "Epoch [600/1000], Train Loss: 188282.8540, Val Loss: 189055.5173\n",
      "Epoch [601/1000], Train Loss: 188319.7053, Val Loss: 188956.4466\n",
      "Epoch [602/1000], Train Loss: 188307.0167, Val Loss: 188958.7505\n",
      "Epoch [603/1000], Train Loss: 188310.7274, Val Loss: 188954.1736\n",
      "Epoch [604/1000], Train Loss: 188314.8747, Val Loss: 188955.8795\n",
      "Epoch [605/1000], Train Loss: 188308.3947, Val Loss: 188996.8030\n",
      "Epoch [606/1000], Train Loss: 188329.4292, Val Loss: 188957.1459\n",
      "Epoch [607/1000], Train Loss: 188294.1869, Val Loss: 188962.5767\n",
      "Epoch [608/1000], Train Loss: 188293.0709, Val Loss: 188953.2875\n",
      "Epoch [609/1000], Train Loss: 188308.2865, Val Loss: 188964.2016\n",
      "Epoch [610/1000], Train Loss: 188298.0140, Val Loss: 188978.1917\n",
      "Epoch [611/1000], Train Loss: 188288.1874, Val Loss: 189003.7754\n",
      "Epoch [612/1000], Train Loss: 188303.7324, Val Loss: 189023.9720\n",
      "Epoch [613/1000], Train Loss: 188320.3267, Val Loss: 188952.8197\n",
      "Epoch [614/1000], Train Loss: 188304.4589, Val Loss: 188960.3585\n",
      "Epoch [615/1000], Train Loss: 188301.1239, Val Loss: 188963.6632\n",
      "Epoch [616/1000], Train Loss: 188309.7749, Val Loss: 188968.0757\n",
      "Epoch [617/1000], Train Loss: 188313.5126, Val Loss: 188955.7637\n",
      "Epoch [618/1000], Train Loss: 188298.0077, Val Loss: 188966.4803\n",
      "Epoch [619/1000], Train Loss: 188301.7341, Val Loss: 188959.6536\n",
      "Epoch [620/1000], Train Loss: 188306.8999, Val Loss: 188997.8379\n",
      "Epoch [621/1000], Train Loss: 188331.6604, Val Loss: 188954.6381\n",
      "Epoch [622/1000], Train Loss: 188319.6482, Val Loss: 188952.7668\n",
      "Epoch [623/1000], Train Loss: 188307.7004, Val Loss: 188963.1093\n",
      "Epoch [624/1000], Train Loss: 188303.0368, Val Loss: 188976.3292\n",
      "Epoch [625/1000], Train Loss: 188304.2998, Val Loss: 188953.9361\n",
      "Epoch [626/1000], Train Loss: 188302.3298, Val Loss: 188954.1100\n",
      "Epoch [627/1000], Train Loss: 188333.0054, Val Loss: 188952.9798\n",
      "Epoch [628/1000], Train Loss: 188302.3508, Val Loss: 188953.1663\n",
      "Epoch [629/1000], Train Loss: 188300.9459, Val Loss: 188966.1650\n",
      "Epoch [630/1000], Train Loss: 188298.5289, Val Loss: 188973.6588\n",
      "Epoch [631/1000], Train Loss: 188326.8442, Val Loss: 188983.9645\n",
      "Epoch [632/1000], Train Loss: 188291.3637, Val Loss: 188961.1689\n",
      "Epoch [633/1000], Train Loss: 188300.0381, Val Loss: 188954.8108\n",
      "Epoch [634/1000], Train Loss: 188310.1861, Val Loss: 188953.0214\n",
      "Epoch [635/1000], Train Loss: 188301.1439, Val Loss: 188954.7627\n",
      "Epoch [636/1000], Train Loss: 188297.0720, Val Loss: 188953.2566\n",
      "Epoch [637/1000], Train Loss: 188306.0656, Val Loss: 188958.3023\n",
      "Epoch [638/1000], Train Loss: 188310.0066, Val Loss: 189001.2525\n",
      "Epoch [639/1000], Train Loss: 188310.7422, Val Loss: 188954.5511\n",
      "Epoch [640/1000], Train Loss: 188308.7352, Val Loss: 188953.2990\n",
      "Epoch [641/1000], Train Loss: 188317.6316, Val Loss: 188965.5058\n",
      "Epoch [642/1000], Train Loss: 188282.6467, Val Loss: 189021.4162\n",
      "Epoch [643/1000], Train Loss: 188311.2431, Val Loss: 188953.9787\n",
      "Epoch [644/1000], Train Loss: 188293.8572, Val Loss: 188952.8018\n",
      "Epoch [645/1000], Train Loss: 188293.1497, Val Loss: 188987.5202\n",
      "Epoch [646/1000], Train Loss: 188316.9790, Val Loss: 188991.8608\n",
      "Epoch [647/1000], Train Loss: 188330.4218, Val Loss: 188964.4106\n",
      "Epoch [648/1000], Train Loss: 188313.9639, Val Loss: 188988.9824\n",
      "Epoch [649/1000], Train Loss: 188301.0871, Val Loss: 188981.5491\n",
      "Epoch [650/1000], Train Loss: 188316.3110, Val Loss: 188956.1803\n",
      "Epoch [651/1000], Train Loss: 188307.3286, Val Loss: 188960.1057\n",
      "Epoch [652/1000], Train Loss: 188292.0483, Val Loss: 188952.9600\n",
      "Epoch [653/1000], Train Loss: 188306.4394, Val Loss: 188952.7674\n",
      "Epoch [654/1000], Train Loss: 188309.7130, Val Loss: 188954.7946\n",
      "Epoch [655/1000], Train Loss: 188300.0321, Val Loss: 188954.7227\n",
      "Epoch [656/1000], Train Loss: 188311.1831, Val Loss: 188986.7874\n",
      "Epoch [657/1000], Train Loss: 188324.6044, Val Loss: 189069.7848\n",
      "Epoch [658/1000], Train Loss: 188314.4749, Val Loss: 188952.9440\n",
      "Epoch [659/1000], Train Loss: 188290.5150, Val Loss: 188952.8995\n",
      "Epoch [660/1000], Train Loss: 188299.8290, Val Loss: 188990.8436\n",
      "Epoch [661/1000], Train Loss: 188301.4839, Val Loss: 188957.5777\n",
      "Epoch [662/1000], Train Loss: 188314.7532, Val Loss: 189001.0907\n",
      "Epoch [663/1000], Train Loss: 188306.6097, Val Loss: 188952.7610\n",
      "Epoch [664/1000], Train Loss: 188313.2040, Val Loss: 188956.3752\n",
      "Epoch [665/1000], Train Loss: 188302.6210, Val Loss: 188955.5202\n",
      "Epoch [666/1000], Train Loss: 188310.2221, Val Loss: 188967.7865\n",
      "Epoch [667/1000], Train Loss: 188305.9652, Val Loss: 188959.3317\n",
      "Epoch [668/1000], Train Loss: 188302.9496, Val Loss: 188953.1346\n",
      "Epoch [669/1000], Train Loss: 188317.5303, Val Loss: 188964.7786\n",
      "Epoch [670/1000], Train Loss: 188301.8727, Val Loss: 188956.9753\n",
      "Epoch [671/1000], Train Loss: 188298.1312, Val Loss: 188954.2492\n",
      "Epoch [672/1000], Train Loss: 188298.1095, Val Loss: 189031.7132\n",
      "Epoch [673/1000], Train Loss: 188314.6973, Val Loss: 188969.5856\n",
      "Epoch [674/1000], Train Loss: 188316.5982, Val Loss: 188952.9130\n",
      "Epoch [675/1000], Train Loss: 188310.0201, Val Loss: 188966.4501\n",
      "Epoch [676/1000], Train Loss: 188306.3939, Val Loss: 188960.4450\n",
      "Epoch [677/1000], Train Loss: 188332.2264, Val Loss: 188952.7670\n",
      "Epoch [678/1000], Train Loss: 188288.9230, Val Loss: 188992.2134\n",
      "Epoch [679/1000], Train Loss: 188295.2781, Val Loss: 188955.3661\n",
      "Epoch [680/1000], Train Loss: 188295.9821, Val Loss: 188954.9523\n",
      "Epoch [681/1000], Train Loss: 188305.4077, Val Loss: 188958.8272\n",
      "Epoch [682/1000], Train Loss: 188294.8704, Val Loss: 188962.9144\n",
      "Epoch [683/1000], Train Loss: 188303.2875, Val Loss: 188953.8088\n",
      "Epoch [684/1000], Train Loss: 188299.6846, Val Loss: 188953.0163\n",
      "Epoch [685/1000], Train Loss: 188297.2848, Val Loss: 188952.7652\n",
      "Epoch [686/1000], Train Loss: 188302.5458, Val Loss: 188953.6555\n",
      "Epoch [687/1000], Train Loss: 188325.5894, Val Loss: 188954.9626\n",
      "Epoch [688/1000], Train Loss: 188300.1824, Val Loss: 188955.8274\n",
      "Epoch [689/1000], Train Loss: 188308.1469, Val Loss: 188999.3449\n",
      "Epoch [690/1000], Train Loss: 188335.9060, Val Loss: 188985.6239\n",
      "Epoch [691/1000], Train Loss: 188303.7742, Val Loss: 188997.3770\n",
      "Epoch [692/1000], Train Loss: 188320.8266, Val Loss: 188990.0742\n",
      "Epoch [693/1000], Train Loss: 188321.1771, Val Loss: 188956.1359\n",
      "Epoch [694/1000], Train Loss: 188303.3949, Val Loss: 188952.8314\n",
      "Epoch [695/1000], Train Loss: 188295.7773, Val Loss: 188952.9602\n",
      "Epoch [696/1000], Train Loss: 188300.6168, Val Loss: 188969.4130\n",
      "Epoch [697/1000], Train Loss: 188310.2801, Val Loss: 188987.7460\n",
      "Epoch [698/1000], Train Loss: 188295.6164, Val Loss: 188953.2412\n",
      "Epoch [699/1000], Train Loss: 188312.8096, Val Loss: 188959.1995\n",
      "Epoch [700/1000], Train Loss: 188298.2608, Val Loss: 189022.8351\n",
      "Epoch [701/1000], Train Loss: 188312.7932, Val Loss: 188952.7699\n",
      "Epoch [702/1000], Train Loss: 188299.4319, Val Loss: 188952.9299\n",
      "Epoch [703/1000], Train Loss: 188304.9343, Val Loss: 188953.3903\n",
      "Epoch [704/1000], Train Loss: 188300.6620, Val Loss: 188961.8401\n",
      "Epoch [705/1000], Train Loss: 188307.2387, Val Loss: 188967.3764\n",
      "Epoch [706/1000], Train Loss: 188306.5060, Val Loss: 188974.9182\n",
      "Epoch [707/1000], Train Loss: 188319.0238, Val Loss: 188955.1665\n",
      "Epoch [708/1000], Train Loss: 188310.6996, Val Loss: 188966.7123\n",
      "Epoch [709/1000], Train Loss: 188312.1470, Val Loss: 188972.9573\n",
      "Epoch [710/1000], Train Loss: 188293.0778, Val Loss: 188996.0706\n",
      "Epoch [711/1000], Train Loss: 188317.6931, Val Loss: 188969.0579\n",
      "Epoch [712/1000], Train Loss: 188306.0908, Val Loss: 188961.3887\n",
      "Epoch [713/1000], Train Loss: 188288.6599, Val Loss: 188962.9796\n",
      "Epoch [714/1000], Train Loss: 188313.0271, Val Loss: 188958.4943\n",
      "Epoch [715/1000], Train Loss: 188315.1917, Val Loss: 188969.9986\n",
      "Epoch [716/1000], Train Loss: 188292.2131, Val Loss: 188955.6067\n",
      "Epoch [717/1000], Train Loss: 188311.4171, Val Loss: 188972.4654\n",
      "Epoch [718/1000], Train Loss: 188320.8927, Val Loss: 188956.7210\n",
      "Epoch [719/1000], Train Loss: 188302.6761, Val Loss: 188991.5564\n",
      "Epoch [720/1000], Train Loss: 188303.8973, Val Loss: 188966.4935\n",
      "Epoch [721/1000], Train Loss: 188296.7813, Val Loss: 188956.8250\n",
      "Epoch [722/1000], Train Loss: 188299.0348, Val Loss: 188965.3968\n",
      "Epoch [723/1000], Train Loss: 188315.0384, Val Loss: 188952.9887\n",
      "Epoch [724/1000], Train Loss: 188311.2828, Val Loss: 188960.1606\n",
      "Epoch [725/1000], Train Loss: 188305.4558, Val Loss: 188952.8222\n",
      "Epoch [726/1000], Train Loss: 188301.2993, Val Loss: 189008.9846\n",
      "Epoch [727/1000], Train Loss: 188305.1626, Val Loss: 188953.9070\n",
      "Epoch [728/1000], Train Loss: 188306.2187, Val Loss: 188978.8296\n",
      "Epoch [729/1000], Train Loss: 188323.1390, Val Loss: 188966.9743\n",
      "Epoch [730/1000], Train Loss: 188316.5948, Val Loss: 188952.9152\n",
      "Epoch [731/1000], Train Loss: 188298.8272, Val Loss: 188953.7786\n",
      "Epoch [732/1000], Train Loss: 188311.8059, Val Loss: 188981.3777\n",
      "Epoch [733/1000], Train Loss: 188308.0648, Val Loss: 188957.3975\n",
      "Epoch [734/1000], Train Loss: 188315.6314, Val Loss: 188958.4481\n",
      "Epoch [735/1000], Train Loss: 188323.3061, Val Loss: 189020.2726\n",
      "Epoch [736/1000], Train Loss: 188316.7687, Val Loss: 188961.2940\n",
      "Epoch [737/1000], Train Loss: 188299.9078, Val Loss: 188954.4072\n",
      "Epoch [738/1000], Train Loss: 188305.8626, Val Loss: 188953.8519\n",
      "Epoch [739/1000], Train Loss: 188307.3501, Val Loss: 188965.9316\n",
      "Epoch [740/1000], Train Loss: 188289.3942, Val Loss: 188962.3811\n",
      "Epoch [741/1000], Train Loss: 188312.9168, Val Loss: 188957.8117\n",
      "Epoch [742/1000], Train Loss: 188305.8158, Val Loss: 188959.0920\n",
      "Epoch [743/1000], Train Loss: 188310.4604, Val Loss: 189005.2205\n",
      "Epoch [744/1000], Train Loss: 188300.2462, Val Loss: 188957.0421\n",
      "Epoch [745/1000], Train Loss: 188303.0400, Val Loss: 188960.2786\n",
      "Epoch [746/1000], Train Loss: 188302.2154, Val Loss: 188960.4116\n",
      "Epoch [747/1000], Train Loss: 188307.0615, Val Loss: 188954.2400\n",
      "Epoch [748/1000], Train Loss: 188311.1331, Val Loss: 188963.2536\n",
      "Epoch [749/1000], Train Loss: 188308.0458, Val Loss: 188967.1295\n",
      "Epoch [750/1000], Train Loss: 188311.2230, Val Loss: 188952.7698\n",
      "Epoch [751/1000], Train Loss: 188298.1918, Val Loss: 188966.5966\n",
      "Epoch [752/1000], Train Loss: 188302.1427, Val Loss: 188953.0289\n",
      "Epoch [753/1000], Train Loss: 188314.7747, Val Loss: 188958.9888\n",
      "Epoch [754/1000], Train Loss: 188303.7657, Val Loss: 188975.0430\n",
      "Epoch [755/1000], Train Loss: 188302.5471, Val Loss: 188959.8118\n",
      "Epoch [756/1000], Train Loss: 188300.8022, Val Loss: 188959.3945\n",
      "Epoch [757/1000], Train Loss: 188302.8923, Val Loss: 188974.9965\n",
      "Epoch [758/1000], Train Loss: 188296.6001, Val Loss: 188961.9122\n",
      "Epoch [759/1000], Train Loss: 188304.8766, Val Loss: 188986.0777\n",
      "Epoch [760/1000], Train Loss: 188296.6139, Val Loss: 188952.7893\n",
      "Epoch [761/1000], Train Loss: 188312.6181, Val Loss: 188954.2629\n",
      "Epoch [762/1000], Train Loss: 188308.5683, Val Loss: 188953.2501\n",
      "Epoch [763/1000], Train Loss: 188297.1558, Val Loss: 188954.2228\n",
      "Epoch [764/1000], Train Loss: 188297.8940, Val Loss: 188966.4385\n",
      "Epoch [765/1000], Train Loss: 188318.4325, Val Loss: 188953.1575\n",
      "Epoch [766/1000], Train Loss: 188294.6360, Val Loss: 188952.9009\n",
      "Epoch [767/1000], Train Loss: 188297.6603, Val Loss: 188984.0890\n",
      "Epoch [768/1000], Train Loss: 188296.8195, Val Loss: 188963.2916\n",
      "Epoch [769/1000], Train Loss: 188310.5482, Val Loss: 188959.4824\n",
      "Epoch [770/1000], Train Loss: 188301.5706, Val Loss: 188956.2486\n",
      "Epoch [771/1000], Train Loss: 188314.2770, Val Loss: 188953.1378\n",
      "Epoch [772/1000], Train Loss: 188320.2781, Val Loss: 188952.9594\n",
      "Epoch [773/1000], Train Loss: 188300.0612, Val Loss: 188955.3674\n",
      "Epoch [774/1000], Train Loss: 188344.2381, Val Loss: 188956.5017\n",
      "Epoch [775/1000], Train Loss: 188310.1957, Val Loss: 188964.9830\n",
      "Epoch [776/1000], Train Loss: 188316.2026, Val Loss: 188956.4577\n",
      "Epoch [777/1000], Train Loss: 188299.9092, Val Loss: 188996.1387\n",
      "Epoch [778/1000], Train Loss: 188315.7969, Val Loss: 188952.7719\n",
      "Epoch [779/1000], Train Loss: 188317.8864, Val Loss: 188956.9844\n",
      "Epoch [780/1000], Train Loss: 188305.8612, Val Loss: 188967.5613\n",
      "Epoch [781/1000], Train Loss: 188311.0477, Val Loss: 188981.8321\n",
      "Epoch [782/1000], Train Loss: 188315.2216, Val Loss: 189017.2714\n",
      "Epoch [783/1000], Train Loss: 188301.9705, Val Loss: 188966.3256\n",
      "Epoch [784/1000], Train Loss: 188302.7068, Val Loss: 188952.7628\n",
      "Epoch [785/1000], Train Loss: 188316.7352, Val Loss: 188956.9176\n",
      "Epoch [786/1000], Train Loss: 188301.5058, Val Loss: 188953.6146\n",
      "Epoch [787/1000], Train Loss: 188312.0389, Val Loss: 188963.8524\n",
      "Epoch [788/1000], Train Loss: 188318.7149, Val Loss: 188965.8465\n",
      "Epoch [789/1000], Train Loss: 188308.7713, Val Loss: 188954.9583\n",
      "Epoch [790/1000], Train Loss: 188304.3699, Val Loss: 188962.8688\n",
      "Epoch [791/1000], Train Loss: 188307.0040, Val Loss: 188953.2759\n",
      "Epoch [792/1000], Train Loss: 188306.8990, Val Loss: 188988.3665\n",
      "Epoch [793/1000], Train Loss: 188331.0065, Val Loss: 188994.8476\n",
      "Epoch [794/1000], Train Loss: 188303.8692, Val Loss: 188969.3960\n",
      "Epoch [795/1000], Train Loss: 188313.3689, Val Loss: 188980.7656\n",
      "Epoch [796/1000], Train Loss: 188302.5956, Val Loss: 188952.7768\n",
      "Epoch [797/1000], Train Loss: 188306.4777, Val Loss: 188957.4218\n",
      "Epoch [798/1000], Train Loss: 188318.4022, Val Loss: 188980.4530\n",
      "Epoch [799/1000], Train Loss: 188304.8955, Val Loss: 188956.0429\n",
      "Epoch [800/1000], Train Loss: 188287.6114, Val Loss: 188997.9032\n",
      "Epoch [801/1000], Train Loss: 188305.9004, Val Loss: 188953.5799\n",
      "Epoch [802/1000], Train Loss: 188303.1500, Val Loss: 188952.8232\n",
      "Epoch [803/1000], Train Loss: 188305.3017, Val Loss: 188952.9334\n",
      "Epoch [804/1000], Train Loss: 188285.6134, Val Loss: 188982.2406\n",
      "Epoch [805/1000], Train Loss: 188304.5652, Val Loss: 188961.2338\n",
      "Epoch [806/1000], Train Loss: 188306.2803, Val Loss: 188964.5146\n",
      "Epoch [807/1000], Train Loss: 188298.2374, Val Loss: 188968.1796\n",
      "Epoch [808/1000], Train Loss: 188295.9832, Val Loss: 188961.6033\n",
      "Epoch [809/1000], Train Loss: 188299.9384, Val Loss: 188992.5905\n",
      "Epoch [810/1000], Train Loss: 188304.3899, Val Loss: 188961.9064\n",
      "Epoch [811/1000], Train Loss: 188297.0420, Val Loss: 188958.3293\n",
      "Epoch [812/1000], Train Loss: 188307.8931, Val Loss: 188956.8663\n",
      "Epoch [813/1000], Train Loss: 188302.0625, Val Loss: 188973.9025\n",
      "Epoch [814/1000], Train Loss: 188295.2487, Val Loss: 188967.0081\n",
      "Epoch [815/1000], Train Loss: 188303.5769, Val Loss: 188982.1135\n",
      "Epoch [816/1000], Train Loss: 188294.5414, Val Loss: 188989.2914\n",
      "Epoch [817/1000], Train Loss: 188308.5303, Val Loss: 188954.7977\n",
      "Epoch [818/1000], Train Loss: 188291.1605, Val Loss: 188953.6929\n",
      "Epoch [819/1000], Train Loss: 188310.2615, Val Loss: 188977.1179\n",
      "Epoch [820/1000], Train Loss: 188313.1582, Val Loss: 188952.7604\n",
      "Epoch [821/1000], Train Loss: 188314.1390, Val Loss: 188967.5273\n",
      "Epoch [822/1000], Train Loss: 188308.9364, Val Loss: 188954.2333\n",
      "Epoch [823/1000], Train Loss: 188295.0542, Val Loss: 188955.1977\n",
      "Epoch [824/1000], Train Loss: 188319.2899, Val Loss: 188960.4644\n",
      "Epoch [825/1000], Train Loss: 188308.1379, Val Loss: 188952.7627\n",
      "Epoch [826/1000], Train Loss: 188299.4987, Val Loss: 188974.9024\n",
      "Epoch [827/1000], Train Loss: 188299.4684, Val Loss: 189042.6669\n",
      "Epoch [828/1000], Train Loss: 188305.2310, Val Loss: 188964.7931\n",
      "Epoch [829/1000], Train Loss: 188328.5140, Val Loss: 188991.9736\n",
      "Epoch [830/1000], Train Loss: 188327.7134, Val Loss: 188965.9751\n",
      "Epoch [831/1000], Train Loss: 188293.1896, Val Loss: 188984.1273\n",
      "Epoch [832/1000], Train Loss: 188300.1832, Val Loss: 188959.4972\n",
      "Epoch [833/1000], Train Loss: 188293.6088, Val Loss: 188966.0588\n",
      "Epoch [834/1000], Train Loss: 188289.8889, Val Loss: 189035.6422\n",
      "Epoch [835/1000], Train Loss: 188292.4080, Val Loss: 188959.1631\n",
      "Epoch [836/1000], Train Loss: 188310.8023, Val Loss: 188998.7732\n",
      "Epoch [837/1000], Train Loss: 188310.5876, Val Loss: 188982.7897\n",
      "Epoch [838/1000], Train Loss: 188300.6179, Val Loss: 189017.6250\n",
      "Epoch [839/1000], Train Loss: 188315.2075, Val Loss: 188959.9955\n",
      "Epoch [840/1000], Train Loss: 188310.1947, Val Loss: 188967.9464\n",
      "Epoch [841/1000], Train Loss: 188292.9478, Val Loss: 188992.4342\n",
      "Epoch [842/1000], Train Loss: 188344.2110, Val Loss: 188986.3110\n",
      "Epoch [843/1000], Train Loss: 188328.9707, Val Loss: 188952.8617\n",
      "Epoch [844/1000], Train Loss: 188297.5751, Val Loss: 188957.3604\n",
      "Epoch [845/1000], Train Loss: 188302.4961, Val Loss: 188969.2590\n",
      "Epoch [846/1000], Train Loss: 188301.1605, Val Loss: 188952.8097\n",
      "Epoch [847/1000], Train Loss: 188300.0457, Val Loss: 188972.3657\n",
      "Epoch [848/1000], Train Loss: 188296.2192, Val Loss: 188953.1680\n",
      "Epoch [849/1000], Train Loss: 188320.9483, Val Loss: 189008.9661\n",
      "Epoch [850/1000], Train Loss: 188314.5986, Val Loss: 188962.5860\n",
      "Epoch [851/1000], Train Loss: 188295.7570, Val Loss: 188953.5172\n",
      "Epoch [852/1000], Train Loss: 188314.8765, Val Loss: 188955.5815\n",
      "Epoch [853/1000], Train Loss: 188298.2120, Val Loss: 188957.9191\n",
      "Epoch [854/1000], Train Loss: 188315.7723, Val Loss: 188952.9675\n",
      "Epoch [855/1000], Train Loss: 188304.4885, Val Loss: 188967.7757\n",
      "Epoch [856/1000], Train Loss: 188307.7194, Val Loss: 188952.8169\n",
      "Epoch [857/1000], Train Loss: 188296.8403, Val Loss: 188960.4253\n",
      "Epoch [858/1000], Train Loss: 188308.0341, Val Loss: 188973.8866\n",
      "Epoch [859/1000], Train Loss: 188317.6768, Val Loss: 188952.7656\n",
      "Epoch [860/1000], Train Loss: 188301.6397, Val Loss: 188954.6759\n",
      "Epoch [861/1000], Train Loss: 188305.7439, Val Loss: 188966.8384\n",
      "Epoch [862/1000], Train Loss: 188306.6216, Val Loss: 188958.9671\n",
      "Epoch [863/1000], Train Loss: 188297.8504, Val Loss: 188965.0430\n",
      "Epoch [864/1000], Train Loss: 188303.1296, Val Loss: 188969.2786\n",
      "Epoch [865/1000], Train Loss: 188301.1431, Val Loss: 188978.0911\n",
      "Epoch [866/1000], Train Loss: 188293.5572, Val Loss: 188956.6212\n",
      "Epoch [867/1000], Train Loss: 188304.0489, Val Loss: 188954.9194\n",
      "Epoch [868/1000], Train Loss: 188295.4010, Val Loss: 188953.5631\n",
      "Epoch [869/1000], Train Loss: 188314.2867, Val Loss: 188953.1959\n",
      "Epoch [870/1000], Train Loss: 188300.9420, Val Loss: 188964.8643\n",
      "Epoch [871/1000], Train Loss: 188301.2947, Val Loss: 188979.7893\n",
      "Epoch [872/1000], Train Loss: 188298.3667, Val Loss: 188952.9109\n",
      "Epoch [873/1000], Train Loss: 188298.9192, Val Loss: 188954.5132\n",
      "Epoch [874/1000], Train Loss: 188306.4184, Val Loss: 188968.9333\n",
      "Epoch [875/1000], Train Loss: 188307.3999, Val Loss: 188953.0793\n",
      "Epoch [876/1000], Train Loss: 188305.3978, Val Loss: 188954.5363\n",
      "Epoch [877/1000], Train Loss: 188298.3482, Val Loss: 188953.4124\n",
      "Epoch [878/1000], Train Loss: 188308.3617, Val Loss: 188956.3351\n",
      "Epoch [879/1000], Train Loss: 188303.9800, Val Loss: 188976.0433\n",
      "Epoch [880/1000], Train Loss: 188315.3764, Val Loss: 188954.4933\n",
      "Epoch [881/1000], Train Loss: 188303.7437, Val Loss: 188955.2319\n",
      "Epoch [882/1000], Train Loss: 188307.8827, Val Loss: 188953.3709\n",
      "Epoch [883/1000], Train Loss: 188306.0727, Val Loss: 188954.2364\n",
      "Epoch [884/1000], Train Loss: 188302.2881, Val Loss: 188952.8072\n",
      "Epoch [885/1000], Train Loss: 188294.7115, Val Loss: 188960.1636\n",
      "Epoch [886/1000], Train Loss: 188306.2387, Val Loss: 188953.7777\n",
      "Epoch [887/1000], Train Loss: 188292.9465, Val Loss: 188964.9990\n",
      "Epoch [888/1000], Train Loss: 188326.5133, Val Loss: 188971.2838\n",
      "Epoch [889/1000], Train Loss: 188310.2012, Val Loss: 188957.0871\n",
      "Epoch [890/1000], Train Loss: 188311.3589, Val Loss: 188960.6706\n",
      "Epoch [891/1000], Train Loss: 188302.3070, Val Loss: 188954.9344\n",
      "Epoch [892/1000], Train Loss: 188294.5032, Val Loss: 188952.7636\n",
      "Epoch [893/1000], Train Loss: 188289.4479, Val Loss: 188978.5485\n",
      "Epoch [894/1000], Train Loss: 188301.8562, Val Loss: 188977.9867\n",
      "Epoch [895/1000], Train Loss: 188306.5228, Val Loss: 188970.3718\n",
      "Epoch [896/1000], Train Loss: 188308.3290, Val Loss: 188969.3811\n",
      "Epoch [897/1000], Train Loss: 188306.7474, Val Loss: 188975.0419\n",
      "Epoch [898/1000], Train Loss: 188309.3949, Val Loss: 188973.5848\n",
      "Epoch [899/1000], Train Loss: 188296.2223, Val Loss: 188971.0114\n",
      "Epoch [900/1000], Train Loss: 188302.6578, Val Loss: 188952.8718\n",
      "Epoch [901/1000], Train Loss: 188299.3158, Val Loss: 188964.8088\n",
      "Epoch [902/1000], Train Loss: 188308.6623, Val Loss: 188957.0034\n",
      "Epoch [903/1000], Train Loss: 188320.2574, Val Loss: 188955.1820\n",
      "Epoch [904/1000], Train Loss: 188309.7561, Val Loss: 188955.9845\n",
      "Epoch [905/1000], Train Loss: 188303.4348, Val Loss: 188960.3015\n",
      "Epoch [906/1000], Train Loss: 188301.0512, Val Loss: 188953.7744\n",
      "Epoch [907/1000], Train Loss: 188305.8569, Val Loss: 188974.3896\n",
      "Epoch [908/1000], Train Loss: 188329.7268, Val Loss: 188953.4513\n",
      "Epoch [909/1000], Train Loss: 188308.4350, Val Loss: 188953.8142\n",
      "Epoch [910/1000], Train Loss: 188297.8755, Val Loss: 188956.7846\n",
      "Epoch [911/1000], Train Loss: 188306.0634, Val Loss: 188953.6910\n",
      "Epoch [912/1000], Train Loss: 188298.7327, Val Loss: 188967.3337\n",
      "Epoch [913/1000], Train Loss: 188294.3214, Val Loss: 188967.0484\n",
      "Epoch [914/1000], Train Loss: 188332.5281, Val Loss: 188964.5886\n",
      "Epoch [915/1000], Train Loss: 188298.6442, Val Loss: 188982.0072\n",
      "Epoch [916/1000], Train Loss: 188295.4939, Val Loss: 188988.9311\n",
      "Epoch [917/1000], Train Loss: 188298.7343, Val Loss: 188953.0118\n",
      "Epoch [918/1000], Train Loss: 188307.2110, Val Loss: 188957.3248\n",
      "Epoch [919/1000], Train Loss: 188302.5377, Val Loss: 188956.3817\n",
      "Epoch [920/1000], Train Loss: 188326.8377, Val Loss: 188952.8955\n",
      "Epoch [921/1000], Train Loss: 188298.7359, Val Loss: 188963.0484\n",
      "Epoch [922/1000], Train Loss: 188302.6107, Val Loss: 188960.8039\n",
      "Epoch [923/1000], Train Loss: 188310.8992, Val Loss: 188952.7613\n",
      "Epoch [924/1000], Train Loss: 188304.2041, Val Loss: 188964.5762\n",
      "Epoch [925/1000], Train Loss: 188302.7189, Val Loss: 188952.8483\n",
      "Epoch [926/1000], Train Loss: 188309.1625, Val Loss: 188953.7863\n",
      "Epoch [927/1000], Train Loss: 188306.0304, Val Loss: 188958.3190\n",
      "Epoch [928/1000], Train Loss: 188313.7550, Val Loss: 188981.5450\n",
      "Epoch [929/1000], Train Loss: 188299.8280, Val Loss: 188957.0872\n",
      "Epoch [930/1000], Train Loss: 188298.2370, Val Loss: 188967.9937\n",
      "Epoch [931/1000], Train Loss: 188305.3037, Val Loss: 188959.8294\n",
      "Epoch [932/1000], Train Loss: 188308.2270, Val Loss: 188952.9793\n",
      "Epoch [933/1000], Train Loss: 188298.2986, Val Loss: 188953.1406\n",
      "Epoch [934/1000], Train Loss: 188295.7653, Val Loss: 188955.9792\n",
      "Epoch [935/1000], Train Loss: 188297.5039, Val Loss: 188955.3370\n",
      "Epoch [936/1000], Train Loss: 188300.1286, Val Loss: 188961.5492\n",
      "Epoch [937/1000], Train Loss: 188325.4382, Val Loss: 188958.2138\n",
      "Epoch [938/1000], Train Loss: 188306.7688, Val Loss: 188968.5118\n",
      "Epoch [939/1000], Train Loss: 188300.4934, Val Loss: 188953.6937\n",
      "Epoch [940/1000], Train Loss: 188293.8089, Val Loss: 188961.8935\n",
      "Epoch [941/1000], Train Loss: 188302.0132, Val Loss: 188988.6368\n",
      "Epoch [942/1000], Train Loss: 188301.2993, Val Loss: 188959.6617\n",
      "Epoch [943/1000], Train Loss: 188302.6723, Val Loss: 188953.4642\n",
      "Epoch [944/1000], Train Loss: 188308.0106, Val Loss: 188959.0879\n",
      "Epoch [945/1000], Train Loss: 188299.2958, Val Loss: 188987.7592\n",
      "Epoch [946/1000], Train Loss: 188298.2366, Val Loss: 188955.4301\n",
      "Epoch [947/1000], Train Loss: 188298.2553, Val Loss: 188956.1178\n",
      "Epoch [948/1000], Train Loss: 188309.1388, Val Loss: 188957.8515\n",
      "Epoch [949/1000], Train Loss: 188303.6768, Val Loss: 188975.0981\n",
      "Epoch [950/1000], Train Loss: 188303.4543, Val Loss: 188971.7915\n",
      "Epoch [951/1000], Train Loss: 188303.1705, Val Loss: 188953.1856\n",
      "Epoch [952/1000], Train Loss: 188297.5604, Val Loss: 188952.7636\n",
      "Epoch [953/1000], Train Loss: 188322.1086, Val Loss: 188957.1919\n",
      "Epoch [954/1000], Train Loss: 188304.8001, Val Loss: 188954.3308\n",
      "Epoch [955/1000], Train Loss: 188306.7281, Val Loss: 188957.6748\n",
      "Epoch [956/1000], Train Loss: 188301.5091, Val Loss: 188962.3334\n",
      "Epoch [957/1000], Train Loss: 188306.3053, Val Loss: 188962.1541\n",
      "Epoch [958/1000], Train Loss: 188301.4598, Val Loss: 189050.7355\n",
      "Epoch [959/1000], Train Loss: 188302.3182, Val Loss: 189045.7866\n",
      "Epoch [960/1000], Train Loss: 188313.4516, Val Loss: 188957.4559\n",
      "Epoch [961/1000], Train Loss: 188296.0197, Val Loss: 188952.7660\n",
      "Epoch [962/1000], Train Loss: 188297.3179, Val Loss: 188956.8588\n",
      "Epoch [963/1000], Train Loss: 188307.2685, Val Loss: 188952.7655\n",
      "Epoch [964/1000], Train Loss: 188311.3018, Val Loss: 188959.3206\n",
      "Epoch [965/1000], Train Loss: 188303.7135, Val Loss: 188953.0590\n",
      "Epoch [966/1000], Train Loss: 188300.4993, Val Loss: 188954.2516\n",
      "Epoch [967/1000], Train Loss: 188299.9218, Val Loss: 188959.6862\n",
      "Epoch [968/1000], Train Loss: 188304.0305, Val Loss: 188953.3067\n",
      "Epoch [969/1000], Train Loss: 188312.0033, Val Loss: 188954.3644\n",
      "Epoch [970/1000], Train Loss: 188313.6422, Val Loss: 188953.5903\n",
      "Epoch [971/1000], Train Loss: 188313.4612, Val Loss: 188963.6645\n",
      "Epoch [972/1000], Train Loss: 188305.3965, Val Loss: 188952.8297\n",
      "Epoch [973/1000], Train Loss: 188305.9801, Val Loss: 188954.5304\n",
      "Epoch [974/1000], Train Loss: 188308.6295, Val Loss: 188953.8357\n",
      "Epoch [975/1000], Train Loss: 188299.6581, Val Loss: 188964.8686\n",
      "Epoch [976/1000], Train Loss: 188311.6869, Val Loss: 188958.9352\n",
      "Epoch [977/1000], Train Loss: 188299.5155, Val Loss: 188958.5741\n",
      "Epoch [978/1000], Train Loss: 188319.1873, Val Loss: 188964.4368\n",
      "Epoch [979/1000], Train Loss: 188306.0049, Val Loss: 188963.5070\n",
      "Epoch [980/1000], Train Loss: 188296.9191, Val Loss: 188952.7844\n",
      "Epoch [981/1000], Train Loss: 188302.8722, Val Loss: 188955.3217\n",
      "Epoch [982/1000], Train Loss: 188304.0166, Val Loss: 188955.7733\n",
      "Epoch [983/1000], Train Loss: 188312.3010, Val Loss: 188954.0911\n",
      "Epoch [984/1000], Train Loss: 188314.8356, Val Loss: 188954.0784\n",
      "Epoch [985/1000], Train Loss: 188308.6894, Val Loss: 188956.4084\n",
      "Epoch [986/1000], Train Loss: 188304.4387, Val Loss: 188953.0753\n",
      "Epoch [987/1000], Train Loss: 188307.9380, Val Loss: 188953.4313\n",
      "Epoch [988/1000], Train Loss: 188310.5965, Val Loss: 188956.2126\n",
      "Epoch [989/1000], Train Loss: 188325.5799, Val Loss: 188975.5766\n",
      "Epoch [990/1000], Train Loss: 188298.2693, Val Loss: 188953.5972\n",
      "Epoch [991/1000], Train Loss: 188298.2413, Val Loss: 188967.9510\n",
      "Epoch [992/1000], Train Loss: 188314.9607, Val Loss: 188989.2731\n",
      "Epoch [993/1000], Train Loss: 188312.9714, Val Loss: 188952.8071\n",
      "Epoch [994/1000], Train Loss: 188294.0018, Val Loss: 188952.8230\n",
      "Epoch [995/1000], Train Loss: 188303.8753, Val Loss: 188961.3400\n",
      "Epoch [996/1000], Train Loss: 188305.7445, Val Loss: 188959.5084\n",
      "Epoch [997/1000], Train Loss: 188314.0645, Val Loss: 188952.7768\n",
      "Epoch [998/1000], Train Loss: 188301.1248, Val Loss: 188954.7049\n",
      "Epoch [999/1000], Train Loss: 188294.4784, Val Loss: 188954.6850\n",
      "Epoch [1000/1000], Train Loss: 188299.9065, Val Loss: 188952.7939\n"
     ]
    }
   ],
   "source": [
    "val_loss_list=[]\n",
    "train_loss_list=[]\n",
    "# Train the model\n",
    "epochs = 1000\n",
    "for epoch in range(epochs):\n",
    "    train_loss = 0.0\n",
    "    val_loss = 0.0\n",
    "    model.train()  # set the model to training mode\n",
    "    for x, y in train_loader:\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(x)\n",
    "        loss = criterion(output, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * x.size(0)\n",
    "    train_loss /= len(train_dataset)\n",
    "    train_loss_list.append(train_loss)\n",
    "    model.eval()  # set the model to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        for x, y in val_loader:\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            output = model(x)\n",
    "            loss = criterion(output, y)\n",
    "            val_loss += loss.item() * x.size(0)\n",
    "        val_loss /= len(val_dataset)\n",
    "        val_loss_list.append(val_loss)\n",
    "    \n",
    "\n",
    "\n",
    "    print('Epoch [{}/{}], Train Loss: {:.4f}, Val Loss: {:.4f}'.format(epoch+1, epochs, train_loss, val_loss))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_loss(train_loss, val_loss, epochs):\n",
    "    \"\"\"\n",
    "    Plots the training and validation loss against the number of epochs.\n",
    "\n",
    "    Args:\n",
    "        train_loss (list): List of training loss values.\n",
    "        val_loss (list): List of validation loss values.\n",
    "        epochs (int): Number of epochs trained for.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    plt.plot(range(1, epochs+1), train_loss, label='Training Loss')\n",
    "    plt.plot(range(1, epochs+1), val_loss, label='Validation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_loss(train_loss_list,val_loss_list,epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
