{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import copy\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArithmeticExpressionDataset(Dataset):\n",
    "    def __init__(self, X, Y):\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.X[idx]\n",
    "        y = self.Y[idx]\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HelperClass:\n",
    "    def __init__(self,X,Y,model,epochs,lr=1e-3):\n",
    "        self.model=model\n",
    "        self.epochs=epochs\n",
    "        self.optimizer=optim.Adam(self.model.parameters(), lr=lr)\n",
    "        self.X=X\n",
    "        self.Y=Y\n",
    "        self.criterion=nn.MSELoss()\n",
    "    \n",
    "    @staticmethod\n",
    "    def init_device():\n",
    "        device = torch.device('mps' if torch.backends.mps.is_available() else 'mps')\n",
    "        torch.manual_seed(1)\n",
    "        return device\n",
    "    \n",
    "    @staticmethod\n",
    "    def plot_loss(train_loss, val_loss, epochs):\n",
    "        plt.plot(range(1, epochs+1), train_loss, label='Training Loss')\n",
    "        plt.plot(range(1, epochs+1), val_loss, label='Validation Loss')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.title('Training and Validation Loss')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "    \n",
    "    def dataloader(self,X,Y,batch_size=1024):\n",
    "        #X_train, X_val, Y_train, Y_val = train_test_split(Xtr, Ytr, test_size=0.2)\n",
    "        dataset = ArithmeticExpressionDataset(X,Y)\n",
    "        dataloader=DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "        return dataloader\n",
    "\n",
    "    def saver(self,best_weights,PATH=\"./models\"):\n",
    "        name=''.join(f\"{key}{val}\" for key, val in self.model.__repr__().items())\n",
    "        path=PATH+\"/\"+name\n",
    "        torch.save(best_weights,path)\n",
    "\n",
    "    def trainer(self):\n",
    "        device=self.init_device()\n",
    "        val_loss_list=[]\n",
    "        train_loss_list=[]\n",
    "        X_train, X_val, Y_train, Y_val = train_test_split(self.X, self.Y, test_size=0.2)\n",
    "        train_loader=self.dataloader(X_train,Y_train)\n",
    "        val_loader=self.dataloader(X_val,Y_val)\n",
    "        best_valid_loss = float(\"inf\")\n",
    "        for epoch in range(self.epochs):\n",
    "            train_loss = 0.0\n",
    "            val_loss = 0.0\n",
    "            self.model.train()\n",
    "            for x, y in train_loader:\n",
    "                x = x.to(device)\n",
    "                y = y.to(device)\n",
    "                self.optimizer.zero_grad()\n",
    "                output = self.model(x)\n",
    "                loss = self.criterion(output, y)\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                train_loss += loss.item() * x.size(0)\n",
    "            train_loss /= len(train_loader.dataset)\n",
    "            train_loss_list.append(train_loss)\n",
    "            self.model.eval()\n",
    "            with torch.no_grad():\n",
    "                for x, y in val_loader:\n",
    "                    x = x.to(device)\n",
    "                    y = y.to(device)\n",
    "                    output = self.model(x)\n",
    "                    loss = self.criterion(output, y)\n",
    "                    val_loss += loss.item() * x.size(0)\n",
    "                val_loss /= len(val_loader.dataset)\n",
    "                val_loss_list.append(val_loss)\n",
    "                if val_loss < best_valid_loss:\n",
    "                    best_valid_loss=val_loss\n",
    "                    best_weights = copy.deepcopy(self.model.state_dict())\n",
    "            print('Epoch [{}/{}], Train Loss: {:.4f}, Val Loss: {:.4f}'.format(epoch+1, self.epochs, train_loss, val_loss))\n",
    "        self.plot_loss(train_loss_list,val_loss_list,self.epochs)\n",
    "        self.saver(best_weights)\n",
    "        return best_valid_loss\n",
    "    \n",
    "    def tester(self,X,Y):\n",
    "        device=self.init_device()\n",
    "        name=''.join(f\"{key}{val}\" for key, val in self.model.__repr__().items())\n",
    "        path=\"./models/\"+name\n",
    "        self.model.load_state_dict(torch.load(path))\n",
    "        test_loader=self.dataloader(X,Y)\n",
    "        self.model.eval()\n",
    "        test_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for x, y in test_loader:\n",
    "                x = x.to(device)\n",
    "                y = y.to(device)\n",
    "                output = self.model(x)\n",
    "                loss = self.criterion(output, y)\n",
    "                test_loss += loss.item()*x.size(0)\n",
    "            test_loss /= len(test_loader.dataset)\n",
    "            print('Test Loss: {:.6f}'.format(test_loss))\n",
    "        return test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNModel(nn.Module):\n",
    "    def __repr__(self):\n",
    "        return {\"model\":\"RNN\",\"ES\":self.embedding_size,\"HS\":self.hidden_size}\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers, embedding_size):\n",
    "        super(RNNModel, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.num_layers = num_layers\n",
    "        self.embedding_size = embedding_size\n",
    "        #Model Arch\n",
    "        self.embedding = nn.Embedding(input_size, embedding_size)\n",
    "        self.rnn = nn.RNN(embedding_size, hidden_size, num_layers, batch_first=True,nonlinearity=\"relu\")\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        out, _ = self.rnn(x, h0)\n",
    "        out = self.fc(out[:, -1, :])\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "torch.manual_seed(1)\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load(\"./data/ae_data.npz\")\n",
    "\n",
    "Str = data[\"Str\"]\n",
    "Xtr = torch.from_numpy(data[\"Xtr\"]).type(torch.int)\n",
    "Ytr = torch.from_numpy(data[\"Ytr\"]).type(torch.float)\n",
    "\n",
    "Ste = data[\"Ste\"]\n",
    "Xte = torch.from_numpy(data[\"Xte\"]).type(torch.int)\n",
    "Yte = torch.from_numpy(data[\"Yte\"]).type(torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 2\n",
    "batch_size = 1024\n",
    "learning_rate = 1e-4\n",
    "hidden_size = 64\n",
    "num_layers = 4\n",
    "embedding_size=16\n",
    "model = RNNModel(input_size=13, hidden_size=hidden_size, num_layers=num_layers, output_size=1,embedding_size=embedding_size).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn=HelperClass(Xtr,Ytr,model,num_epochs,device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerModel(nn.Module):\n",
    "    def __repr__(self):\n",
    "        return {\"model\":\"Transformer\",\"DM\":self.d_model,\"NH\":self.nhead,\"NL\":self.num_layers}\n",
    "    \n",
    "    \n",
    "    def __init__(self, input_dim, d_model, nhead, num_layers, output_dim):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.d_model=d_model\n",
    "        self.nhead=nhead\n",
    "        self.num_layers=num_layers\n",
    "\n",
    "        # Embedding layer\n",
    "        self.embedding = nn.Embedding(input_dim, d_model)\n",
    "\n",
    "        # Transformer model\n",
    "        self.transformer = nn.Transformer(d_model=d_model, nhead=nhead, num_encoder_layers=num_layers,\n",
    "                                          num_decoder_layers=num_layers)\n",
    "\n",
    "        # Linear output layer\n",
    "        self.output = nn.Linear(d_model, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Prepare the input tensor for the transformer model\n",
    "        x = self.embedding(x)\n",
    "        x = x.permute(1, 0, 2)\n",
    "\n",
    "        # Forward propagate transformer model\n",
    "        out = self.transformer(x, x)\n",
    "\n",
    "        # Decode the output of the transformer model\n",
    "        out = self.output(out[-1, :, :])\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_vocab_size=1\n",
    "num_features=13\n",
    "d_model=64\n",
    "nhead=4\n",
    "num_layers=4\n",
    "model = TransformerModel(num_features,d_model,nhead,num_layers,output_vocab_size).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans=HelperClass(Xtr,Ytr,model,num_epochs,device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPModel(nn.Module):\n",
    "    def __repr__(self):\n",
    "        return {\"model\":\"MLP\",\"FL\":self.input_dim,\"HL\":self.hidden_dim}\n",
    "    \n",
    "    def __init__(self, input_dimension, hidden_dimension, output_dimension):\n",
    "        super(MLPModel, self).__init__()\n",
    "        self.input_dim = input_dimension\n",
    "        self.hidden_dim = hidden_dimension\n",
    "        self.output_dim = output_dimension\n",
    "        self.layers=nn.Sequential(\n",
    "            nn.Linear(input_dimension, hidden_dimension),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dimension, hidden_dimension),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dimension, output_dimension)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x=x.to(torch.float32)\n",
    "        x = self.layers(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLPModel(7, 512, 1).to(torch.device(\"mps\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp=HelperClass(Xtr,Ytr,model,1000,torch.device(\"cpu\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1000], Train Loss: 175367.1403, Val Loss: 147450.6169\n",
      "Epoch [2/1000], Train Loss: 128124.7341, Val Loss: 120034.8819\n",
      "Epoch [3/1000], Train Loss: 113241.4988, Val Loss: 114014.2295\n",
      "Epoch [4/1000], Train Loss: 109870.1180, Val Loss: 111857.8111\n",
      "Epoch [5/1000], Train Loss: 107892.0471, Val Loss: 109866.1818\n",
      "Epoch [6/1000], Train Loss: 106462.4312, Val Loss: 109387.2518\n",
      "Epoch [7/1000], Train Loss: 105335.0437, Val Loss: 107388.5142\n",
      "Epoch [8/1000], Train Loss: 104112.9704, Val Loss: 106406.4295\n",
      "Epoch [9/1000], Train Loss: 102933.0935, Val Loss: 105067.3419\n",
      "Epoch [10/1000], Train Loss: 101829.8720, Val Loss: 104580.5915\n",
      "Epoch [11/1000], Train Loss: 100670.2126, Val Loss: 102764.6740\n",
      "Epoch [12/1000], Train Loss: 99282.4828, Val Loss: 102078.3423\n",
      "Epoch [13/1000], Train Loss: 97692.6617, Val Loss: 99994.7327\n",
      "Epoch [14/1000], Train Loss: 96150.9796, Val Loss: 98667.3356\n",
      "Epoch [15/1000], Train Loss: 94672.6552, Val Loss: 95948.4840\n",
      "Epoch [16/1000], Train Loss: 92523.5482, Val Loss: 94257.4393\n",
      "Epoch [17/1000], Train Loss: 90001.2023, Val Loss: 90880.8031\n",
      "Epoch [18/1000], Train Loss: 87358.7859, Val Loss: 88033.4579\n",
      "Epoch [19/1000], Train Loss: 83377.9052, Val Loss: 86588.9437\n",
      "Epoch [20/1000], Train Loss: 79315.3824, Val Loss: 78848.4829\n",
      "Epoch [21/1000], Train Loss: 74309.4176, Val Loss: 73362.1801\n",
      "Epoch [22/1000], Train Loss: 68512.2026, Val Loss: 67885.1673\n",
      "Epoch [23/1000], Train Loss: 62195.9001, Val Loss: 67136.2374\n",
      "Epoch [24/1000], Train Loss: 59313.3748, Val Loss: 55812.9627\n",
      "Epoch [25/1000], Train Loss: 52162.8919, Val Loss: 50875.7591\n",
      "Epoch [26/1000], Train Loss: 46827.3824, Val Loss: 45349.9369\n",
      "Epoch [27/1000], Train Loss: 41241.9000, Val Loss: 41677.0096\n",
      "Epoch [28/1000], Train Loss: 38124.5362, Val Loss: 36579.7122\n",
      "Epoch [29/1000], Train Loss: 33259.5634, Val Loss: 32012.9983\n",
      "Epoch [30/1000], Train Loss: 29323.6810, Val Loss: 28295.1935\n",
      "Epoch [31/1000], Train Loss: 26354.9417, Val Loss: 25239.7845\n",
      "Epoch [32/1000], Train Loss: 23311.6348, Val Loss: 22158.8858\n",
      "Epoch [33/1000], Train Loss: 20330.6265, Val Loss: 19635.1342\n",
      "Epoch [34/1000], Train Loss: 17875.9003, Val Loss: 17572.4603\n",
      "Epoch [35/1000], Train Loss: 15702.9095, Val Loss: 15720.6684\n",
      "Epoch [36/1000], Train Loss: 15389.2206, Val Loss: 22493.2731\n",
      "Epoch [37/1000], Train Loss: 12784.3833, Val Loss: 12551.1326\n",
      "Epoch [38/1000], Train Loss: 11411.8128, Val Loss: 11063.2807\n",
      "Epoch [39/1000], Train Loss: 10075.9946, Val Loss: 10021.5628\n",
      "Epoch [40/1000], Train Loss: 9341.1322, Val Loss: 9798.0042\n",
      "Epoch [41/1000], Train Loss: 8314.2294, Val Loss: 8306.1427\n",
      "Epoch [42/1000], Train Loss: 7808.7790, Val Loss: 7619.3804\n",
      "Epoch [43/1000], Train Loss: 7317.1943, Val Loss: 7150.1574\n",
      "Epoch [44/1000], Train Loss: 6490.3185, Val Loss: 6593.4921\n",
      "Epoch [45/1000], Train Loss: 6254.6711, Val Loss: 6304.7134\n",
      "Epoch [46/1000], Train Loss: 5672.4267, Val Loss: 5673.7266\n",
      "Epoch [47/1000], Train Loss: 5274.4481, Val Loss: 6657.2521\n",
      "Epoch [48/1000], Train Loss: 5193.0614, Val Loss: 5030.8101\n",
      "Epoch [49/1000], Train Loss: 4699.0353, Val Loss: 4969.7793\n",
      "Epoch [50/1000], Train Loss: 4871.8777, Val Loss: 4693.1619\n",
      "Epoch [51/1000], Train Loss: 4128.1113, Val Loss: 4659.2715\n",
      "Epoch [52/1000], Train Loss: 3956.9340, Val Loss: 4175.6789\n",
      "Epoch [53/1000], Train Loss: 3863.1100, Val Loss: 5060.0389\n",
      "Epoch [54/1000], Train Loss: 3526.9928, Val Loss: 3477.1489\n",
      "Epoch [55/1000], Train Loss: 3242.5801, Val Loss: 4124.1527\n",
      "Epoch [56/1000], Train Loss: 3323.4161, Val Loss: 3568.8889\n",
      "Epoch [57/1000], Train Loss: 3253.4031, Val Loss: 3596.4599\n",
      "Epoch [58/1000], Train Loss: 2851.0909, Val Loss: 3040.6899\n",
      "Epoch [59/1000], Train Loss: 2754.9604, Val Loss: 2757.1818\n",
      "Epoch [60/1000], Train Loss: 2617.5861, Val Loss: 2696.9460\n",
      "Epoch [61/1000], Train Loss: 2468.2985, Val Loss: 2520.7049\n",
      "Epoch [62/1000], Train Loss: 2355.7020, Val Loss: 2461.1005\n",
      "Epoch [63/1000], Train Loss: 2338.4449, Val Loss: 2468.0510\n",
      "Epoch [64/1000], Train Loss: 2247.4411, Val Loss: 2269.1145\n",
      "Epoch [65/1000], Train Loss: 2300.1160, Val Loss: 2109.1110\n",
      "Epoch [66/1000], Train Loss: 2298.8957, Val Loss: 2376.8971\n",
      "Epoch [67/1000], Train Loss: 2082.4716, Val Loss: 2243.0223\n",
      "Epoch [68/1000], Train Loss: 1901.6917, Val Loss: 1914.1732\n",
      "Epoch [69/1000], Train Loss: 1874.6673, Val Loss: 1924.5238\n",
      "Epoch [70/1000], Train Loss: 2026.0574, Val Loss: 1787.4482\n",
      "Epoch [71/1000], Train Loss: 1823.7900, Val Loss: 2310.4582\n",
      "Epoch [72/1000], Train Loss: 1810.5977, Val Loss: 1923.2100\n",
      "Epoch [73/1000], Train Loss: 1712.3691, Val Loss: 1786.1393\n",
      "Epoch [74/1000], Train Loss: 1586.8677, Val Loss: 1576.8893\n",
      "Epoch [75/1000], Train Loss: 1610.5832, Val Loss: 2011.3172\n",
      "Epoch [76/1000], Train Loss: 1498.0505, Val Loss: 1513.0481\n",
      "Epoch [77/1000], Train Loss: 2829.0444, Val Loss: 1472.6056\n",
      "Epoch [78/1000], Train Loss: 1379.2354, Val Loss: 1533.4593\n",
      "Epoch [79/1000], Train Loss: 1366.8059, Val Loss: 1879.9669\n",
      "Epoch [80/1000], Train Loss: 1378.2186, Val Loss: 1446.3721\n",
      "Epoch [81/1000], Train Loss: 1310.4370, Val Loss: 1643.7705\n",
      "Epoch [82/1000], Train Loss: 1428.4234, Val Loss: 1298.6456\n",
      "Epoch [83/1000], Train Loss: 1374.4775, Val Loss: 1293.4104\n",
      "Epoch [84/1000], Train Loss: 1389.7106, Val Loss: 1675.9389\n",
      "Epoch [85/1000], Train Loss: 1220.7704, Val Loss: 1170.1924\n",
      "Epoch [86/1000], Train Loss: 2030.6656, Val Loss: 1166.9912\n",
      "Epoch [87/1000], Train Loss: 1250.5043, Val Loss: 1129.7101\n",
      "Epoch [88/1000], Train Loss: 1073.7318, Val Loss: 1141.9043\n",
      "Epoch [89/1000], Train Loss: 1101.7526, Val Loss: 1079.4110\n",
      "Epoch [90/1000], Train Loss: 1063.5550, Val Loss: 1107.3121\n",
      "Epoch [91/1000], Train Loss: 1060.7224, Val Loss: 1026.2783\n",
      "Epoch [92/1000], Train Loss: 1045.0270, Val Loss: 1281.6709\n",
      "Epoch [93/1000], Train Loss: 1188.7052, Val Loss: 1020.5165\n",
      "Epoch [94/1000], Train Loss: 1003.9397, Val Loss: 1001.9296\n",
      "Epoch [95/1000], Train Loss: 1767.8462, Val Loss: 1169.3916\n",
      "Epoch [96/1000], Train Loss: 975.1848, Val Loss: 950.7854\n",
      "Epoch [97/1000], Train Loss: 974.1986, Val Loss: 908.4788\n",
      "Epoch [98/1000], Train Loss: 920.0232, Val Loss: 1033.9043\n",
      "Epoch [99/1000], Train Loss: 932.1370, Val Loss: 903.0846\n",
      "Epoch [100/1000], Train Loss: 1409.0842, Val Loss: 1030.3353\n",
      "Epoch [101/1000], Train Loss: 1129.3247, Val Loss: 864.7204\n",
      "Epoch [102/1000], Train Loss: 884.0342, Val Loss: 905.2050\n",
      "Epoch [103/1000], Train Loss: 919.7910, Val Loss: 815.8985\n",
      "Epoch [104/1000], Train Loss: 840.3197, Val Loss: 834.7876\n",
      "Epoch [105/1000], Train Loss: 829.7321, Val Loss: 813.5878\n",
      "Epoch [106/1000], Train Loss: 932.2645, Val Loss: 781.3869\n",
      "Epoch [107/1000], Train Loss: 2268.8098, Val Loss: 1373.7049\n",
      "Epoch [108/1000], Train Loss: 807.9960, Val Loss: 879.9755\n",
      "Epoch [109/1000], Train Loss: 750.7270, Val Loss: 787.6499\n",
      "Epoch [110/1000], Train Loss: 762.0430, Val Loss: 776.5611\n",
      "Epoch [111/1000], Train Loss: 770.5955, Val Loss: 853.2563\n",
      "Epoch [112/1000], Train Loss: 730.3029, Val Loss: 727.9904\n",
      "Epoch [113/1000], Train Loss: 703.4818, Val Loss: 781.4545\n",
      "Epoch [114/1000], Train Loss: 785.6616, Val Loss: 693.6313\n",
      "Epoch [115/1000], Train Loss: 762.2634, Val Loss: 735.1223\n",
      "Epoch [116/1000], Train Loss: 689.9915, Val Loss: 732.3651\n",
      "Epoch [117/1000], Train Loss: 742.6598, Val Loss: 882.9134\n",
      "Epoch [118/1000], Train Loss: 1458.6888, Val Loss: 1092.4208\n",
      "Epoch [119/1000], Train Loss: 1580.1605, Val Loss: 993.8842\n",
      "Epoch [120/1000], Train Loss: 652.4570, Val Loss: 696.8281\n",
      "Epoch [121/1000], Train Loss: 638.6219, Val Loss: 639.7726\n",
      "Epoch [122/1000], Train Loss: 618.0594, Val Loss: 658.2641\n",
      "Epoch [123/1000], Train Loss: 607.0181, Val Loss: 733.5158\n",
      "Epoch [124/1000], Train Loss: 620.6471, Val Loss: 634.8008\n",
      "Epoch [125/1000], Train Loss: 614.2383, Val Loss: 869.2386\n",
      "Epoch [126/1000], Train Loss: 864.2999, Val Loss: 988.6874\n",
      "Epoch [127/1000], Train Loss: 631.8851, Val Loss: 615.2533\n",
      "Epoch [128/1000], Train Loss: 607.6564, Val Loss: 609.4061\n",
      "Epoch [129/1000], Train Loss: 579.2085, Val Loss: 598.5578\n",
      "Epoch [130/1000], Train Loss: 692.3027, Val Loss: 570.9313\n",
      "Epoch [131/1000], Train Loss: 830.4186, Val Loss: 573.7472\n",
      "Epoch [132/1000], Train Loss: 757.4849, Val Loss: 582.5483\n",
      "Epoch [133/1000], Train Loss: 666.1933, Val Loss: 1130.0943\n",
      "Epoch [134/1000], Train Loss: 671.8844, Val Loss: 668.4148\n",
      "Epoch [135/1000], Train Loss: 597.6009, Val Loss: 742.1225\n",
      "Epoch [136/1000], Train Loss: 592.2760, Val Loss: 531.3933\n",
      "Epoch [137/1000], Train Loss: 636.4297, Val Loss: 530.9161\n",
      "Epoch [138/1000], Train Loss: 644.7991, Val Loss: 1103.3010\n",
      "Epoch [139/1000], Train Loss: 697.8337, Val Loss: 526.7584\n",
      "Epoch [140/1000], Train Loss: 529.0863, Val Loss: 582.4471\n",
      "Epoch [141/1000], Train Loss: 611.7857, Val Loss: 712.3393\n",
      "Epoch [142/1000], Train Loss: 623.7084, Val Loss: 1365.3356\n",
      "Epoch [143/1000], Train Loss: 606.5995, Val Loss: 521.7610\n",
      "Epoch [144/1000], Train Loss: 513.2279, Val Loss: 847.7814\n",
      "Epoch [145/1000], Train Loss: 989.4885, Val Loss: 1123.3820\n",
      "Epoch [146/1000], Train Loss: 586.5430, Val Loss: 607.2880\n",
      "Epoch [147/1000], Train Loss: 547.8682, Val Loss: 817.8801\n",
      "Epoch [148/1000], Train Loss: 568.5671, Val Loss: 701.0192\n",
      "Epoch [149/1000], Train Loss: 504.5284, Val Loss: 730.1527\n",
      "Epoch [150/1000], Train Loss: 538.1713, Val Loss: 538.9283\n",
      "Epoch [151/1000], Train Loss: 845.8934, Val Loss: 652.7632\n",
      "Epoch [152/1000], Train Loss: 626.6786, Val Loss: 450.1157\n",
      "Epoch [153/1000], Train Loss: 503.3594, Val Loss: 447.4858\n",
      "Epoch [154/1000], Train Loss: 524.4463, Val Loss: 571.4579\n",
      "Epoch [155/1000], Train Loss: 489.8508, Val Loss: 580.2831\n",
      "Epoch [156/1000], Train Loss: 480.6305, Val Loss: 850.6541\n",
      "Epoch [157/1000], Train Loss: 668.8685, Val Loss: 1015.2601\n",
      "Epoch [158/1000], Train Loss: 622.4103, Val Loss: 885.2040\n",
      "Epoch [159/1000], Train Loss: 692.6450, Val Loss: 454.3290\n",
      "Epoch [160/1000], Train Loss: 419.6463, Val Loss: 436.4346\n",
      "Epoch [161/1000], Train Loss: 717.0886, Val Loss: 660.5074\n",
      "Epoch [162/1000], Train Loss: 604.1386, Val Loss: 2310.6767\n",
      "Epoch [163/1000], Train Loss: 3409.0211, Val Loss: 597.7290\n",
      "Epoch [164/1000], Train Loss: 467.8129, Val Loss: 422.3279\n",
      "Epoch [165/1000], Train Loss: 387.7832, Val Loss: 429.5479\n",
      "Epoch [166/1000], Train Loss: 398.2405, Val Loss: 402.5948\n",
      "Epoch [167/1000], Train Loss: 381.3491, Val Loss: 451.2652\n",
      "Epoch [168/1000], Train Loss: 432.7714, Val Loss: 559.4655\n",
      "Epoch [169/1000], Train Loss: 383.0279, Val Loss: 434.5026\n",
      "Epoch [170/1000], Train Loss: 366.0202, Val Loss: 385.2478\n",
      "Epoch [171/1000], Train Loss: 380.0998, Val Loss: 383.6247\n",
      "Epoch [172/1000], Train Loss: 387.5234, Val Loss: 438.4002\n",
      "Epoch [173/1000], Train Loss: 393.1416, Val Loss: 440.1774\n",
      "Epoch [174/1000], Train Loss: 394.3622, Val Loss: 538.6683\n",
      "Epoch [175/1000], Train Loss: 379.3355, Val Loss: 391.1786\n",
      "Epoch [176/1000], Train Loss: 417.4272, Val Loss: 414.7081\n",
      "Epoch [177/1000], Train Loss: 441.5198, Val Loss: 366.5312\n",
      "Epoch [178/1000], Train Loss: 680.0953, Val Loss: 418.1994\n",
      "Epoch [179/1000], Train Loss: 394.4758, Val Loss: 350.2643\n",
      "Epoch [180/1000], Train Loss: 498.3316, Val Loss: 402.7819\n",
      "Epoch [181/1000], Train Loss: 533.8540, Val Loss: 521.6019\n",
      "Epoch [182/1000], Train Loss: 358.3387, Val Loss: 476.1285\n",
      "Epoch [183/1000], Train Loss: 395.7064, Val Loss: 396.0873\n",
      "Epoch [184/1000], Train Loss: 348.6588, Val Loss: 436.5294\n",
      "Epoch [185/1000], Train Loss: 351.4483, Val Loss: 337.6872\n",
      "Epoch [186/1000], Train Loss: 382.2271, Val Loss: 364.4113\n",
      "Epoch [187/1000], Train Loss: 997.2256, Val Loss: 454.9515\n",
      "Epoch [188/1000], Train Loss: 422.7012, Val Loss: 813.3881\n",
      "Epoch [189/1000], Train Loss: 428.3204, Val Loss: 320.2145\n",
      "Epoch [190/1000], Train Loss: 314.6050, Val Loss: 372.5307\n",
      "Epoch [191/1000], Train Loss: 437.4328, Val Loss: 1400.3839\n",
      "Epoch [192/1000], Train Loss: 661.3693, Val Loss: 341.8249\n",
      "Epoch [193/1000], Train Loss: 328.4515, Val Loss: 414.4105\n",
      "Epoch [194/1000], Train Loss: 487.2017, Val Loss: 339.9437\n",
      "Epoch [195/1000], Train Loss: 376.9006, Val Loss: 832.0575\n",
      "Epoch [196/1000], Train Loss: 404.8064, Val Loss: 431.0303\n",
      "Epoch [197/1000], Train Loss: 344.0984, Val Loss: 512.8118\n",
      "Epoch [198/1000], Train Loss: 579.4266, Val Loss: 339.0446\n",
      "Epoch [199/1000], Train Loss: 346.3049, Val Loss: 300.7825\n",
      "Epoch [200/1000], Train Loss: 406.2972, Val Loss: 763.2896\n",
      "Epoch [201/1000], Train Loss: 554.1759, Val Loss: 404.0667\n",
      "Epoch [202/1000], Train Loss: 479.7446, Val Loss: 727.4857\n",
      "Epoch [203/1000], Train Loss: 328.5771, Val Loss: 1658.7560\n",
      "Epoch [204/1000], Train Loss: 415.2635, Val Loss: 336.0026\n",
      "Epoch [205/1000], Train Loss: 342.8398, Val Loss: 376.3830\n",
      "Epoch [206/1000], Train Loss: 332.3372, Val Loss: 288.6458\n",
      "Epoch [207/1000], Train Loss: 646.7752, Val Loss: 1085.6856\n",
      "Epoch [208/1000], Train Loss: 460.9470, Val Loss: 291.9570\n",
      "Epoch [209/1000], Train Loss: 297.1293, Val Loss: 434.5749\n",
      "Epoch [210/1000], Train Loss: 370.5530, Val Loss: 307.4800\n",
      "Epoch [211/1000], Train Loss: 395.1613, Val Loss: 897.0108\n",
      "Epoch [212/1000], Train Loss: 1005.0711, Val Loss: 1352.3839\n",
      "Epoch [213/1000], Train Loss: 561.5989, Val Loss: 294.1825\n",
      "Epoch [214/1000], Train Loss: 268.1577, Val Loss: 271.1627\n",
      "Epoch [215/1000], Train Loss: 322.7125, Val Loss: 275.9471\n",
      "Epoch [216/1000], Train Loss: 290.9490, Val Loss: 332.7206\n",
      "Epoch [217/1000], Train Loss: 288.0985, Val Loss: 269.1942\n",
      "Epoch [218/1000], Train Loss: 293.4045, Val Loss: 629.8954\n",
      "Epoch [219/1000], Train Loss: 323.3189, Val Loss: 279.8407\n",
      "Epoch [220/1000], Train Loss: 268.1600, Val Loss: 267.0621\n",
      "Epoch [221/1000], Train Loss: 299.6520, Val Loss: 263.6896\n",
      "Epoch [222/1000], Train Loss: 399.1356, Val Loss: 473.5019\n",
      "Epoch [223/1000], Train Loss: 486.1998, Val Loss: 275.0993\n",
      "Epoch [224/1000], Train Loss: 275.5313, Val Loss: 1440.1889\n",
      "Epoch [225/1000], Train Loss: 735.2549, Val Loss: 364.5795\n",
      "Epoch [226/1000], Train Loss: 289.7602, Val Loss: 259.3980\n",
      "Epoch [227/1000], Train Loss: 616.2849, Val Loss: 308.4896\n",
      "Epoch [228/1000], Train Loss: 4580.2471, Val Loss: 609.0875\n",
      "Epoch [229/1000], Train Loss: 272.9519, Val Loss: 248.4777\n",
      "Epoch [230/1000], Train Loss: 233.5057, Val Loss: 354.3566\n",
      "Epoch [231/1000], Train Loss: 231.1412, Val Loss: 267.2683\n",
      "Epoch [232/1000], Train Loss: 222.4501, Val Loss: 232.3928\n",
      "Epoch [233/1000], Train Loss: 234.3435, Val Loss: 239.6460\n",
      "Epoch [234/1000], Train Loss: 233.6591, Val Loss: 233.4200\n",
      "Epoch [235/1000], Train Loss: 212.4406, Val Loss: 255.4884\n",
      "Epoch [236/1000], Train Loss: 230.0175, Val Loss: 226.9023\n",
      "Epoch [237/1000], Train Loss: 236.0279, Val Loss: 232.5928\n",
      "Epoch [238/1000], Train Loss: 241.7617, Val Loss: 260.9315\n",
      "Epoch [239/1000], Train Loss: 210.6999, Val Loss: 281.3442\n",
      "Epoch [240/1000], Train Loss: 263.1757, Val Loss: 227.4697\n",
      "Epoch [241/1000], Train Loss: 211.8788, Val Loss: 268.7879\n",
      "Epoch [242/1000], Train Loss: 223.3556, Val Loss: 264.2361\n",
      "Epoch [243/1000], Train Loss: 370.4624, Val Loss: 233.6850\n",
      "Epoch [244/1000], Train Loss: 242.1963, Val Loss: 342.9242\n",
      "Epoch [245/1000], Train Loss: 229.7205, Val Loss: 215.2936\n",
      "Epoch [246/1000], Train Loss: 221.4553, Val Loss: 237.1894\n",
      "Epoch [247/1000], Train Loss: 342.4887, Val Loss: 231.9530\n",
      "Epoch [248/1000], Train Loss: 232.8607, Val Loss: 288.8670\n",
      "Epoch [249/1000], Train Loss: 279.2503, Val Loss: 259.7855\n",
      "Epoch [250/1000], Train Loss: 230.0468, Val Loss: 217.7165\n",
      "Epoch [251/1000], Train Loss: 317.0740, Val Loss: 282.9361\n",
      "Epoch [252/1000], Train Loss: 684.7782, Val Loss: 4530.6751\n",
      "Epoch [253/1000], Train Loss: 3684.9683, Val Loss: 286.8038\n",
      "Epoch [254/1000], Train Loss: 213.1610, Val Loss: 212.4052\n",
      "Epoch [255/1000], Train Loss: 192.5219, Val Loss: 321.1522\n",
      "Epoch [256/1000], Train Loss: 193.0157, Val Loss: 247.3674\n",
      "Epoch [257/1000], Train Loss: 187.4396, Val Loss: 209.9841\n",
      "Epoch [258/1000], Train Loss: 180.2431, Val Loss: 215.4133\n",
      "Epoch [259/1000], Train Loss: 184.7024, Val Loss: 260.8239\n",
      "Epoch [260/1000], Train Loss: 196.4611, Val Loss: 229.7990\n",
      "Epoch [261/1000], Train Loss: 182.6221, Val Loss: 203.0064\n",
      "Epoch [262/1000], Train Loss: 190.0938, Val Loss: 194.7366\n",
      "Epoch [263/1000], Train Loss: 179.5743, Val Loss: 190.1640\n",
      "Epoch [264/1000], Train Loss: 195.0114, Val Loss: 260.5959\n",
      "Epoch [265/1000], Train Loss: 204.3369, Val Loss: 194.4293\n",
      "Epoch [266/1000], Train Loss: 183.7917, Val Loss: 197.2114\n",
      "Epoch [267/1000], Train Loss: 197.9225, Val Loss: 205.6589\n",
      "Epoch [268/1000], Train Loss: 208.0122, Val Loss: 203.4397\n",
      "Epoch [269/1000], Train Loss: 543.2151, Val Loss: 1511.1188\n",
      "Epoch [270/1000], Train Loss: 457.6967, Val Loss: 471.2853\n",
      "Epoch [271/1000], Train Loss: 195.0111, Val Loss: 262.5852\n",
      "Epoch [272/1000], Train Loss: 212.3201, Val Loss: 186.4385\n",
      "Epoch [273/1000], Train Loss: 179.7291, Val Loss: 238.1312\n",
      "Epoch [274/1000], Train Loss: 281.9406, Val Loss: 567.3453\n",
      "Epoch [275/1000], Train Loss: 199.8887, Val Loss: 205.4862\n",
      "Epoch [276/1000], Train Loss: 178.0577, Val Loss: 196.2932\n",
      "Epoch [277/1000], Train Loss: 2939.1330, Val Loss: 8723.0189\n",
      "Epoch [278/1000], Train Loss: 2149.8344, Val Loss: 206.6387\n",
      "Epoch [279/1000], Train Loss: 171.9147, Val Loss: 192.8511\n",
      "Epoch [280/1000], Train Loss: 159.5254, Val Loss: 179.6268\n",
      "Epoch [281/1000], Train Loss: 163.2013, Val Loss: 195.2220\n",
      "Epoch [282/1000], Train Loss: 165.5496, Val Loss: 175.8241\n",
      "Epoch [283/1000], Train Loss: 152.6729, Val Loss: 173.2141\n",
      "Epoch [284/1000], Train Loss: 157.1923, Val Loss: 188.8435\n",
      "Epoch [285/1000], Train Loss: 159.1230, Val Loss: 226.6639\n",
      "Epoch [286/1000], Train Loss: 159.2444, Val Loss: 170.4975\n",
      "Epoch [287/1000], Train Loss: 168.3587, Val Loss: 170.1242\n",
      "Epoch [288/1000], Train Loss: 157.6677, Val Loss: 230.8034\n",
      "Epoch [289/1000], Train Loss: 170.4672, Val Loss: 167.5718\n",
      "Epoch [290/1000], Train Loss: 155.1709, Val Loss: 208.2853\n",
      "Epoch [291/1000], Train Loss: 159.6246, Val Loss: 163.4751\n",
      "Epoch [292/1000], Train Loss: 168.5559, Val Loss: 169.0687\n",
      "Epoch [293/1000], Train Loss: 167.1612, Val Loss: 180.9159\n",
      "Epoch [294/1000], Train Loss: 164.6476, Val Loss: 204.3112\n",
      "Epoch [295/1000], Train Loss: 190.5580, Val Loss: 200.0351\n",
      "Epoch [296/1000], Train Loss: 361.9243, Val Loss: 233.1664\n",
      "Epoch [297/1000], Train Loss: 181.7993, Val Loss: 170.8291\n",
      "Epoch [298/1000], Train Loss: 219.4660, Val Loss: 292.2604\n",
      "Epoch [299/1000], Train Loss: 203.3858, Val Loss: 243.1896\n",
      "Epoch [300/1000], Train Loss: 160.9154, Val Loss: 205.5391\n",
      "Epoch [301/1000], Train Loss: 533.0591, Val Loss: 178.9021\n",
      "Epoch [302/1000], Train Loss: 159.4597, Val Loss: 155.1256\n",
      "Epoch [303/1000], Train Loss: 154.7500, Val Loss: 199.1747\n",
      "Epoch [304/1000], Train Loss: 250.6062, Val Loss: 159.4837\n",
      "Epoch [305/1000], Train Loss: 172.1249, Val Loss: 313.0392\n",
      "Epoch [306/1000], Train Loss: 970.1888, Val Loss: 707.0956\n",
      "Epoch [307/1000], Train Loss: 203.9390, Val Loss: 169.4419\n",
      "Epoch [308/1000], Train Loss: 180.6533, Val Loss: 165.8098\n",
      "Epoch [309/1000], Train Loss: 148.2075, Val Loss: 295.5791\n",
      "Epoch [310/1000], Train Loss: 228.5706, Val Loss: 454.9348\n",
      "Epoch [311/1000], Train Loss: 180.0106, Val Loss: 224.8157\n",
      "Epoch [312/1000], Train Loss: 504.9519, Val Loss: 175.0547\n",
      "Epoch [313/1000], Train Loss: 403.5839, Val Loss: 758.5477\n",
      "Epoch [314/1000], Train Loss: 2256.1977, Val Loss: 432.8558\n",
      "Epoch [315/1000], Train Loss: 182.2459, Val Loss: 149.5465\n",
      "Epoch [316/1000], Train Loss: 130.7331, Val Loss: 157.3513\n",
      "Epoch [317/1000], Train Loss: 128.2419, Val Loss: 160.2105\n",
      "Epoch [318/1000], Train Loss: 131.7320, Val Loss: 139.7969\n",
      "Epoch [319/1000], Train Loss: 125.7342, Val Loss: 142.7529\n",
      "Epoch [320/1000], Train Loss: 124.6915, Val Loss: 146.6630\n",
      "Epoch [321/1000], Train Loss: 144.1507, Val Loss: 144.9286\n",
      "Epoch [322/1000], Train Loss: 151.4064, Val Loss: 160.7224\n",
      "Epoch [323/1000], Train Loss: 130.8656, Val Loss: 140.6079\n",
      "Epoch [324/1000], Train Loss: 135.2353, Val Loss: 135.3041\n",
      "Epoch [325/1000], Train Loss: 153.2624, Val Loss: 153.2001\n",
      "Epoch [326/1000], Train Loss: 139.9313, Val Loss: 163.5437\n",
      "Epoch [327/1000], Train Loss: 317.8240, Val Loss: 342.5246\n",
      "Epoch [328/1000], Train Loss: 369.4840, Val Loss: 228.5726\n",
      "Epoch [329/1000], Train Loss: 145.3250, Val Loss: 179.8168\n",
      "Epoch [330/1000], Train Loss: 144.5016, Val Loss: 165.6880\n",
      "Epoch [331/1000], Train Loss: 165.9281, Val Loss: 242.3076\n",
      "Epoch [332/1000], Train Loss: 987.7339, Val Loss: 594.5551\n",
      "Epoch [333/1000], Train Loss: 148.4157, Val Loss: 164.9322\n",
      "Epoch [334/1000], Train Loss: 125.5502, Val Loss: 163.6383\n",
      "Epoch [335/1000], Train Loss: 120.0400, Val Loss: 186.3185\n",
      "Epoch [336/1000], Train Loss: 141.0272, Val Loss: 238.8357\n",
      "Epoch [337/1000], Train Loss: 168.8869, Val Loss: 214.5006\n",
      "Epoch [338/1000], Train Loss: 140.7079, Val Loss: 279.5695\n",
      "Epoch [339/1000], Train Loss: 384.3470, Val Loss: 155.5291\n",
      "Epoch [340/1000], Train Loss: 352.1912, Val Loss: 183.7108\n",
      "Epoch [341/1000], Train Loss: 133.2483, Val Loss: 417.3113\n",
      "Epoch [342/1000], Train Loss: 272.4211, Val Loss: 160.9796\n",
      "Epoch [343/1000], Train Loss: 152.3867, Val Loss: 304.0971\n",
      "Epoch [344/1000], Train Loss: 158.9015, Val Loss: 126.9731\n",
      "Epoch [345/1000], Train Loss: 2568.7524, Val Loss: 40406.8991\n",
      "Epoch [346/1000], Train Loss: 4517.7502, Val Loss: 210.7749\n",
      "Epoch [347/1000], Train Loss: 144.1342, Val Loss: 173.9523\n",
      "Epoch [348/1000], Train Loss: 131.5071, Val Loss: 146.6087\n",
      "Epoch [349/1000], Train Loss: 115.4456, Val Loss: 130.4193\n",
      "Epoch [350/1000], Train Loss: 109.4910, Val Loss: 165.8024\n",
      "Epoch [351/1000], Train Loss: 110.0888, Val Loss: 145.1124\n",
      "Epoch [352/1000], Train Loss: 114.2579, Val Loss: 128.3243\n",
      "Epoch [353/1000], Train Loss: 107.1626, Val Loss: 121.8406\n",
      "Epoch [354/1000], Train Loss: 105.6947, Val Loss: 134.6595\n",
      "Epoch [355/1000], Train Loss: 104.1325, Val Loss: 161.5472\n",
      "Epoch [356/1000], Train Loss: 103.4687, Val Loss: 117.9106\n",
      "Epoch [357/1000], Train Loss: 105.4020, Val Loss: 141.4090\n",
      "Epoch [358/1000], Train Loss: 109.7209, Val Loss: 126.0730\n",
      "Epoch [359/1000], Train Loss: 103.0929, Val Loss: 119.6419\n",
      "Epoch [360/1000], Train Loss: 103.1689, Val Loss: 118.9642\n",
      "Epoch [361/1000], Train Loss: 106.8280, Val Loss: 126.2546\n",
      "Epoch [362/1000], Train Loss: 126.8436, Val Loss: 231.8503\n",
      "Epoch [363/1000], Train Loss: 164.0355, Val Loss: 152.6736\n",
      "Epoch [364/1000], Train Loss: 124.0776, Val Loss: 116.7606\n",
      "Epoch [365/1000], Train Loss: 100.5921, Val Loss: 201.6937\n",
      "Epoch [366/1000], Train Loss: 111.7783, Val Loss: 141.3032\n",
      "Epoch [367/1000], Train Loss: 179.5129, Val Loss: 169.0546\n",
      "Epoch [368/1000], Train Loss: 164.3931, Val Loss: 120.9441\n",
      "Epoch [369/1000], Train Loss: 117.2518, Val Loss: 128.1113\n",
      "Epoch [370/1000], Train Loss: 127.0678, Val Loss: 175.3083\n",
      "Epoch [371/1000], Train Loss: 244.9268, Val Loss: 146.8697\n",
      "Epoch [372/1000], Train Loss: 257.0741, Val Loss: 337.6845\n",
      "Epoch [373/1000], Train Loss: 202.0787, Val Loss: 150.5447\n",
      "Epoch [374/1000], Train Loss: 158.5898, Val Loss: 115.4673\n",
      "Epoch [375/1000], Train Loss: 254.1739, Val Loss: 252.1307\n",
      "Epoch [376/1000], Train Loss: 350.9715, Val Loss: 925.9131\n",
      "Epoch [377/1000], Train Loss: 245.7201, Val Loss: 259.8031\n",
      "Epoch [378/1000], Train Loss: 194.2401, Val Loss: 135.3939\n",
      "Epoch [379/1000], Train Loss: 135.9483, Val Loss: 370.4342\n",
      "Epoch [380/1000], Train Loss: 124.1095, Val Loss: 129.3246\n",
      "Epoch [381/1000], Train Loss: 253.7893, Val Loss: 227.6961\n",
      "Epoch [382/1000], Train Loss: 198.0528, Val Loss: 116.6629\n",
      "Epoch [383/1000], Train Loss: 215.0360, Val Loss: 289.4771\n",
      "Epoch [384/1000], Train Loss: 764.1292, Val Loss: 852.7144\n",
      "Epoch [385/1000], Train Loss: 2608.0331, Val Loss: 722.4774\n",
      "Epoch [386/1000], Train Loss: 256.4856, Val Loss: 115.1484\n",
      "Epoch [387/1000], Train Loss: 99.0964, Val Loss: 115.8244\n",
      "Epoch [388/1000], Train Loss: 96.1883, Val Loss: 112.0164\n",
      "Epoch [389/1000], Train Loss: 92.2923, Val Loss: 108.3296\n",
      "Epoch [390/1000], Train Loss: 87.8965, Val Loss: 107.8842\n",
      "Epoch [391/1000], Train Loss: 89.0110, Val Loss: 108.4601\n",
      "Epoch [392/1000], Train Loss: 93.3593, Val Loss: 101.6030\n",
      "Epoch [393/1000], Train Loss: 87.2381, Val Loss: 103.3876\n",
      "Epoch [394/1000], Train Loss: 100.1397, Val Loss: 110.1871\n",
      "Epoch [395/1000], Train Loss: 91.2882, Val Loss: 113.8732\n",
      "Epoch [396/1000], Train Loss: 117.4522, Val Loss: 191.5692\n",
      "Epoch [397/1000], Train Loss: 103.6191, Val Loss: 134.3724\n",
      "Epoch [398/1000], Train Loss: 117.3054, Val Loss: 333.7416\n",
      "Epoch [399/1000], Train Loss: 128.3909, Val Loss: 122.9889\n",
      "Epoch [400/1000], Train Loss: 118.4266, Val Loss: 125.0732\n",
      "Epoch [401/1000], Train Loss: 170.0226, Val Loss: 299.1051\n",
      "Epoch [402/1000], Train Loss: 169.9473, Val Loss: 100.8584\n",
      "Epoch [403/1000], Train Loss: 129.1699, Val Loss: 126.7038\n",
      "Epoch [404/1000], Train Loss: 178.8320, Val Loss: 129.3442\n",
      "Epoch [405/1000], Train Loss: 1475.8770, Val Loss: 138.9798\n",
      "Epoch [406/1000], Train Loss: 99.3745, Val Loss: 140.0030\n",
      "Epoch [407/1000], Train Loss: 84.2572, Val Loss: 152.1803\n",
      "Epoch [408/1000], Train Loss: 100.5245, Val Loss: 103.1041\n",
      "Epoch [409/1000], Train Loss: 85.2651, Val Loss: 111.3230\n",
      "Epoch [410/1000], Train Loss: 93.1498, Val Loss: 104.1746\n",
      "Epoch [411/1000], Train Loss: 97.2588, Val Loss: 103.1927\n",
      "Epoch [412/1000], Train Loss: 200.0602, Val Loss: 126.2383\n",
      "Epoch [413/1000], Train Loss: 103.8367, Val Loss: 219.4642\n",
      "Epoch [414/1000], Train Loss: 110.5422, Val Loss: 209.2267\n",
      "Epoch [415/1000], Train Loss: 120.3350, Val Loss: 139.4620\n",
      "Epoch [416/1000], Train Loss: 401.3641, Val Loss: 840.8865\n",
      "Epoch [417/1000], Train Loss: 280.4349, Val Loss: 104.0641\n",
      "Epoch [418/1000], Train Loss: 106.1057, Val Loss: 391.9521\n",
      "Epoch [419/1000], Train Loss: 217.5062, Val Loss: 298.2878\n",
      "Epoch [420/1000], Train Loss: 223.3484, Val Loss: 109.4546\n",
      "Epoch [421/1000], Train Loss: 243.4131, Val Loss: 115.0696\n",
      "Epoch [422/1000], Train Loss: 132.1635, Val Loss: 101.3084\n",
      "Epoch [423/1000], Train Loss: 130.1569, Val Loss: 109.3306\n",
      "Epoch [424/1000], Train Loss: 104.7687, Val Loss: 111.5814\n",
      "Epoch [425/1000], Train Loss: 742.2130, Val Loss: 265.1442\n",
      "Epoch [426/1000], Train Loss: 115.5456, Val Loss: 100.9450\n",
      "Epoch [427/1000], Train Loss: 86.7474, Val Loss: 108.8633\n",
      "Epoch [428/1000], Train Loss: 212.1925, Val Loss: 177.3187\n",
      "Epoch [429/1000], Train Loss: 237.7737, Val Loss: 184.5675\n",
      "Epoch [430/1000], Train Loss: 149.1705, Val Loss: 95.8668\n",
      "Epoch [431/1000], Train Loss: 100.1170, Val Loss: 98.0188\n",
      "Epoch [432/1000], Train Loss: 88.6561, Val Loss: 130.2201\n",
      "Epoch [433/1000], Train Loss: 103.8445, Val Loss: 185.2920\n",
      "Epoch [434/1000], Train Loss: 583.8734, Val Loss: 169.8511\n",
      "Epoch [435/1000], Train Loss: 190.3631, Val Loss: 95.3917\n",
      "Epoch [436/1000], Train Loss: 3083.6965, Val Loss: 1340.3827\n",
      "Epoch [437/1000], Train Loss: 212.1658, Val Loss: 109.1062\n",
      "Epoch [438/1000], Train Loss: 80.0286, Val Loss: 91.7529\n",
      "Epoch [439/1000], Train Loss: 76.4292, Val Loss: 96.6259\n",
      "Epoch [440/1000], Train Loss: 83.3412, Val Loss: 100.4858\n",
      "Epoch [441/1000], Train Loss: 74.3244, Val Loss: 89.5726\n",
      "Epoch [442/1000], Train Loss: 75.3373, Val Loss: 88.9482\n",
      "Epoch [443/1000], Train Loss: 74.9071, Val Loss: 112.5876\n",
      "Epoch [444/1000], Train Loss: 80.3007, Val Loss: 109.8006\n",
      "Epoch [445/1000], Train Loss: 79.7757, Val Loss: 169.4156\n",
      "Epoch [446/1000], Train Loss: 83.9814, Val Loss: 90.2938\n",
      "Epoch [447/1000], Train Loss: 77.2706, Val Loss: 101.1571\n",
      "Epoch [448/1000], Train Loss: 89.0028, Val Loss: 207.9729\n",
      "Epoch [449/1000], Train Loss: 86.5853, Val Loss: 96.7649\n",
      "Epoch [450/1000], Train Loss: 73.9576, Val Loss: 86.1325\n",
      "Epoch [451/1000], Train Loss: 80.9945, Val Loss: 97.8470\n",
      "Epoch [452/1000], Train Loss: 86.4269, Val Loss: 95.7893\n",
      "Epoch [453/1000], Train Loss: 97.5102, Val Loss: 484.8800\n",
      "Epoch [454/1000], Train Loss: 163.0640, Val Loss: 94.0955\n",
      "Epoch [455/1000], Train Loss: 170.6052, Val Loss: 128.6375\n",
      "Epoch [456/1000], Train Loss: 1240.6535, Val Loss: 108.6413\n",
      "Epoch [457/1000], Train Loss: 99.0745, Val Loss: 123.6245\n",
      "Epoch [458/1000], Train Loss: 78.3072, Val Loss: 102.9435\n",
      "Epoch [459/1000], Train Loss: 75.8723, Val Loss: 86.7241\n",
      "Epoch [460/1000], Train Loss: 71.8404, Val Loss: 155.2152\n",
      "Epoch [461/1000], Train Loss: 116.1489, Val Loss: 82.8356\n",
      "Epoch [462/1000], Train Loss: 922.6940, Val Loss: 6278.7671\n",
      "Epoch [463/1000], Train Loss: 3648.2838, Val Loss: 207.6801\n",
      "Epoch [464/1000], Train Loss: 88.9584, Val Loss: 89.8529\n",
      "Epoch [465/1000], Train Loss: 74.3300, Val Loss: 86.1577\n",
      "Epoch [466/1000], Train Loss: 68.1635, Val Loss: 87.1459\n",
      "Epoch [467/1000], Train Loss: 71.8239, Val Loss: 97.8633\n",
      "Epoch [468/1000], Train Loss: 69.6984, Val Loss: 84.3593\n",
      "Epoch [469/1000], Train Loss: 72.8977, Val Loss: 82.0358\n",
      "Epoch [470/1000], Train Loss: 67.7812, Val Loss: 91.8981\n",
      "Epoch [471/1000], Train Loss: 67.3458, Val Loss: 83.8414\n",
      "Epoch [472/1000], Train Loss: 67.7669, Val Loss: 90.7788\n",
      "Epoch [473/1000], Train Loss: 72.3797, Val Loss: 80.8569\n",
      "Epoch [474/1000], Train Loss: 76.1517, Val Loss: 88.7696\n",
      "Epoch [475/1000], Train Loss: 70.5726, Val Loss: 101.3230\n",
      "Epoch [476/1000], Train Loss: 70.3045, Val Loss: 79.1257\n",
      "Epoch [477/1000], Train Loss: 88.7462, Val Loss: 137.6407\n",
      "Epoch [478/1000], Train Loss: 94.9913, Val Loss: 80.7025\n",
      "Epoch [479/1000], Train Loss: 69.5497, Val Loss: 181.3773\n",
      "Epoch [480/1000], Train Loss: 168.0086, Val Loss: 200.0744\n",
      "Epoch [481/1000], Train Loss: 86.0139, Val Loss: 100.7287\n",
      "Epoch [482/1000], Train Loss: 81.1298, Val Loss: 103.0278\n",
      "Epoch [483/1000], Train Loss: 153.6593, Val Loss: 80.0394\n",
      "Epoch [484/1000], Train Loss: 88.9897, Val Loss: 96.8313\n",
      "Epoch [485/1000], Train Loss: 2871.0838, Val Loss: 4273.4777\n",
      "Epoch [486/1000], Train Loss: 505.4338, Val Loss: 86.9195\n",
      "Epoch [487/1000], Train Loss: 71.5761, Val Loss: 83.7159\n",
      "Epoch [488/1000], Train Loss: 64.4335, Val Loss: 78.3779\n",
      "Epoch [489/1000], Train Loss: 70.3219, Val Loss: 78.2090\n",
      "Epoch [490/1000], Train Loss: 62.1360, Val Loss: 94.2234\n",
      "Epoch [491/1000], Train Loss: 67.0045, Val Loss: 91.7148\n",
      "Epoch [492/1000], Train Loss: 69.6554, Val Loss: 75.4794\n",
      "Epoch [493/1000], Train Loss: 73.4288, Val Loss: 76.5649\n",
      "Epoch [494/1000], Train Loss: 83.5860, Val Loss: 76.6720\n",
      "Epoch [495/1000], Train Loss: 69.0853, Val Loss: 79.9163\n",
      "Epoch [496/1000], Train Loss: 65.1274, Val Loss: 108.2729\n",
      "Epoch [497/1000], Train Loss: 65.9446, Val Loss: 79.8753\n",
      "Epoch [498/1000], Train Loss: 68.7078, Val Loss: 80.5648\n",
      "Epoch [499/1000], Train Loss: 97.2385, Val Loss: 99.2250\n",
      "Epoch [500/1000], Train Loss: 100.0968, Val Loss: 97.8248\n",
      "Epoch [501/1000], Train Loss: 103.2909, Val Loss: 342.7988\n",
      "Epoch [502/1000], Train Loss: 100.1006, Val Loss: 100.9725\n",
      "Epoch [503/1000], Train Loss: 82.2246, Val Loss: 101.0172\n",
      "Epoch [504/1000], Train Loss: 484.4895, Val Loss: 645.8647\n",
      "Epoch [505/1000], Train Loss: 230.0035, Val Loss: 97.2154\n",
      "Epoch [506/1000], Train Loss: 69.0180, Val Loss: 74.8578\n",
      "Epoch [507/1000], Train Loss: 120.3498, Val Loss: 350.2401\n",
      "Epoch [508/1000], Train Loss: 399.3622, Val Loss: 85.3621\n",
      "Epoch [509/1000], Train Loss: 156.9702, Val Loss: 242.8270\n",
      "Epoch [510/1000], Train Loss: 89.1087, Val Loss: 79.4826\n",
      "Epoch [511/1000], Train Loss: 63.6984, Val Loss: 80.0968\n",
      "Epoch [512/1000], Train Loss: 194.1539, Val Loss: 133.5650\n",
      "Epoch [513/1000], Train Loss: 213.0015, Val Loss: 95.1838\n",
      "Epoch [514/1000], Train Loss: 79.3096, Val Loss: 76.1160\n",
      "Epoch [515/1000], Train Loss: 134.2975, Val Loss: 101.9188\n",
      "Epoch [516/1000], Train Loss: 171.3780, Val Loss: 242.1263\n",
      "Epoch [517/1000], Train Loss: 150.7046, Val Loss: 109.7188\n",
      "Epoch [518/1000], Train Loss: 1450.4177, Val Loss: 1125.6086\n",
      "Epoch [519/1000], Train Loss: 282.5782, Val Loss: 108.2473\n",
      "Epoch [520/1000], Train Loss: 75.7656, Val Loss: 92.9778\n",
      "Epoch [521/1000], Train Loss: 62.9886, Val Loss: 73.9828\n",
      "Epoch [522/1000], Train Loss: 73.3649, Val Loss: 100.4830\n",
      "Epoch [523/1000], Train Loss: 68.4448, Val Loss: 81.4148\n",
      "Epoch [524/1000], Train Loss: 94.4960, Val Loss: 561.7739\n",
      "Epoch [525/1000], Train Loss: 76.7354, Val Loss: 73.7218\n",
      "Epoch [526/1000], Train Loss: 75.6477, Val Loss: 77.5438\n",
      "Epoch [527/1000], Train Loss: 75.3710, Val Loss: 84.8321\n",
      "Epoch [528/1000], Train Loss: 94.3095, Val Loss: 555.2486\n",
      "Epoch [529/1000], Train Loss: 601.3062, Val Loss: 80.9276\n",
      "Epoch [530/1000], Train Loss: 86.8917, Val Loss: 156.3210\n",
      "Epoch [531/1000], Train Loss: 206.7591, Val Loss: 80.2310\n",
      "Epoch [532/1000], Train Loss: 70.8972, Val Loss: 77.9133\n",
      "Epoch [533/1000], Train Loss: 71.7975, Val Loss: 72.7187\n",
      "Epoch [534/1000], Train Loss: 312.5385, Val Loss: 135.0147\n",
      "Epoch [535/1000], Train Loss: 72.3974, Val Loss: 99.3864\n",
      "Epoch [536/1000], Train Loss: 191.0161, Val Loss: 156.9836\n",
      "Epoch [537/1000], Train Loss: 473.0688, Val Loss: 110.1277\n",
      "Epoch [538/1000], Train Loss: 67.6749, Val Loss: 79.2520\n",
      "Epoch [539/1000], Train Loss: 124.0536, Val Loss: 71.3244\n",
      "Epoch [540/1000], Train Loss: 103.6185, Val Loss: 237.0423\n",
      "Epoch [541/1000], Train Loss: 88.8158, Val Loss: 70.6661\n",
      "Epoch [542/1000], Train Loss: 142.9414, Val Loss: 506.9294\n",
      "Epoch [543/1000], Train Loss: 1726.4009, Val Loss: 190.3156\n",
      "Epoch [544/1000], Train Loss: 216.5975, Val Loss: 82.5606\n",
      "Epoch [545/1000], Train Loss: 65.9407, Val Loss: 69.8075\n",
      "Epoch [546/1000], Train Loss: 61.4697, Val Loss: 93.0618\n",
      "Epoch [547/1000], Train Loss: 89.9176, Val Loss: 85.8784\n",
      "Epoch [548/1000], Train Loss: 58.5133, Val Loss: 69.6266\n",
      "Epoch [549/1000], Train Loss: 70.3874, Val Loss: 67.3072\n",
      "Epoch [550/1000], Train Loss: 67.3843, Val Loss: 71.9959\n",
      "Epoch [551/1000], Train Loss: 70.0507, Val Loss: 84.3276\n",
      "Epoch [552/1000], Train Loss: 110.8037, Val Loss: 88.7809\n",
      "Epoch [553/1000], Train Loss: 92.6821, Val Loss: 136.8876\n",
      "Epoch [554/1000], Train Loss: 285.9136, Val Loss: 149.3112\n",
      "Epoch [555/1000], Train Loss: 81.9840, Val Loss: 191.3008\n",
      "Epoch [556/1000], Train Loss: 160.9414, Val Loss: 96.7963\n",
      "Epoch [557/1000], Train Loss: 66.5910, Val Loss: 78.5480\n",
      "Epoch [558/1000], Train Loss: 67.3768, Val Loss: 73.2517\n",
      "Epoch [559/1000], Train Loss: 59.3461, Val Loss: 68.7284\n",
      "Epoch [560/1000], Train Loss: 385.9792, Val Loss: 108.3441\n",
      "Epoch [561/1000], Train Loss: 106.4111, Val Loss: 74.3558\n",
      "Epoch [562/1000], Train Loss: 71.9101, Val Loss: 86.1027\n",
      "Epoch [563/1000], Train Loss: 191.4668, Val Loss: 68.3599\n",
      "Epoch [564/1000], Train Loss: 147.7776, Val Loss: 642.0286\n",
      "Epoch [565/1000], Train Loss: 128.1739, Val Loss: 131.2492\n",
      "Epoch [566/1000], Train Loss: 119.3478, Val Loss: 367.9448\n",
      "Epoch [567/1000], Train Loss: 145.9812, Val Loss: 197.0432\n",
      "Epoch [568/1000], Train Loss: 124.7208, Val Loss: 660.3950\n",
      "Epoch [569/1000], Train Loss: 269.8772, Val Loss: 80.0003\n",
      "Epoch [570/1000], Train Loss: 261.0547, Val Loss: 144.0213\n",
      "Epoch [571/1000], Train Loss: 123.7264, Val Loss: 71.5561\n",
      "Epoch [572/1000], Train Loss: 975.5329, Val Loss: 399.9729\n",
      "Epoch [573/1000], Train Loss: 92.7612, Val Loss: 98.7541\n",
      "Epoch [574/1000], Train Loss: 66.1225, Val Loss: 71.3369\n",
      "Epoch [575/1000], Train Loss: 55.0405, Val Loss: 77.4831\n",
      "Epoch [576/1000], Train Loss: 72.2896, Val Loss: 70.8618\n",
      "Epoch [577/1000], Train Loss: 79.2402, Val Loss: 84.8384\n",
      "Epoch [578/1000], Train Loss: 55.3780, Val Loss: 159.7280\n",
      "Epoch [579/1000], Train Loss: 133.6460, Val Loss: 123.4117\n",
      "Epoch [580/1000], Train Loss: 189.7770, Val Loss: 668.4701\n",
      "Epoch [581/1000], Train Loss: 118.9250, Val Loss: 156.8426\n",
      "Epoch [582/1000], Train Loss: 175.2692, Val Loss: 74.5015\n",
      "Epoch [583/1000], Train Loss: 115.7802, Val Loss: 111.3106\n",
      "Epoch [584/1000], Train Loss: 85.2471, Val Loss: 98.3518\n",
      "Epoch [585/1000], Train Loss: 941.1944, Val Loss: 108.5006\n",
      "Epoch [586/1000], Train Loss: 61.0507, Val Loss: 140.1320\n",
      "Epoch [587/1000], Train Loss: 60.2427, Val Loss: 81.5153\n",
      "Epoch [588/1000], Train Loss: 64.7208, Val Loss: 111.7531\n",
      "Epoch [589/1000], Train Loss: 77.5126, Val Loss: 95.9410\n",
      "Epoch [590/1000], Train Loss: 56.5023, Val Loss: 273.9478\n",
      "Epoch [591/1000], Train Loss: 165.3779, Val Loss: 72.4648\n",
      "Epoch [592/1000], Train Loss: 262.7365, Val Loss: 1564.5105\n",
      "Epoch [593/1000], Train Loss: 459.4292, Val Loss: 120.9395\n",
      "Epoch [594/1000], Train Loss: 57.9083, Val Loss: 63.2873\n",
      "Epoch [595/1000], Train Loss: 57.8075, Val Loss: 81.3647\n",
      "Epoch [596/1000], Train Loss: 53.0507, Val Loss: 75.0643\n",
      "Epoch [597/1000], Train Loss: 502.0711, Val Loss: 7614.0191\n",
      "Epoch [598/1000], Train Loss: 4278.0243, Val Loss: 154.5082\n",
      "Epoch [599/1000], Train Loss: 67.3731, Val Loss: 69.8879\n",
      "Epoch [600/1000], Train Loss: 52.6225, Val Loss: 72.2425\n",
      "Epoch [601/1000], Train Loss: 50.8959, Val Loss: 68.0609\n",
      "Epoch [602/1000], Train Loss: 49.0003, Val Loss: 60.9864\n",
      "Epoch [603/1000], Train Loss: 46.7674, Val Loss: 67.3910\n",
      "Epoch [604/1000], Train Loss: 52.7375, Val Loss: 61.9807\n",
      "Epoch [605/1000], Train Loss: 47.6691, Val Loss: 59.8521\n",
      "Epoch [606/1000], Train Loss: 47.4878, Val Loss: 71.7360\n",
      "Epoch [607/1000], Train Loss: 46.8525, Val Loss: 63.1527\n",
      "Epoch [608/1000], Train Loss: 48.6170, Val Loss: 63.1456\n",
      "Epoch [609/1000], Train Loss: 51.0093, Val Loss: 73.7829\n",
      "Epoch [610/1000], Train Loss: 48.9654, Val Loss: 59.5973\n",
      "Epoch [611/1000], Train Loss: 46.2654, Val Loss: 62.1398\n",
      "Epoch [612/1000], Train Loss: 52.2476, Val Loss: 69.3770\n",
      "Epoch [613/1000], Train Loss: 51.2190, Val Loss: 205.6467\n",
      "Epoch [614/1000], Train Loss: 75.8918, Val Loss: 60.0103\n",
      "Epoch [615/1000], Train Loss: 80.0054, Val Loss: 129.7007\n",
      "Epoch [616/1000], Train Loss: 52.7088, Val Loss: 62.4803\n",
      "Epoch [617/1000], Train Loss: 66.0811, Val Loss: 65.8204\n",
      "Epoch [618/1000], Train Loss: 53.6724, Val Loss: 73.9711\n",
      "Epoch [619/1000], Train Loss: 78.7432, Val Loss: 63.9444\n",
      "Epoch [620/1000], Train Loss: 60.3843, Val Loss: 83.9389\n",
      "Epoch [621/1000], Train Loss: 144.7265, Val Loss: 73.6028\n",
      "Epoch [622/1000], Train Loss: 1598.1855, Val Loss: 6336.2046\n",
      "Epoch [623/1000], Train Loss: 967.4384, Val Loss: 70.1433\n",
      "Epoch [624/1000], Train Loss: 54.9856, Val Loss: 61.9637\n",
      "Epoch [625/1000], Train Loss: 51.8446, Val Loss: 58.7990\n",
      "Epoch [626/1000], Train Loss: 50.4030, Val Loss: 59.7668\n",
      "Epoch [627/1000], Train Loss: 46.6263, Val Loss: 66.2046\n",
      "Epoch [628/1000], Train Loss: 48.2773, Val Loss: 56.8013\n",
      "Epoch [629/1000], Train Loss: 46.4004, Val Loss: 56.2578\n",
      "Epoch [630/1000], Train Loss: 44.1305, Val Loss: 61.0294\n",
      "Epoch [631/1000], Train Loss: 57.9220, Val Loss: 61.8158\n",
      "Epoch [632/1000], Train Loss: 49.5013, Val Loss: 104.7713\n",
      "Epoch [633/1000], Train Loss: 53.3176, Val Loss: 78.2785\n",
      "Epoch [634/1000], Train Loss: 206.3664, Val Loss: 294.8188\n",
      "Epoch [635/1000], Train Loss: 107.2112, Val Loss: 91.8669\n",
      "Epoch [636/1000], Train Loss: 59.4164, Val Loss: 78.1089\n",
      "Epoch [637/1000], Train Loss: 91.0174, Val Loss: 85.7604\n",
      "Epoch [638/1000], Train Loss: 76.2059, Val Loss: 78.5004\n",
      "Epoch [639/1000], Train Loss: 103.0089, Val Loss: 138.5285\n",
      "Epoch [640/1000], Train Loss: 344.4419, Val Loss: 62.9194\n",
      "Epoch [641/1000], Train Loss: 54.0290, Val Loss: 64.3605\n",
      "Epoch [642/1000], Train Loss: 186.4129, Val Loss: 139.4933\n",
      "Epoch [643/1000], Train Loss: 315.8512, Val Loss: 148.6307\n",
      "Epoch [644/1000], Train Loss: 86.4206, Val Loss: 101.7888\n",
      "Epoch [645/1000], Train Loss: 121.7191, Val Loss: 77.4368\n",
      "Epoch [646/1000], Train Loss: 240.2433, Val Loss: 172.7843\n",
      "Epoch [647/1000], Train Loss: 140.0392, Val Loss: 286.6896\n",
      "Epoch [648/1000], Train Loss: 72.5859, Val Loss: 101.7264\n",
      "Epoch [649/1000], Train Loss: 141.2389, Val Loss: 137.8784\n",
      "Epoch [650/1000], Train Loss: 84.6166, Val Loss: 57.3732\n",
      "Epoch [651/1000], Train Loss: 94.9888, Val Loss: 60.8809\n",
      "Epoch [652/1000], Train Loss: 5685.9941, Val Loss: 583.0882\n",
      "Epoch [653/1000], Train Loss: 267.1441, Val Loss: 76.5622\n",
      "Epoch [654/1000], Train Loss: 55.6821, Val Loss: 65.4837\n",
      "Epoch [655/1000], Train Loss: 48.0036, Val Loss: 63.9594\n",
      "Epoch [656/1000], Train Loss: 50.0756, Val Loss: 57.7544\n",
      "Epoch [657/1000], Train Loss: 43.8216, Val Loss: 60.5483\n",
      "Epoch [658/1000], Train Loss: 42.3896, Val Loss: 58.0614\n",
      "Epoch [659/1000], Train Loss: 40.4613, Val Loss: 58.1645\n",
      "Epoch [660/1000], Train Loss: 41.5410, Val Loss: 54.7784\n",
      "Epoch [661/1000], Train Loss: 41.0561, Val Loss: 54.3026\n",
      "Epoch [662/1000], Train Loss: 42.2639, Val Loss: 92.1440\n",
      "Epoch [663/1000], Train Loss: 46.3887, Val Loss: 54.0895\n",
      "Epoch [664/1000], Train Loss: 39.3678, Val Loss: 61.4572\n",
      "Epoch [665/1000], Train Loss: 41.9375, Val Loss: 54.8165\n",
      "Epoch [666/1000], Train Loss: 45.6735, Val Loss: 53.6681\n",
      "Epoch [667/1000], Train Loss: 40.9098, Val Loss: 56.3462\n",
      "Epoch [668/1000], Train Loss: 49.5031, Val Loss: 55.2281\n",
      "Epoch [669/1000], Train Loss: 48.1184, Val Loss: 88.3017\n",
      "Epoch [670/1000], Train Loss: 49.3256, Val Loss: 84.0197\n",
      "Epoch [671/1000], Train Loss: 58.0417, Val Loss: 58.2502\n",
      "Epoch [672/1000], Train Loss: 65.3935, Val Loss: 53.7727\n",
      "Epoch [673/1000], Train Loss: 73.1492, Val Loss: 219.7077\n",
      "Epoch [674/1000], Train Loss: 160.7264, Val Loss: 237.2509\n",
      "Epoch [675/1000], Train Loss: 205.3946, Val Loss: 79.0065\n",
      "Epoch [676/1000], Train Loss: 47.9032, Val Loss: 76.4084\n",
      "Epoch [677/1000], Train Loss: 53.4804, Val Loss: 94.9164\n",
      "Epoch [678/1000], Train Loss: 74.6018, Val Loss: 155.3513\n",
      "Epoch [679/1000], Train Loss: 279.0089, Val Loss: 1183.4679\n",
      "Epoch [680/1000], Train Loss: 190.0222, Val Loss: 433.9097\n",
      "Epoch [681/1000], Train Loss: 244.5576, Val Loss: 622.0218\n",
      "Epoch [682/1000], Train Loss: 300.5029, Val Loss: 94.0938\n",
      "Epoch [683/1000], Train Loss: 52.6336, Val Loss: 55.7186\n",
      "Epoch [684/1000], Train Loss: 49.5072, Val Loss: 391.5295\n",
      "Epoch [685/1000], Train Loss: 137.5092, Val Loss: 81.7602\n",
      "Epoch [686/1000], Train Loss: 671.1014, Val Loss: 362.4782\n",
      "Epoch [687/1000], Train Loss: 129.8283, Val Loss: 53.6760\n",
      "Epoch [688/1000], Train Loss: 45.8607, Val Loss: 58.3363\n",
      "Epoch [689/1000], Train Loss: 51.2008, Val Loss: 69.9551\n",
      "Epoch [690/1000], Train Loss: 97.7608, Val Loss: 58.7188\n",
      "Epoch [691/1000], Train Loss: 69.1986, Val Loss: 56.0372\n",
      "Epoch [692/1000], Train Loss: 56.2893, Val Loss: 56.8245\n",
      "Epoch [693/1000], Train Loss: 76.9609, Val Loss: 53.4604\n",
      "Epoch [694/1000], Train Loss: 103.2330, Val Loss: 100.0662\n",
      "Epoch [695/1000], Train Loss: 2454.7173, Val Loss: 951.4602\n",
      "Epoch [696/1000], Train Loss: 162.3031, Val Loss: 58.5040\n",
      "Epoch [697/1000], Train Loss: 46.4760, Val Loss: 57.1834\n",
      "Epoch [698/1000], Train Loss: 42.8323, Val Loss: 61.4555\n",
      "Epoch [699/1000], Train Loss: 39.0345, Val Loss: 50.7864\n",
      "Epoch [700/1000], Train Loss: 37.9570, Val Loss: 52.0613\n",
      "Epoch [701/1000], Train Loss: 38.0394, Val Loss: 63.4005\n",
      "Epoch [702/1000], Train Loss: 54.0898, Val Loss: 50.5862\n",
      "Epoch [703/1000], Train Loss: 50.9766, Val Loss: 48.8065\n",
      "Epoch [704/1000], Train Loss: 47.0802, Val Loss: 160.5254\n",
      "Epoch [705/1000], Train Loss: 47.5569, Val Loss: 51.4782\n",
      "Epoch [706/1000], Train Loss: 40.1475, Val Loss: 60.9434\n",
      "Epoch [707/1000], Train Loss: 65.5312, Val Loss: 51.0648\n",
      "Epoch [708/1000], Train Loss: 70.0149, Val Loss: 93.8948\n",
      "Epoch [709/1000], Train Loss: 79.6090, Val Loss: 156.0841\n",
      "Epoch [710/1000], Train Loss: 3522.4114, Val Loss: 323.7227\n",
      "Epoch [711/1000], Train Loss: 127.9563, Val Loss: 64.7064\n",
      "Epoch [712/1000], Train Loss: 45.5311, Val Loss: 58.0108\n",
      "Epoch [713/1000], Train Loss: 40.1231, Val Loss: 52.7291\n",
      "Epoch [714/1000], Train Loss: 39.8096, Val Loss: 74.6723\n",
      "Epoch [715/1000], Train Loss: 41.0867, Val Loss: 50.8913\n",
      "Epoch [716/1000], Train Loss: 37.3531, Val Loss: 84.0448\n",
      "Epoch [717/1000], Train Loss: 41.6925, Val Loss: 51.3300\n",
      "Epoch [718/1000], Train Loss: 39.8166, Val Loss: 48.8186\n",
      "Epoch [719/1000], Train Loss: 38.6351, Val Loss: 51.2082\n",
      "Epoch [720/1000], Train Loss: 46.1702, Val Loss: 51.3815\n",
      "Epoch [721/1000], Train Loss: 36.8645, Val Loss: 49.7601\n",
      "Epoch [722/1000], Train Loss: 38.2932, Val Loss: 50.7643\n",
      "Epoch [723/1000], Train Loss: 37.3571, Val Loss: 56.4470\n",
      "Epoch [724/1000], Train Loss: 47.9685, Val Loss: 48.1998\n",
      "Epoch [725/1000], Train Loss: 39.3732, Val Loss: 58.4069\n",
      "Epoch [726/1000], Train Loss: 51.8050, Val Loss: 65.7883\n",
      "Epoch [727/1000], Train Loss: 51.2586, Val Loss: 111.7896\n",
      "Epoch [728/1000], Train Loss: 65.2441, Val Loss: 65.2501\n",
      "Epoch [729/1000], Train Loss: 306.0265, Val Loss: 102.1737\n",
      "Epoch [730/1000], Train Loss: 75.7488, Val Loss: 61.2796\n",
      "Epoch [731/1000], Train Loss: 84.6720, Val Loss: 58.6197\n",
      "Epoch [732/1000], Train Loss: 486.9784, Val Loss: 396.1754\n",
      "Epoch [733/1000], Train Loss: 111.1024, Val Loss: 54.6839\n",
      "Epoch [734/1000], Train Loss: 44.1358, Val Loss: 71.3412\n",
      "Epoch [735/1000], Train Loss: 46.9250, Val Loss: 61.6344\n",
      "Epoch [736/1000], Train Loss: 52.5703, Val Loss: 132.4292\n",
      "Epoch [737/1000], Train Loss: 124.8619, Val Loss: 224.8763\n",
      "Epoch [738/1000], Train Loss: 75.4712, Val Loss: 56.6512\n",
      "Epoch [739/1000], Train Loss: 864.9816, Val Loss: 15168.7965\n",
      "Epoch [740/1000], Train Loss: 1929.0623, Val Loss: 76.5656\n",
      "Epoch [741/1000], Train Loss: 54.7717, Val Loss: 51.9058\n",
      "Epoch [742/1000], Train Loss: 37.0458, Val Loss: 49.4369\n",
      "Epoch [743/1000], Train Loss: 40.1808, Val Loss: 48.1826\n",
      "Epoch [744/1000], Train Loss: 34.8599, Val Loss: 47.7399\n",
      "Epoch [745/1000], Train Loss: 37.8763, Val Loss: 55.4778\n",
      "Epoch [746/1000], Train Loss: 35.5827, Val Loss: 48.9702\n",
      "Epoch [747/1000], Train Loss: 39.4668, Val Loss: 51.8117\n",
      "Epoch [748/1000], Train Loss: 36.8859, Val Loss: 57.5977\n",
      "Epoch [749/1000], Train Loss: 43.9066, Val Loss: 53.6073\n",
      "Epoch [750/1000], Train Loss: 62.3436, Val Loss: 59.9420\n",
      "Epoch [751/1000], Train Loss: 73.7727, Val Loss: 56.0639\n",
      "Epoch [752/1000], Train Loss: 44.1937, Val Loss: 82.1578\n",
      "Epoch [753/1000], Train Loss: 61.9960, Val Loss: 49.4991\n",
      "Epoch [754/1000], Train Loss: 59.7124, Val Loss: 52.9292\n",
      "Epoch [755/1000], Train Loss: 54.0862, Val Loss: 46.9400\n",
      "Epoch [756/1000], Train Loss: 50.0554, Val Loss: 109.7912\n",
      "Epoch [757/1000], Train Loss: 72.3628, Val Loss: 67.4442\n",
      "Epoch [758/1000], Train Loss: 3236.0118, Val Loss: 7809.3064\n",
      "Epoch [759/1000], Train Loss: 770.6292, Val Loss: 77.7851\n",
      "Epoch [760/1000], Train Loss: 45.5416, Val Loss: 51.3538\n",
      "Epoch [761/1000], Train Loss: 40.0209, Val Loss: 51.5611\n",
      "Epoch [762/1000], Train Loss: 36.8666, Val Loss: 59.9435\n",
      "Epoch [763/1000], Train Loss: 43.5756, Val Loss: 53.4639\n",
      "Epoch [764/1000], Train Loss: 36.9777, Val Loss: 51.6771\n",
      "Epoch [765/1000], Train Loss: 37.6335, Val Loss: 52.6882\n",
      "Epoch [766/1000], Train Loss: 36.8587, Val Loss: 48.7833\n",
      "Epoch [767/1000], Train Loss: 34.4631, Val Loss: 47.8202\n",
      "Epoch [768/1000], Train Loss: 33.7315, Val Loss: 46.2048\n",
      "Epoch [769/1000], Train Loss: 40.4874, Val Loss: 45.7236\n",
      "Epoch [770/1000], Train Loss: 34.0733, Val Loss: 52.5572\n",
      "Epoch [771/1000], Train Loss: 42.8950, Val Loss: 47.1141\n",
      "Epoch [772/1000], Train Loss: 39.7431, Val Loss: 52.9111\n",
      "Epoch [773/1000], Train Loss: 37.2758, Val Loss: 57.6011\n",
      "Epoch [774/1000], Train Loss: 54.0723, Val Loss: 118.5237\n",
      "Epoch [775/1000], Train Loss: 44.0292, Val Loss: 48.6213\n",
      "Epoch [776/1000], Train Loss: 51.2354, Val Loss: 135.3049\n",
      "Epoch [777/1000], Train Loss: 143.0303, Val Loss: 97.5228\n",
      "Epoch [778/1000], Train Loss: 139.5093, Val Loss: 48.8252\n",
      "Epoch [779/1000], Train Loss: 120.9198, Val Loss: 79.1277\n",
      "Epoch [780/1000], Train Loss: 74.0174, Val Loss: 47.1590\n",
      "Epoch [781/1000], Train Loss: 127.5153, Val Loss: 637.4771\n",
      "Epoch [782/1000], Train Loss: 419.1228, Val Loss: 69.6011\n",
      "Epoch [783/1000], Train Loss: 44.9240, Val Loss: 60.0299\n",
      "Epoch [784/1000], Train Loss: 37.6732, Val Loss: 92.3890\n",
      "Epoch [785/1000], Train Loss: 80.7631, Val Loss: 90.7370\n",
      "Epoch [786/1000], Train Loss: 988.3331, Val Loss: 2021.3036\n",
      "Epoch [787/1000], Train Loss: 2622.7175, Val Loss: 363.0809\n",
      "Epoch [788/1000], Train Loss: 76.7404, Val Loss: 50.3732\n",
      "Epoch [789/1000], Train Loss: 37.9401, Val Loss: 58.3712\n",
      "Epoch [790/1000], Train Loss: 37.4022, Val Loss: 45.2398\n",
      "Epoch [791/1000], Train Loss: 33.2570, Val Loss: 54.6210\n",
      "Epoch [792/1000], Train Loss: 34.9370, Val Loss: 47.5178\n",
      "Epoch [793/1000], Train Loss: 31.8436, Val Loss: 42.8683\n",
      "Epoch [794/1000], Train Loss: 33.1192, Val Loss: 49.8588\n",
      "Epoch [795/1000], Train Loss: 33.7864, Val Loss: 59.3152\n",
      "Epoch [796/1000], Train Loss: 35.6714, Val Loss: 44.3543\n",
      "Epoch [797/1000], Train Loss: 33.0953, Val Loss: 44.4167\n",
      "Epoch [798/1000], Train Loss: 37.8137, Val Loss: 45.3470\n",
      "Epoch [799/1000], Train Loss: 36.7544, Val Loss: 66.4942\n",
      "Epoch [800/1000], Train Loss: 48.7054, Val Loss: 59.0871\n",
      "Epoch [801/1000], Train Loss: 64.4521, Val Loss: 174.0354\n",
      "Epoch [802/1000], Train Loss: 957.6135, Val Loss: 2724.5183\n",
      "Epoch [803/1000], Train Loss: 692.1469, Val Loss: 47.1883\n",
      "Epoch [804/1000], Train Loss: 36.3267, Val Loss: 47.6580\n",
      "Epoch [805/1000], Train Loss: 32.8231, Val Loss: 43.8811\n",
      "Epoch [806/1000], Train Loss: 34.6314, Val Loss: 45.6266\n",
      "Epoch [807/1000], Train Loss: 34.2362, Val Loss: 42.5107\n",
      "Epoch [808/1000], Train Loss: 37.2288, Val Loss: 42.0328\n",
      "Epoch [809/1000], Train Loss: 32.6297, Val Loss: 44.1881\n",
      "Epoch [810/1000], Train Loss: 35.1818, Val Loss: 77.8806\n",
      "Epoch [811/1000], Train Loss: 36.0253, Val Loss: 99.4605\n",
      "Epoch [812/1000], Train Loss: 61.4704, Val Loss: 64.3592\n",
      "Epoch [813/1000], Train Loss: 36.6052, Val Loss: 47.1910\n",
      "Epoch [814/1000], Train Loss: 105.1432, Val Loss: 752.5231\n",
      "Epoch [815/1000], Train Loss: 276.1897, Val Loss: 247.8566\n",
      "Epoch [816/1000], Train Loss: 55.9859, Val Loss: 42.6740\n",
      "Epoch [817/1000], Train Loss: 36.9303, Val Loss: 70.6008\n",
      "Epoch [818/1000], Train Loss: 109.6447, Val Loss: 42.3989\n",
      "Epoch [819/1000], Train Loss: 141.9860, Val Loss: 235.2165\n",
      "Epoch [820/1000], Train Loss: 452.7136, Val Loss: 502.6056\n",
      "Epoch [821/1000], Train Loss: 158.7870, Val Loss: 89.3217\n",
      "Epoch [822/1000], Train Loss: 73.8341, Val Loss: 69.4717\n",
      "Epoch [823/1000], Train Loss: 63.2276, Val Loss: 58.2263\n",
      "Epoch [824/1000], Train Loss: 67.5984, Val Loss: 43.0276\n",
      "Epoch [825/1000], Train Loss: 50.1300, Val Loss: 48.8087\n",
      "Epoch [826/1000], Train Loss: 76.3137, Val Loss: 59.2988\n",
      "Epoch [827/1000], Train Loss: 165.7113, Val Loss: 68.9324\n",
      "Epoch [828/1000], Train Loss: 139.3358, Val Loss: 205.7779\n",
      "Epoch [829/1000], Train Loss: 68.0540, Val Loss: 43.6172\n",
      "Epoch [830/1000], Train Loss: 1007.5360, Val Loss: 1789.7402\n",
      "Epoch [831/1000], Train Loss: 312.0376, Val Loss: 57.7336\n",
      "Epoch [832/1000], Train Loss: 47.2989, Val Loss: 52.6865\n",
      "Epoch [833/1000], Train Loss: 31.9167, Val Loss: 42.6464\n",
      "Epoch [834/1000], Train Loss: 31.3135, Val Loss: 49.2655\n",
      "Epoch [835/1000], Train Loss: 64.3922, Val Loss: 47.3287\n",
      "Epoch [836/1000], Train Loss: 39.2240, Val Loss: 50.2160\n",
      "Epoch [837/1000], Train Loss: 60.2637, Val Loss: 85.3542\n",
      "Epoch [838/1000], Train Loss: 163.8817, Val Loss: 50.6741\n",
      "Epoch [839/1000], Train Loss: 47.4955, Val Loss: 41.8547\n",
      "Epoch [840/1000], Train Loss: 41.9009, Val Loss: 63.9309\n",
      "Epoch [841/1000], Train Loss: 70.9758, Val Loss: 59.2052\n",
      "Epoch [842/1000], Train Loss: 103.6830, Val Loss: 160.0991\n",
      "Epoch [843/1000], Train Loss: 1196.5428, Val Loss: 113.5310\n",
      "Epoch [844/1000], Train Loss: 43.1049, Val Loss: 42.2730\n",
      "Epoch [845/1000], Train Loss: 34.2267, Val Loss: 41.1711\n",
      "Epoch [846/1000], Train Loss: 31.9269, Val Loss: 41.0509\n",
      "Epoch [847/1000], Train Loss: 34.0291, Val Loss: 64.0298\n",
      "Epoch [848/1000], Train Loss: 38.5351, Val Loss: 83.9567\n",
      "Epoch [849/1000], Train Loss: 41.3872, Val Loss: 59.7570\n",
      "Epoch [850/1000], Train Loss: 42.9941, Val Loss: 40.4140\n",
      "Epoch [851/1000], Train Loss: 198.4916, Val Loss: 75.7044\n",
      "Epoch [852/1000], Train Loss: 662.2231, Val Loss: 53.3660\n",
      "Epoch [853/1000], Train Loss: 43.4452, Val Loss: 52.4902\n",
      "Epoch [854/1000], Train Loss: 43.8533, Val Loss: 43.1658\n",
      "Epoch [855/1000], Train Loss: 40.6166, Val Loss: 102.1068\n",
      "Epoch [856/1000], Train Loss: 77.1011, Val Loss: 166.6513\n",
      "Epoch [857/1000], Train Loss: 63.6118, Val Loss: 46.6928\n",
      "Epoch [858/1000], Train Loss: 56.1855, Val Loss: 160.8696\n",
      "Epoch [859/1000], Train Loss: 99.1947, Val Loss: 72.9133\n",
      "Epoch [860/1000], Train Loss: 58.7555, Val Loss: 83.2531\n",
      "Epoch [861/1000], Train Loss: 2307.7522, Val Loss: 374.1125\n",
      "Epoch [862/1000], Train Loss: 320.6522, Val Loss: 49.9655\n",
      "Epoch [863/1000], Train Loss: 34.2774, Val Loss: 41.8600\n",
      "Epoch [864/1000], Train Loss: 34.4591, Val Loss: 44.7924\n",
      "Epoch [865/1000], Train Loss: 35.3885, Val Loss: 48.8677\n",
      "Epoch [866/1000], Train Loss: 34.5314, Val Loss: 41.0644\n",
      "Epoch [867/1000], Train Loss: 29.0516, Val Loss: 43.3736\n",
      "Epoch [868/1000], Train Loss: 32.1546, Val Loss: 52.8966\n",
      "Epoch [869/1000], Train Loss: 34.5165, Val Loss: 42.9964\n",
      "Epoch [870/1000], Train Loss: 30.5646, Val Loss: 48.3985\n",
      "Epoch [871/1000], Train Loss: 32.9492, Val Loss: 53.9043\n",
      "Epoch [872/1000], Train Loss: 33.4806, Val Loss: 50.2101\n",
      "Epoch [873/1000], Train Loss: 35.1823, Val Loss: 65.8611\n",
      "Epoch [874/1000], Train Loss: 51.4510, Val Loss: 40.9859\n",
      "Epoch [875/1000], Train Loss: 47.1069, Val Loss: 45.8820\n",
      "Epoch [876/1000], Train Loss: 41.4743, Val Loss: 68.0685\n",
      "Epoch [877/1000], Train Loss: 40.0345, Val Loss: 41.4770\n",
      "Epoch [878/1000], Train Loss: 71.7349, Val Loss: 167.7753\n",
      "Epoch [879/1000], Train Loss: 610.5086, Val Loss: 94.6698\n",
      "Epoch [880/1000], Train Loss: 76.9724, Val Loss: 89.8187\n",
      "Epoch [881/1000], Train Loss: 36.0379, Val Loss: 51.0719\n",
      "Epoch [882/1000], Train Loss: 41.0728, Val Loss: 52.0309\n",
      "Epoch [883/1000], Train Loss: 52.9960, Val Loss: 98.8793\n",
      "Epoch [884/1000], Train Loss: 119.1702, Val Loss: 819.1817\n",
      "Epoch [885/1000], Train Loss: 5023.5636, Val Loss: 111.7393\n",
      "Epoch [886/1000], Train Loss: 65.8932, Val Loss: 48.4032\n",
      "Epoch [887/1000], Train Loss: 35.0679, Val Loss: 45.6755\n",
      "Epoch [888/1000], Train Loss: 33.4146, Val Loss: 71.0117\n",
      "Epoch [889/1000], Train Loss: 32.3465, Val Loss: 53.4903\n",
      "Epoch [890/1000], Train Loss: 29.9461, Val Loss: 50.8838\n",
      "Epoch [891/1000], Train Loss: 31.2673, Val Loss: 46.9421\n",
      "Epoch [892/1000], Train Loss: 31.3400, Val Loss: 38.3441\n",
      "Epoch [893/1000], Train Loss: 29.1576, Val Loss: 41.5272\n",
      "Epoch [894/1000], Train Loss: 27.8948, Val Loss: 38.5289\n",
      "Epoch [895/1000], Train Loss: 28.2863, Val Loss: 38.9643\n",
      "Epoch [896/1000], Train Loss: 27.1516, Val Loss: 39.0910\n",
      "Epoch [897/1000], Train Loss: 34.2393, Val Loss: 47.9349\n",
      "Epoch [898/1000], Train Loss: 30.7422, Val Loss: 37.9030\n",
      "Epoch [899/1000], Train Loss: 31.1606, Val Loss: 57.7856\n",
      "Epoch [900/1000], Train Loss: 38.1416, Val Loss: 39.3929\n",
      "Epoch [901/1000], Train Loss: 33.8002, Val Loss: 43.5229\n",
      "Epoch [902/1000], Train Loss: 30.0080, Val Loss: 42.7703\n",
      "Epoch [903/1000], Train Loss: 38.2330, Val Loss: 39.7406\n",
      "Epoch [904/1000], Train Loss: 37.4010, Val Loss: 44.6388\n",
      "Epoch [905/1000], Train Loss: 62.5041, Val Loss: 119.7618\n",
      "Epoch [906/1000], Train Loss: 80.2412, Val Loss: 177.7025\n",
      "Epoch [907/1000], Train Loss: 51.2536, Val Loss: 82.9025\n",
      "Epoch [908/1000], Train Loss: 86.8717, Val Loss: 591.9161\n",
      "Epoch [909/1000], Train Loss: 201.8953, Val Loss: 52.7041\n",
      "Epoch [910/1000], Train Loss: 47.6087, Val Loss: 47.7472\n",
      "Epoch [911/1000], Train Loss: 90.3203, Val Loss: 58.7694\n",
      "Epoch [912/1000], Train Loss: 41.8675, Val Loss: 39.1210\n",
      "Epoch [913/1000], Train Loss: 534.5282, Val Loss: 976.8869\n",
      "Epoch [914/1000], Train Loss: 360.8372, Val Loss: 56.1652\n",
      "Epoch [915/1000], Train Loss: 42.6649, Val Loss: 62.5434\n",
      "Epoch [916/1000], Train Loss: 38.4065, Val Loss: 39.6922\n",
      "Epoch [917/1000], Train Loss: 29.1931, Val Loss: 36.9850\n",
      "Epoch [918/1000], Train Loss: 41.2302, Val Loss: 66.1483\n",
      "Epoch [919/1000], Train Loss: 79.3586, Val Loss: 64.7135\n",
      "Epoch [920/1000], Train Loss: 179.3521, Val Loss: 824.4785\n",
      "Epoch [921/1000], Train Loss: 198.3012, Val Loss: 1302.8052\n",
      "Epoch [922/1000], Train Loss: 3019.1114, Val Loss: 88.8786\n",
      "Epoch [923/1000], Train Loss: 45.3859, Val Loss: 44.5103\n",
      "Epoch [924/1000], Train Loss: 29.7454, Val Loss: 47.1647\n",
      "Epoch [925/1000], Train Loss: 32.8187, Val Loss: 40.9787\n",
      "Epoch [926/1000], Train Loss: 29.0296, Val Loss: 70.9798\n",
      "Epoch [927/1000], Train Loss: 28.8274, Val Loss: 37.7910\n",
      "Epoch [928/1000], Train Loss: 28.9156, Val Loss: 40.4872\n",
      "Epoch [929/1000], Train Loss: 29.2545, Val Loss: 38.5855\n",
      "Epoch [930/1000], Train Loss: 32.4793, Val Loss: 38.7073\n",
      "Epoch [931/1000], Train Loss: 30.2778, Val Loss: 38.9490\n",
      "Epoch [932/1000], Train Loss: 30.1410, Val Loss: 38.4539\n",
      "Epoch [933/1000], Train Loss: 35.6111, Val Loss: 39.6829\n",
      "Epoch [934/1000], Train Loss: 29.6490, Val Loss: 40.5716\n",
      "Epoch [935/1000], Train Loss: 46.0176, Val Loss: 39.7588\n",
      "Epoch [936/1000], Train Loss: 31.7910, Val Loss: 42.3956\n",
      "Epoch [937/1000], Train Loss: 37.9830, Val Loss: 78.8870\n",
      "Epoch [938/1000], Train Loss: 73.2818, Val Loss: 134.6566\n",
      "Epoch [939/1000], Train Loss: 56.7311, Val Loss: 60.4618\n",
      "Epoch [940/1000], Train Loss: 118.7086, Val Loss: 70.5953\n",
      "Epoch [941/1000], Train Loss: 50.5290, Val Loss: 36.1561\n",
      "Epoch [942/1000], Train Loss: 42.6735, Val Loss: 129.9720\n",
      "Epoch [943/1000], Train Loss: 103.0643, Val Loss: 59.6977\n",
      "Epoch [944/1000], Train Loss: 58.8260, Val Loss: 42.8657\n",
      "Epoch [945/1000], Train Loss: 116.8442, Val Loss: 134.0893\n",
      "Epoch [946/1000], Train Loss: 158.9661, Val Loss: 75.7218\n",
      "Epoch [947/1000], Train Loss: 767.0234, Val Loss: 342.8972\n",
      "Epoch [948/1000], Train Loss: 116.3410, Val Loss: 39.5441\n",
      "Epoch [949/1000], Train Loss: 37.4342, Val Loss: 37.8649\n",
      "Epoch [950/1000], Train Loss: 32.0301, Val Loss: 36.2098\n",
      "Epoch [951/1000], Train Loss: 68.6274, Val Loss: 174.4232\n",
      "Epoch [952/1000], Train Loss: 82.2191, Val Loss: 104.0212\n",
      "Epoch [953/1000], Train Loss: 76.4946, Val Loss: 65.6765\n",
      "Epoch [954/1000], Train Loss: 41.0210, Val Loss: 80.7981\n",
      "Epoch [955/1000], Train Loss: 79.3800, Val Loss: 156.1671\n",
      "Epoch [956/1000], Train Loss: 425.2758, Val Loss: 655.7391\n",
      "Epoch [957/1000], Train Loss: 219.7949, Val Loss: 42.2099\n",
      "Epoch [958/1000], Train Loss: 34.4022, Val Loss: 45.2622\n",
      "Epoch [959/1000], Train Loss: 30.7608, Val Loss: 51.9686\n",
      "Epoch [960/1000], Train Loss: 33.5193, Val Loss: 47.4740\n",
      "Epoch [961/1000], Train Loss: 54.9138, Val Loss: 42.7427\n",
      "Epoch [962/1000], Train Loss: 200.2021, Val Loss: 51.8153\n",
      "Epoch [963/1000], Train Loss: 78.6332, Val Loss: 82.4262\n",
      "Epoch [964/1000], Train Loss: 83.2247, Val Loss: 78.0048\n",
      "Epoch [965/1000], Train Loss: 147.1490, Val Loss: 41.7720\n",
      "Epoch [966/1000], Train Loss: 137.4400, Val Loss: 70.1203\n",
      "Epoch [967/1000], Train Loss: 189.0730, Val Loss: 1241.8835\n",
      "Epoch [968/1000], Train Loss: 1099.3161, Val Loss: 101.5408\n",
      "Epoch [969/1000], Train Loss: 59.1752, Val Loss: 36.2682\n",
      "Epoch [970/1000], Train Loss: 37.6173, Val Loss: 43.4334\n",
      "Epoch [971/1000], Train Loss: 33.1138, Val Loss: 37.4529\n",
      "Epoch [972/1000], Train Loss: 25.1380, Val Loss: 39.6650\n",
      "Epoch [973/1000], Train Loss: 41.4080, Val Loss: 183.0147\n",
      "Epoch [974/1000], Train Loss: 60.2481, Val Loss: 61.5718\n",
      "Epoch [975/1000], Train Loss: 100.5775, Val Loss: 49.0748\n",
      "Epoch [976/1000], Train Loss: 75.4971, Val Loss: 62.3073\n",
      "Epoch [977/1000], Train Loss: 31.8619, Val Loss: 35.4884\n",
      "Epoch [978/1000], Train Loss: 53.4184, Val Loss: 567.3880\n",
      "Epoch [979/1000], Train Loss: 1076.1399, Val Loss: 372.2075\n",
      "Epoch [980/1000], Train Loss: 2331.8528, Val Loss: 58.5386\n",
      "Epoch [981/1000], Train Loss: 42.5258, Val Loss: 46.9194\n",
      "Epoch [982/1000], Train Loss: 28.4372, Val Loss: 39.0719\n",
      "Epoch [983/1000], Train Loss: 26.2284, Val Loss: 35.4984\n",
      "Epoch [984/1000], Train Loss: 27.7772, Val Loss: 38.9554\n",
      "Epoch [985/1000], Train Loss: 25.6844, Val Loss: 42.3119\n",
      "Epoch [986/1000], Train Loss: 23.8784, Val Loss: 38.8913\n",
      "Epoch [987/1000], Train Loss: 26.7760, Val Loss: 63.1430\n",
      "Epoch [988/1000], Train Loss: 32.1097, Val Loss: 35.9009\n",
      "Epoch [989/1000], Train Loss: 28.8898, Val Loss: 67.0646\n",
      "Epoch [990/1000], Train Loss: 34.3844, Val Loss: 44.0310\n",
      "Epoch [991/1000], Train Loss: 30.7700, Val Loss: 42.9184\n",
      "Epoch [992/1000], Train Loss: 41.3207, Val Loss: 51.4301\n",
      "Epoch [993/1000], Train Loss: 30.0117, Val Loss: 36.0375\n",
      "Epoch [994/1000], Train Loss: 29.8139, Val Loss: 34.9989\n",
      "Epoch [995/1000], Train Loss: 27.8693, Val Loss: 58.3546\n",
      "Epoch [996/1000], Train Loss: 52.0221, Val Loss: 42.7910\n",
      "Epoch [997/1000], Train Loss: 28.1575, Val Loss: 34.5837\n",
      "Epoch [998/1000], Train Loss: 62.9147, Val Loss: 36.8910\n",
      "Epoch [999/1000], Train Loss: 28.4107, Val Loss: 37.0316\n",
      "Epoch [1000/1000], Train Loss: 586.2846, Val Loss: 46.1979\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(586.2846372951932, 46.19793382432726)"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp.trainer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn=HelperClass(Xtr,Ytr,model,1000,1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1000], Train Loss: 64.1733, Val Loss: 11.1056\n",
      "Epoch [2/1000], Train Loss: 9.4316, Val Loss: 9.7650\n",
      "Epoch [3/1000], Train Loss: 9.2412, Val Loss: 9.8794\n",
      "Epoch [4/1000], Train Loss: 9.2239, Val Loss: 9.8783\n",
      "Epoch [5/1000], Train Loss: 9.3614, Val Loss: 10.1084\n",
      "Epoch [6/1000], Train Loss: 9.5684, Val Loss: 10.1497\n",
      "Epoch [7/1000], Train Loss: 9.2656, Val Loss: 9.8265\n",
      "Epoch [8/1000], Train Loss: 9.4736, Val Loss: 9.9810\n",
      "Epoch [9/1000], Train Loss: 9.3428, Val Loss: 10.4996\n",
      "Epoch [10/1000], Train Loss: 9.3987, Val Loss: 9.9943\n",
      "Epoch [11/1000], Train Loss: 9.4453, Val Loss: 10.6081\n",
      "Epoch [12/1000], Train Loss: 9.5462, Val Loss: 9.9575\n",
      "Epoch [13/1000], Train Loss: 9.4076, Val Loss: 9.9996\n",
      "Epoch [14/1000], Train Loss: 9.5051, Val Loss: 11.5240\n",
      "Epoch [15/1000], Train Loss: 10.0639, Val Loss: 13.7679\n",
      "Epoch [16/1000], Train Loss: 9.7201, Val Loss: 10.2379\n",
      "Epoch [17/1000], Train Loss: 9.7168, Val Loss: 17.0919\n",
      "Epoch [18/1000], Train Loss: 11.3104, Val Loss: 10.2287\n",
      "Epoch [19/1000], Train Loss: 10.2288, Val Loss: 10.2843\n",
      "Epoch [20/1000], Train Loss: 10.1253, Val Loss: 11.4044\n",
      "Epoch [21/1000], Train Loss: 10.0308, Val Loss: 10.2007\n",
      "Epoch [22/1000], Train Loss: 10.4271, Val Loss: 11.0937\n",
      "Epoch [23/1000], Train Loss: 10.3279, Val Loss: 12.7525\n",
      "Epoch [24/1000], Train Loss: 10.1405, Val Loss: 10.5625\n",
      "Epoch [25/1000], Train Loss: 10.1740, Val Loss: 10.5261\n",
      "Epoch [26/1000], Train Loss: 11.1604, Val Loss: 22.4829\n",
      "Epoch [27/1000], Train Loss: 11.4495, Val Loss: 12.9719\n",
      "Epoch [28/1000], Train Loss: 9.7957, Val Loss: 10.4459\n",
      "Epoch [29/1000], Train Loss: 9.5434, Val Loss: 10.6729\n",
      "Epoch [30/1000], Train Loss: 9.7035, Val Loss: 10.6003\n",
      "Epoch [31/1000], Train Loss: 11.3814, Val Loss: 10.6584\n",
      "Epoch [32/1000], Train Loss: 11.4191, Val Loss: 10.4174\n",
      "Epoch [33/1000], Train Loss: 10.6605, Val Loss: 13.0466\n",
      "Epoch [34/1000], Train Loss: 9.8247, Val Loss: 15.9575\n",
      "Epoch [35/1000], Train Loss: 11.2234, Val Loss: 10.2439\n",
      "Epoch [36/1000], Train Loss: 10.6103, Val Loss: 10.2695\n",
      "Epoch [37/1000], Train Loss: 9.6509, Val Loss: 11.0326\n",
      "Epoch [38/1000], Train Loss: 9.8961, Val Loss: 10.1546\n",
      "Epoch [39/1000], Train Loss: 9.7766, Val Loss: 13.9943\n",
      "Epoch [40/1000], Train Loss: 10.2057, Val Loss: 11.9059\n",
      "Epoch [41/1000], Train Loss: 11.3349, Val Loss: 13.2282\n"
     ]
    }
   ],
   "source": [
    "rnn.trainer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 16.070394\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "16.07039447547065"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn.tester(Xte,Yte)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
